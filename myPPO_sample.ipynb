{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed7508b",
   "metadata": {},
   "source": [
    "# 自作のPPOノートブック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b2c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, asdict, is_dataclass\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from myActivator import tanhAndScale\n",
    "from myFunction import make_squashed_gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25772e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "                    stream=sys.stdout, datefmt=\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc437463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:40:57 [INFO] id: Pendulum-v1\n",
      "22:40:57 [INFO] entry_point: gymnasium.envs.classic_control.pendulum:PendulumEnv\n",
      "22:40:57 [INFO] reward_threshold: None\n",
      "22:40:57 [INFO] nondeterministic: False\n",
      "22:40:57 [INFO] max_episode_steps: 200\n",
      "22:40:57 [INFO] order_enforce: True\n",
      "22:40:57 [INFO] disable_env_checker: False\n",
      "22:40:57 [INFO] kwargs: {}\n",
      "22:40:57 [INFO] additional_wrappers: ()\n",
      "22:40:57 [INFO] vector_entry_point: None\n",
      "22:40:57 [INFO] namespace: None\n",
      "22:40:57 [INFO] name: Pendulum\n",
      "22:40:57 [INFO] version: 1\n",
      "22:40:57 [INFO] max_speed: 8\n",
      "22:40:57 [INFO] max_torque: 2.0\n",
      "22:40:57 [INFO] dt: 0.05\n",
      "22:40:57 [INFO] g: 10.0\n",
      "22:40:57 [INFO] m: 1.0\n",
      "22:40:57 [INFO] l: 1.0\n",
      "22:40:57 [INFO] render_mode: None\n",
      "22:40:57 [INFO] screen_dim: 500\n",
      "22:40:57 [INFO] screen: None\n",
      "22:40:57 [INFO] clock: None\n",
      "22:40:57 [INFO] isopen: True\n",
      "22:40:57 [INFO] action_space: Box(-2.0, 2.0, (1,), float32)\n",
      "22:40:57 [INFO] observation_space: Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n",
      "22:40:57 [INFO] spec: EnvSpec(id='Pendulum-v1', entry_point='gymnasium.envs.classic_control.pendulum:PendulumEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, disable_env_checker=True, kwargs={}, namespace=None, name='Pendulum', version=1, additional_wrappers=(), vector_entry_point=None)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "for key in vars(env.spec):\n",
    "    logging.info('%s: %s', key, vars(env.spec)[key])\n",
    "for key in vars(env.unwrapped):\n",
    "    logging.info('%s: %s', key, vars(env.unwrapped)[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e5818b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dd64991",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # ===== Pendulum-v1 specs =====\n",
    "    obs_dim: int = 3\n",
    "    act_dim: int = 1\n",
    "\n",
    "    # TRPOAgent 側は Config.u_llim / Config.u_ulim を参照\n",
    "    u_llim: list[float] = field(default_factory=lambda: [-2.0])\n",
    "    u_ulim: list[float] = field(default_factory=lambda: [ 2.0])\n",
    "\n",
    "    # ===== Network architecture =====\n",
    "    V_net_in: int = 3\n",
    "    P_net_in: int = 3\n",
    "\n",
    "    V_net_sizes: list[int] = field(default_factory=lambda: [64, 64])\n",
    "    P_net_sizes: list[int] = field(default_factory=lambda: [64, 64])\n",
    "\n",
    "    V_net_out: int = 1\n",
    "    P_net_out: int = 1  # = act_dim\n",
    "\n",
    "    # ===== Optimizer =====\n",
    "    V_lr: float = 1e-3\n",
    "    P_lr: float = 3e-4\n",
    "\n",
    "    # ===== GAE / discount =====\n",
    "    gamma: float = 0.99\n",
    "    lam: float = 0.97\n",
    "\n",
    "    # ===== PPO hyperparameters =====\n",
    "    clip_ratio: float = 0.2\n",
    "    policy_train_iters: int = 5\n",
    "    target_kl: float = 0.01\n",
    "    reward_scaling: float = 0.01\n",
    "\n",
    "    # ===== Value function training =====\n",
    "    value_train_iters: int = 5\n",
    "    value_l2_reg: float = 1e-3\n",
    "    v_clip_epsilon: float = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25aa0a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from dataclasses import dataclass, field, asdict, is_dataclass\n",
    "import logging\n",
    "\n",
    "# --- 1. Obs Norm 用のモジュール ---\n",
    "class EmpiricalNormalization(nn.Module):\n",
    "    def __init__(self, shape, epsilon=1e-4):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(shape))\n",
    "        self.register_buffer(\"running_var\", torch.ones(shape))\n",
    "        self.register_buffer(\"count\", torch.tensor(0.0))\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = x.view(-1, x.shape[-1])\n",
    "            batch_mean = x.mean(dim=0)\n",
    "            batch_var = x.var(dim=0, unbiased=False)\n",
    "            batch_count = x.shape[0]\n",
    "\n",
    "            if self.count == 0:\n",
    "                self.running_mean = batch_mean\n",
    "                self.running_var = batch_var\n",
    "                self.count = torch.tensor(float(batch_count), device=x.device)\n",
    "            else:\n",
    "                delta = batch_mean - self.running_mean\n",
    "                total_count = self.count + batch_count\n",
    "                \n",
    "                # 平均・分散のオンライン更新 (Welford's algorithm)\n",
    "                new_mean = self.running_mean + delta * (batch_count / total_count)\n",
    "                m_a = self.running_var * self.count\n",
    "                m_b = batch_var * batch_count\n",
    "                m_2 = m_a + m_b + delta**2 * self.count * batch_count / total_count\n",
    "                new_var = m_2 / total_count\n",
    "\n",
    "                self.running_mean = new_mean\n",
    "                self.running_var = new_var\n",
    "                self.count = total_count\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalize: (x - mean) / std\n",
    "        # -10 ~ 10 にクリップして異常値を弾く\n",
    "        return torch.clamp(\n",
    "            (x - self.running_mean) / torch.sqrt(self.running_var + self.epsilon),\n",
    "            -10.0, 10.0\n",
    "        )\n",
    "\n",
    "# --- 2. 重み初期化用の関数 ---\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # ===== Pendulum-v1 specs =====\n",
    "    obs_dim: int = 3\n",
    "    act_dim: int = 1\n",
    "    u_llim: list[float] = field(default_factory=lambda: [-2.0])\n",
    "    u_ulim: list[float] = field(default_factory=lambda: [ 2.0])\n",
    "\n",
    "    # ===== Network architecture =====\n",
    "    V_net_in: int = 3\n",
    "    P_net_in: int = 3\n",
    "    # 64x64 で十分\n",
    "    V_net_sizes: list[int] = field(default_factory=lambda: [64, 64])\n",
    "    P_net_sizes: list[int] = field(default_factory=lambda: [64, 64])\n",
    "    V_net_out: int = 1\n",
    "    P_net_out: int = 1\n",
    "\n",
    "    # ===== Optimizer =====\n",
    "    # PPOは LR 3e-4 が鉄板\n",
    "    V_lr: float = 1e-3 # Valueは少し高めでもOK\n",
    "    P_lr: float = 3e-4 \n",
    "\n",
    "    # ===== GAE / discount =====\n",
    "    gamma: float = 0.99\n",
    "    lam: float = 0.95\n",
    "\n",
    "    # ===== PPO hyperparameters =====\n",
    "    clip_ratio: float = 0.2\n",
    "    # ★重要: Epochは少なめに\n",
    "    policy_train_iters: int = 10\n",
    "    target_kl: float = 0.015  # 少し緩める\n",
    "    reward_scaling: float = 0.01\n",
    "\n",
    "    # ===== Value function training =====\n",
    "    value_train_iters: int = 10\n",
    "    value_l2_reg: float = 1e-3\n",
    "    v_clip_epsilon: float = 0.2\n",
    "\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, Config, device=None):\n",
    "        if Config is None: raise ValueError(\"No Config!!\")\n",
    "        self.Config = Config\n",
    "        \n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "\n",
    "        self.u_low = torch.tensor(Config.u_llim, dtype=torch.float32, device=self.device)\n",
    "        self.u_high = torch.tensor(Config.u_ulim, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # ★追加: 入力正規化モジュール\n",
    "        self.obs_norm = EmpiricalNormalization(shape=(Config.obs_dim,)).to(self.device)\n",
    "\n",
    "        # networks\n",
    "        self.V_net = self.build_net(Config.V_net_in, Config.V_net_sizes, Config.V_net_out, is_value=True).to(self.device)\n",
    "        self.P_net = self.build_net(Config.P_net_in, Config.P_net_sizes, Config.P_net_out, is_value=False).to(self.device)\n",
    "\n",
    "        self.V_net.train()\n",
    "        self.P_net.train()\n",
    "\n",
    "        # log_std\n",
    "        action_dim = Config.P_net_out\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim, device=self.device))\n",
    "\n",
    "        # optimizer (eps=1e-5 は数値安定性のために推奨)\n",
    "        self.V_optim = optim.Adam(self.V_net.parameters(), lr=Config.V_lr, eps=1e-5)\n",
    "        self.P_optim = optim.Adam(\n",
    "            list(self.P_net.parameters()) + [self.log_std], \n",
    "            lr=Config.P_lr, \n",
    "            eps=1e-5\n",
    "        )\n",
    "\n",
    "        # hyperparams\n",
    "        self.gamma = float(Config.gamma)\n",
    "        self.tau = float(Config.lam)\n",
    "        self.target_kl = float(getattr(Config, \"target_kl\", 0.015))\n",
    "        \n",
    "        self.policy_train_iters = int(getattr(Config, \"policy_train_iters\", 10))\n",
    "        self.value_train_iters = int(getattr(Config, \"value_train_iters\", 10))\n",
    "        self.value_l2_reg = float(getattr(Config, \"value_l2_reg\", 1e-3))\n",
    "        self.v_clip_epsilon = float(getattr(Config, \"v_clip_epsilon\", 0.2))\n",
    "\n",
    "    def build_net(self, input_size, hidden_sizes, output_size, is_value=False):\n",
    "        layers = []\n",
    "        in_size = input_size\n",
    "        for h_size in hidden_sizes:\n",
    "            # 中間層は Tanh + Orthogonal Init (gain=sqrt(2))\n",
    "            layers.append(layer_init(nn.Linear(in_size, h_size), std=np.sqrt(2)))\n",
    "            layers.append(nn.Tanh()) \n",
    "            in_size = h_size\n",
    "        \n",
    "        # 出力層の初期化（これが超重要）\n",
    "        if is_value:\n",
    "            # Valueは gain=1.0\n",
    "            layers.append(layer_init(nn.Linear(in_size, output_size), std=1.0))\n",
    "        else:\n",
    "            # Policyは gain=0.01 (初期行動を0付近にして、ランダム探索を促進)\n",
    "            layers.append(layer_init(nn.Linear(in_size, output_size), std=0.01))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_action_and_log_prob(self, state, deterministic=False, update_rms=False):\n",
    "        \"\"\"\n",
    "        update_rms=True のときだけ Obs Norm を更新する\n",
    "        \"\"\"\n",
    "        s = torch.as_tensor(state, dtype=torch.float32, device=self.device)\n",
    "        if s.dim() == 1:\n",
    "            s = s.unsqueeze(0)\n",
    "\n",
    "        # ★正規化の更新と適用\n",
    "        if update_rms:\n",
    "            self.obs_norm.update(s)\n",
    "        s = self.obs_norm(s)\n",
    "\n",
    "        dist = self._policy_dist(s)\n",
    "\n",
    "        if deterministic:\n",
    "            a = dist.mean\n",
    "            logp = None\n",
    "        else:\n",
    "            a = dist.sample()\n",
    "            logp = dist.log_prob(a).sum(axis=-1)\n",
    "\n",
    "        a = a.squeeze(0)\n",
    "        if logp is not None:\n",
    "            logp = logp.squeeze(0)\n",
    "        return a, logp\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self, state):\n",
    "        # 推論時は RMS 更新しない (update_rms=False)\n",
    "        a, _ = self.get_action_and_log_prob(state, deterministic=True, update_rms=False)\n",
    "        return a.cpu().numpy()\n",
    "    \n",
    "    def _policy_mean(self, states):\n",
    "        return self.P_net(states)\n",
    "    \n",
    "    def _policy_dist(self, states):\n",
    "        mean = self._policy_mean(states)\n",
    "        std = torch.exp(self.log_std)\n",
    "        std = std.unsqueeze(0).expand_as(mean)\n",
    "        return Normal(mean, std)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _compute_gae(self, rewards, values, next_values, dones):\n",
    "        batch_size = rewards.shape[0]\n",
    "        adv = torch.zeros_like(rewards, device=self.device)\n",
    "        gae = 0.0\n",
    "        for t in reversed(range(batch_size)):\n",
    "            if t == batch_size - 1:\n",
    "                nv = next_values[t]\n",
    "            else:\n",
    "                nv = values[t + 1]\n",
    "            delta = rewards[t] + self.gamma * nv * (1 - dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.tau * (1 - dones[t]) * gae\n",
    "            adv[t] = gae\n",
    "        ret = adv + values\n",
    "        return adv, ret\n",
    "    \n",
    "    def _ppo_step(self, states, actions, old_log_probs, advantages):\n",
    "        for _ in range(self.policy_train_iters):\n",
    "            dist = self._policy_dist(states)\n",
    "            log_probs = dist.log_prob(actions).sum(axis=-1)\n",
    "            ratios = torch.exp(log_probs - old_log_probs)\n",
    "\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1.0 - self.Config.clip_ratio, 1.0 + self.Config.clip_ratio) * advantages\n",
    "            \n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            self.P_optim.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            # 勾配クリップ (事故防止)\n",
    "            nn.utils.clip_grad_norm_(self.P_net.parameters(), max_norm=0.5)\n",
    "            self.P_optim.step()\n",
    "\n",
    "        # KL計算\n",
    "        with torch.no_grad():\n",
    "            dist = self._policy_dist(states)\n",
    "            log_probs = dist.log_prob(actions).sum(axis=-1)\n",
    "            change = (old_log_probs - log_probs).mean() # 近似KL\n",
    "        return policy_loss, change\n",
    "    \n",
    "    def _update_value_function(self, states, returns, old_values):\n",
    "        for _ in range(self.value_train_iters):\n",
    "            values = self.V_net(states).squeeze(-1)\n",
    "            value_loss = F.mse_loss(values, returns)\n",
    "            \n",
    "            # Value Clip\n",
    "            v_clip = old_values + torch.clamp(values - old_values, -self.v_clip_epsilon, self.v_clip_epsilon)\n",
    "            v_clip_loss = F.mse_loss(v_clip, returns)\n",
    "            \n",
    "            # L2 reg\n",
    "            l2_reg = torch.tensor(0., device=self.device)\n",
    "            for param in self.V_net.parameters():\n",
    "                l2_reg += torch.norm(param)**2\n",
    "            \n",
    "            loss = torch.max(value_loss, v_clip_loss) + self.value_l2_reg * l2_reg\n",
    "\n",
    "            self.V_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.V_net.parameters(), max_norm=0.5)\n",
    "            self.V_optim.step()\n",
    "        return loss\n",
    "    \n",
    "    def update_net(self, states, actions, log_probs, rewards, next_states, dones):\n",
    "        states = torch.as_tensor(states, dtype=torch.float32, device=self.device)\n",
    "        actions = torch.as_tensor(actions, dtype=torch.float32, device=self.device).view(-1, self.Config.act_dim)\n",
    "        log_probs = torch.as_tensor(log_probs, dtype=torch.float32, device=self.device).view(-1).detach()\n",
    "        rewards = torch.as_tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        next_states = torch.as_tensor(next_states, dtype=torch.float32, device=self.device)\n",
    "        dones = torch.as_tensor(dones, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # ★追加: バッチ全体に正規化を適用\n",
    "        states_norm = self.obs_norm(states)\n",
    "        next_states_norm = self.obs_norm(next_states)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 正規化された state を使う\n",
    "            values = self.V_net(states_norm).squeeze(-1)\n",
    "            next_values = self.V_net(next_states_norm).squeeze(-1)\n",
    "            \n",
    "            advantages, returns = self._compute_gae(rewards, values, next_values, dones)\n",
    "            # Advantage Normalization (学習速度向上に必須)\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # Update (正規化データを使用)\n",
    "        policy_loss, kl_change = self._ppo_step(states_norm, actions, log_probs, advantages)\n",
    "\n",
    "        # Adaptive LR\n",
    "        if kl_change > 1.5 * self.target_kl:\n",
    "            for param_group in self.P_optim.param_groups:\n",
    "                # 下限を少し緩く (1e-6 -> 1e-5)\n",
    "                param_group['lr'] = max(param_group['lr'] / 1.5, 1e-5)\n",
    "                # logging.info(f\"Decreased LR to {param_group['lr']}\")\n",
    "        elif kl_change < self.target_kl / 1.5:\n",
    "            for param_group in self.P_optim.param_groups:\n",
    "                param_group['lr'] = min(param_group['lr'] * 1.5, 1e-2)\n",
    "                # logging.info(f\"Increased LR to {param_group['lr']}\")\n",
    "\n",
    "        value_loss = self._update_value_function(states_norm, returns, values)\n",
    "\n",
    "        return {\"policy_loss\": policy_loss.item(), \"value_loss\": value_loss.item()}\n",
    "    \n",
    "    # ... (to, save_all, load_all はそのまま) ...\n",
    "    def to(self, device):\n",
    "        self.device = torch.device(device)\n",
    "        self.V_net.to(self.device)\n",
    "        self.P_net.to(self.device)\n",
    "        self.log_std.data = self.log_std.data.to(self.device)\n",
    "        self.u_low = self.u_low.to(self.device)\n",
    "        self.u_high = self.u_high.to(self.device)\n",
    "        self.obs_norm.to(self.device) # 忘れずに\n",
    "        return self\n",
    "    \n",
    "    def mode2eval(self):\n",
    "        self.V_net.eval()\n",
    "        self.P_net.eval()\n",
    "\n",
    "    def mode2train(self):\n",
    "        self.V_net.train()\n",
    "        self.P_net.train()\n",
    "\n",
    "    def save_all(self, path: str, extra: dict | None = None):\n",
    "        cfg = asdict(self.Config) if is_dataclass(self.Config) else self.Config\n",
    "        save_dict = {\n",
    "            \"config\": cfg,\n",
    "            \"V_net_state_dict\": self.V_net.state_dict(),\n",
    "            \"P_net_state_dict\": self.P_net.state_dict(),\n",
    "            \"log_std\": self.log_std.data,\n",
    "            # Normの統計情報も保存する\n",
    "            \"obs_norm_state_dict\": self.obs_norm.state_dict() \n",
    "        }\n",
    "        if extra is not None:\n",
    "            save_dict.update(extra)\n",
    "        torch.save(save_dict, path)\n",
    "        \n",
    "    def load_all(self, path: str, map_location=None):\n",
    "        load_dict = torch.load(path, map_location=map_location)\n",
    "        self.V_net.load_state_dict(load_dict[\"V_net_state_dict\"])\n",
    "        self.P_net.load_state_dict(load_dict[\"P_net_state_dict\"])\n",
    "        self.log_std.data = load_dict[\"log_std\"].to(self.device)\n",
    "        # Normのロード\n",
    "        if \"obs_norm_state_dict\" in load_dict:\n",
    "            self.obs_norm.load_state_dict(load_dict[\"obs_norm_state_dict\"])\n",
    "\n",
    "\n",
    "# 評価用関数（元のコードと同じものでOKですが、念のため再掲）\n",
    "def evaluate(env, agent, n_episodes=3):\n",
    "    scores = []\n",
    "    # 変数名修正対応\n",
    "    low_np = agent.u_low.detach().cpu().numpy()\n",
    "    high_np = agent.u_high.detach().cpu().numpy()\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        score = 0.0\n",
    "        while not done:\n",
    "            # step() は deterministic=True になっているはず\n",
    "            action = agent.step(obs)\n",
    "            action = np.clip(action, low_np, high_np)\n",
    "            \n",
    "            obs, rew, term, trunc, _ = env.step(action)\n",
    "            score += rew\n",
    "            done = term or trunc\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71815088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(\n",
    "    env,\n",
    "    agent,\n",
    "    total_step: int = 200_000,\n",
    "    batch_steps: int = 2048,\n",
    "    random_steps: int = 0,\n",
    "    bootstrap_on_timeout: bool = False,\n",
    "    log_interval_updates: int = 1,\n",
    "):\n",
    "    print(f\"Start PPO Training: Device={agent.device}, Batch={batch_steps}\")\n",
    "    low_np = agent.u_low.detach().cpu().numpy()\n",
    "    high_np = agent.u_high.detach().cpu().numpy()\n",
    "\n",
    "    loss_history = {\"policy_loss\": [], \"value_loss\": []}\n",
    "    train_reward_history = []\n",
    "    \n",
    "    rollout = {\"obs\": [], \"act\": [], \"logp\": [], \"rew\": [], \"obs_next\": [], \"done\": []}\n",
    "    def rollout_clear():\n",
    "        for k in rollout: rollout[k].clear()\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    ep_return = 0.0\n",
    "    update_num = 0\n",
    "\n",
    "    for t in range(total_step):\n",
    "        # --- (1) 行動選択 ---\n",
    "        if t < random_steps:\n",
    "            action_raw = np.atleast_1d(env.action_space.sample()).astype(np.float32)\n",
    "            with torch.no_grad():\n",
    "                a_t, logp_t = agent.get_action_and_log_prob(obs, deterministic=False, update_rms=True)\n",
    "                logp_val = logp_t.item()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # ★修正: update_rms=True を追加して、見たデータを学習させる\n",
    "                a_t, logp_t = agent.get_action_and_log_prob(obs, deterministic=False, update_rms=True)\n",
    "            \n",
    "            action_raw = np.atleast_1d(a_t.detach().cpu().numpy()).astype(np.float32)\n",
    "            logp_val = logp_t.item()\n",
    "\n",
    "        # --- (2) env step ---\n",
    "        action_env = np.clip(action_raw, low_np, high_np)\n",
    "        obs_next, reward, terminated, truncated, info = env.step(action_env)\n",
    "\n",
    "        ep_return += float(reward)\n",
    "\n",
    "        if bootstrap_on_timeout:\n",
    "            done_for_gae = float(terminated)\n",
    "        else:\n",
    "            done_for_gae = float(terminated or truncated)\n",
    "        done_for_reset = (terminated or truncated)\n",
    "\n",
    "        # 報酬スケーリング\n",
    "        scaled_reward = float(reward) * agent.Config.reward_scaling\n",
    "\n",
    "        # --- (3) バッファに保存 ---\n",
    "        rollout[\"obs\"].append(np.asarray(obs, dtype=np.float32))\n",
    "        rollout[\"act\"].append(np.asarray(action_raw, dtype=np.float32))\n",
    "        rollout[\"logp\"].append(logp_val)\n",
    "        rollout[\"rew\"].append(float(scaled_reward)) # Scaled reward\n",
    "        rollout[\"obs_next\"].append(np.asarray(obs_next, dtype=np.float32))\n",
    "        rollout[\"done\"].append(float(done_for_gae))\n",
    "\n",
    "        if done_for_reset:\n",
    "            train_reward_history.append(ep_return)\n",
    "            ep_return = 0.0\n",
    "            obs, info = env.reset()\n",
    "        else:\n",
    "            obs = obs_next\n",
    "\n",
    "        # --- (5) Update ---\n",
    "        if len(rollout[\"obs\"]) >= batch_steps:\n",
    "            update_num += 1\n",
    "            states      = np.stack(rollout[\"obs\"], axis=0)\n",
    "            actions     = np.stack(rollout[\"act\"], axis=0)\n",
    "            log_probs   = np.array(rollout[\"logp\"], dtype=np.float32)\n",
    "            rewards     = np.array(rollout[\"rew\"], dtype=np.float32)\n",
    "            states_next = np.stack(rollout[\"obs_next\"], axis=0)\n",
    "            dones       = np.array(rollout[\"done\"], dtype=np.float32)\n",
    "\n",
    "            loss_dict = agent.update_net(states, actions, log_probs, rewards, states_next, dones)\n",
    "            rollout_clear()\n",
    "\n",
    "            loss_history[\"policy_loss\"].append(loss_dict[\"policy_loss\"])\n",
    "            loss_history[\"value_loss\"].append(loss_dict[\"value_loss\"])\n",
    "\n",
    "            if (update_num % log_interval_updates) == 0:\n",
    "                eval_score = evaluate(env, agent, n_episodes=3)\n",
    "                logging.info(\n",
    "                    f\"Update {update_num:4d} | Step {t:6d} | \"\n",
    "                    f\"Eval: {eval_score:8.2f} | \"\n",
    "                    f\"P_Loss: {loss_dict['policy_loss']:.4f} | \"\n",
    "                    f\"V_Loss: {loss_dict['value_loss']:.4f}\"\n",
    "                )\n",
    "\n",
    "    return loss_history, train_reward_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25c1a565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start PPO Training: Device=cuda, Batch=2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1593821/623047813.py:304: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at /pytorch/aten/src/ATen/native/Scalar.cpp:22.)\n",
      "  return {\"policy_loss\": policy_loss.item(), \"value_loss\": value_loss.item()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:41:12 [INFO] Update    1 | Step   2047 | Eval: -1135.39 | P_Loss: -0.0015 | V_Loss: 0.7993\n",
      "22:41:21 [INFO] Update    2 | Step   4095 | Eval: -1112.23 | P_Loss: -0.0006 | V_Loss: 0.8726\n",
      "22:41:27 [INFO] Update    3 | Step   6143 | Eval: -1102.30 | P_Loss: -0.0035 | V_Loss: 0.6703\n",
      "22:41:33 [INFO] Update    4 | Step   8191 | Eval: -1285.84 | P_Loss: -0.0008 | V_Loss: 0.7049\n",
      "22:41:40 [INFO] Update    5 | Step  10239 | Eval:  -893.76 | P_Loss: -0.0046 | V_Loss: 0.5726\n",
      "22:41:48 [INFO] Update    6 | Step  12287 | Eval: -1800.23 | P_Loss: -0.0022 | V_Loss: 0.5199\n",
      "22:41:55 [INFO] Update    7 | Step  14335 | Eval: -1239.03 | P_Loss: -0.0021 | V_Loss: 0.4523\n",
      "22:42:01 [INFO] Update    8 | Step  16383 | Eval: -1617.89 | P_Loss: -0.0029 | V_Loss: 0.5012\n",
      "22:42:07 [INFO] Update    9 | Step  18431 | Eval:  -915.50 | P_Loss: -0.0032 | V_Loss: 0.5521\n",
      "22:42:15 [INFO] Update   10 | Step  20479 | Eval: -1217.77 | P_Loss: -0.0040 | V_Loss: 0.5993\n",
      "22:42:22 [INFO] Update   11 | Step  22527 | Eval: -1191.29 | P_Loss: -0.0037 | V_Loss: 0.5820\n",
      "22:42:29 [INFO] Update   12 | Step  24575 | Eval: -1399.14 | P_Loss: -0.0031 | V_Loss: 0.6329\n",
      "22:42:37 [INFO] Update   13 | Step  26623 | Eval: -1242.50 | P_Loss: -0.0032 | V_Loss: 0.6371\n",
      "22:42:45 [INFO] Update   14 | Step  28671 | Eval: -1131.79 | P_Loss: -0.0024 | V_Loss: 0.6937\n",
      "22:42:52 [INFO] Update   15 | Step  30719 | Eval: -1699.75 | P_Loss: -0.0038 | V_Loss: 0.6905\n",
      "22:42:59 [INFO] Update   16 | Step  32767 | Eval: -1121.59 | P_Loss: -0.0042 | V_Loss: 0.7040\n",
      "22:43:05 [INFO] Update   17 | Step  34815 | Eval: -1130.50 | P_Loss: -0.0032 | V_Loss: 0.5881\n",
      "22:43:15 [INFO] Update   18 | Step  36863 | Eval: -1143.76 | P_Loss: -0.0046 | V_Loss: 0.6210\n",
      "22:43:24 [INFO] Update   19 | Step  38911 | Eval: -1034.40 | P_Loss: -0.0013 | V_Loss: 0.5918\n",
      "22:43:33 [INFO] Update   20 | Step  40959 | Eval: -1152.59 | P_Loss: -0.0038 | V_Loss: 0.6251\n",
      "22:43:42 [INFO] Update   21 | Step  43007 | Eval: -1126.90 | P_Loss: -0.0056 | V_Loss: 0.6165\n",
      "22:43:52 [INFO] Update   22 | Step  45055 | Eval: -1200.92 | P_Loss: -0.0082 | V_Loss: 0.4521\n",
      "22:44:00 [INFO] Update   23 | Step  47103 | Eval: -1104.97 | P_Loss: -0.0037 | V_Loss: 0.4882\n",
      "22:44:08 [INFO] Update   24 | Step  49151 | Eval: -1160.58 | P_Loss: -0.0075 | V_Loss: 0.5446\n",
      "22:44:16 [INFO] Update   25 | Step  51199 | Eval: -1305.90 | P_Loss: -0.0033 | V_Loss: 0.4800\n",
      "22:44:25 [INFO] Update   26 | Step  53247 | Eval: -1099.13 | P_Loss: -0.0065 | V_Loss: 0.4161\n",
      "22:44:33 [INFO] Update   27 | Step  55295 | Eval: -1216.33 | P_Loss: -0.0033 | V_Loss: 0.4631\n",
      "22:44:41 [INFO] Update   28 | Step  57343 | Eval: -1131.13 | P_Loss: -0.0062 | V_Loss: 0.4647\n",
      "22:44:50 [INFO] Update   29 | Step  59391 | Eval: -1228.18 | P_Loss: -0.0040 | V_Loss: 0.4673\n",
      "22:44:59 [INFO] Update   30 | Step  61439 | Eval: -1161.57 | P_Loss: -0.0038 | V_Loss: 0.4669\n",
      "22:45:07 [INFO] Update   31 | Step  63487 | Eval: -1120.31 | P_Loss: -0.0057 | V_Loss: 0.5324\n",
      "22:45:16 [INFO] Update   32 | Step  65535 | Eval: -1159.99 | P_Loss: -0.0043 | V_Loss: 0.5752\n",
      "22:45:25 [INFO] Update   33 | Step  67583 | Eval: -1177.69 | P_Loss: -0.0042 | V_Loss: 0.5434\n",
      "22:45:34 [INFO] Update   34 | Step  69631 | Eval: -1185.01 | P_Loss: -0.0069 | V_Loss: 0.5065\n",
      "22:45:42 [INFO] Update   35 | Step  71679 | Eval: -1218.52 | P_Loss: -0.0060 | V_Loss: 0.5607\n",
      "22:45:51 [INFO] Update   36 | Step  73727 | Eval: -1176.95 | P_Loss: -0.0072 | V_Loss: 0.5262\n",
      "22:46:00 [INFO] Update   37 | Step  75775 | Eval: -1208.38 | P_Loss: -0.0064 | V_Loss: 0.5211\n",
      "22:46:08 [INFO] Update   38 | Step  77823 | Eval: -1164.43 | P_Loss: -0.0046 | V_Loss: 0.5088\n",
      "22:46:16 [INFO] Update   39 | Step  79871 | Eval: -1185.47 | P_Loss: -0.0060 | V_Loss: 0.5502\n",
      "22:46:25 [INFO] Update   40 | Step  81919 | Eval: -1236.09 | P_Loss: -0.0041 | V_Loss: 0.5524\n",
      "22:46:33 [INFO] Update   41 | Step  83967 | Eval: -1230.17 | P_Loss: -0.0062 | V_Loss: 0.5690\n",
      "22:46:41 [INFO] Update   42 | Step  86015 | Eval: -1140.27 | P_Loss: -0.0057 | V_Loss: 0.5870\n",
      "22:46:50 [INFO] Update   43 | Step  88063 | Eval: -1197.17 | P_Loss: -0.0055 | V_Loss: 0.6082\n",
      "22:46:59 [INFO] Update   44 | Step  90111 | Eval: -1248.35 | P_Loss: -0.0051 | V_Loss: 0.5409\n",
      "22:47:08 [INFO] Update   45 | Step  92159 | Eval: -1217.48 | P_Loss: -0.0061 | V_Loss: 0.5182\n",
      "22:47:16 [INFO] Update   46 | Step  94207 | Eval: -1186.99 | P_Loss: -0.0004 | V_Loss: 0.5725\n",
      "22:47:24 [INFO] Update   47 | Step  96255 | Eval: -1203.53 | P_Loss: -0.0047 | V_Loss: 0.5656\n",
      "22:47:34 [INFO] Update   48 | Step  98303 | Eval: -1142.15 | P_Loss: -0.0044 | V_Loss: 0.5672\n",
      "22:47:41 [INFO] Update   49 | Step 100351 | Eval: -1191.77 | P_Loss: -0.0057 | V_Loss: 0.5687\n",
      "22:47:50 [INFO] Update   50 | Step 102399 | Eval: -1182.27 | P_Loss: -0.0069 | V_Loss: 0.5580\n",
      "22:48:00 [INFO] Update   51 | Step 104447 | Eval: -1143.02 | P_Loss: -0.0024 | V_Loss: 0.5255\n",
      "22:48:10 [INFO] Update   52 | Step 106495 | Eval: -1154.59 | P_Loss: -0.0078 | V_Loss: 0.5254\n",
      "22:48:18 [INFO] Update   53 | Step 108543 | Eval: -1204.28 | P_Loss: -0.0045 | V_Loss: 0.5621\n",
      "22:48:26 [INFO] Update   54 | Step 110591 | Eval: -1148.83 | P_Loss: -0.0085 | V_Loss: 0.5075\n",
      "22:48:35 [INFO] Update   55 | Step 112639 | Eval: -1181.95 | P_Loss: -0.0058 | V_Loss: 0.5350\n",
      "22:48:42 [INFO] Update   56 | Step 114687 | Eval: -1204.55 | P_Loss: -0.0056 | V_Loss: 0.5067\n",
      "22:48:52 [INFO] Update   57 | Step 116735 | Eval: -1115.21 | P_Loss: -0.0054 | V_Loss: 0.5154\n",
      "22:49:01 [INFO] Update   58 | Step 118783 | Eval: -1197.87 | P_Loss: -0.0053 | V_Loss: 0.4682\n",
      "22:49:10 [INFO] Update   59 | Step 120831 | Eval: -1122.06 | P_Loss: -0.0050 | V_Loss: 0.4905\n",
      "22:49:17 [INFO] Update   60 | Step 122879 | Eval: -1169.02 | P_Loss: -0.0078 | V_Loss: 0.4959\n",
      "22:49:25 [INFO] Update   61 | Step 124927 | Eval: -1068.44 | P_Loss: -0.0118 | V_Loss: 0.4508\n",
      "22:49:35 [INFO] Update   62 | Step 126975 | Eval: -1197.80 | P_Loss: -0.0055 | V_Loss: 0.5286\n",
      "22:49:43 [INFO] Update   63 | Step 129023 | Eval: -1141.71 | P_Loss: -0.0055 | V_Loss: 0.5694\n",
      "22:49:50 [INFO] Update   64 | Step 131071 | Eval: -1122.77 | P_Loss: -0.0051 | V_Loss: 0.5297\n",
      "22:49:58 [INFO] Update   65 | Step 133119 | Eval: -1081.89 | P_Loss: -0.0082 | V_Loss: 0.5021\n",
      "22:50:07 [INFO] Update   66 | Step 135167 | Eval: -1110.73 | P_Loss: -0.0066 | V_Loss: 0.5148\n",
      "22:50:16 [INFO] Update   67 | Step 137215 | Eval: -1068.13 | P_Loss: -0.0087 | V_Loss: 0.4845\n",
      "22:50:25 [INFO] Update   68 | Step 139263 | Eval: -1062.66 | P_Loss: -0.0076 | V_Loss: 0.4452\n",
      "22:50:34 [INFO] Update   69 | Step 141311 | Eval: -1069.20 | P_Loss: -0.0042 | V_Loss: 0.4541\n",
      "22:50:43 [INFO] Update   70 | Step 143359 | Eval: -1177.18 | P_Loss: -0.0072 | V_Loss: 0.4722\n",
      "22:50:51 [INFO] Update   71 | Step 145407 | Eval: -1072.73 | P_Loss: -0.0063 | V_Loss: 0.4285\n",
      "22:51:00 [INFO] Update   72 | Step 147455 | Eval: -1083.29 | P_Loss: -0.0005 | V_Loss: 0.4820\n",
      "22:51:10 [INFO] Update   73 | Step 149503 | Eval: -1128.46 | P_Loss: -0.0051 | V_Loss: 0.4488\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m agent \u001b[38;5;241m=\u001b[39m PPOAgent(Config\u001b[38;5;241m=\u001b[39mConfig(),device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      2\u001b[0m total_step\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000000\u001b[39m\n\u001b[0;32m----> 4\u001b[0m lh, rh \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ppo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 35\u001b[0m, in \u001b[0;36mtrain_ppo\u001b[0;34m(env, agent, total_step, batch_steps, random_steps, bootstrap_on_timeout, log_interval_updates)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;66;03m# ★修正: update_rms=True を追加して、見たデータを学習させる\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m         a_t, logp_t \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action_and_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_rms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     action_raw \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(a_t\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     38\u001b[0m     logp_val \u001b[38;5;241m=\u001b[39m logp_t\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 173\u001b[0m, in \u001b[0;36mPPOAgent.get_action_and_log_prob\u001b[0;34m(self, state, deterministic, update_rms)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# ★正規化の更新と適用\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update_rms:\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_norm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_norm(s)\n\u001b[1;32m    176\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy_dist(s)\n",
      "Cell \u001b[0;32mIn[6], line 43\u001b[0m, in \u001b[0;36mEmpiricalNormalization.update\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;241m=\u001b[39m new_mean\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;241m=\u001b[39m new_var\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m \u001b[38;5;241m=\u001b[39m total_count\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1944\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1939\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m   1940\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1941\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1942\u001b[0m     )\n\u001b[0;32m-> 1944\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, value: Union[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1945\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mremove_from\u001b[39m(\u001b[38;5;241m*\u001b[39mdicts_or_sets):\n\u001b[1;32m   1946\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dicts_or_sets:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = PPOAgent(Config=Config(),device=device)\n",
    "total_step= 1000000\n",
    "\n",
    "lh, rh = train_ppo(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    total_step=total_step,\n",
    "    batch_steps=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b24ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc1c0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to models/ppo_final_20260201_222738.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def make_unique_path(path: str | Path) -> Path:\n",
    "    \"\"\"\n",
    "    path が既に存在する場合、末尾に _1, _2, ... を付けて未使用のパスを返す。\n",
    "    例: ddpg_final_20251221_235959.pth -> ddpg_final_20251221_235959_1.pth -> ...\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "\n",
    "    # 存在しないならそのまま使う\n",
    "    if not p.exists():\n",
    "        return p\n",
    "\n",
    "    parent = p.parent\n",
    "    stem = p.stem      # 拡張子抜きファイル名\n",
    "    suffix = p.suffix  # \".pth\"\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        cand = parent / f\"{stem}_{i}{suffix}\"\n",
    "        if not cand.exists():\n",
    "            return cand\n",
    "        i += 1\n",
    "\n",
    "\n",
    "# 推論用に eval モードにしておく（保存自体は train のままでも可）\n",
    "agent.mode2eval()\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "models_dir = Path(\"./models\")\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "base_path = models_dir / f\"ppo_final_{stamp}.pth\"\n",
    "save_path = make_unique_path(base_path)\n",
    "\n",
    "agent.save_all(\n",
    "    save_path.as_posix(),\n",
    "    extra={\n",
    "        \"total_step\": int(total_step),\n",
    "        \"reward_history\": rh,  # 必要ならそのままでOK\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
