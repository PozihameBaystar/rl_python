{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "498cd9b1",
   "metadata": {},
   "source": [
    "# 自作のPPOコード（環境並列版）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c76d4803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, asdict, is_dataclass\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from myActivator import tanhAndScale\n",
    "from myFunction import make_squashed_gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b595904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "                    stream=sys.stdout, datefmt=\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "761b2dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:48:39 [INFO] === Environment Specs (Reference) ===\n",
      "01:48:39 [INFO] id: Pendulum-v1\n",
      "01:48:39 [INFO] entry_point: gymnasium.envs.classic_control.pendulum:PendulumEnv\n",
      "01:48:39 [INFO] reward_threshold: None\n",
      "01:48:39 [INFO] nondeterministic: False\n",
      "01:48:39 [INFO] max_episode_steps: 200\n",
      "01:48:39 [INFO] order_enforce: True\n",
      "01:48:39 [INFO] disable_env_checker: False\n",
      "01:48:39 [INFO] kwargs: {}\n",
      "01:48:39 [INFO] additional_wrappers: ()\n",
      "01:48:39 [INFO] vector_entry_point: None\n",
      "01:48:39 [INFO] namespace: None\n",
      "01:48:39 [INFO] name: Pendulum\n",
      "01:48:39 [INFO] version: 1\n",
      "01:48:39 [INFO] max_speed: 8\n",
      "01:48:39 [INFO] max_torque: 2.0\n",
      "01:48:39 [INFO] dt: 0.05\n",
      "01:48:39 [INFO] g: 10.0\n",
      "01:48:39 [INFO] m: 1.0\n",
      "01:48:39 [INFO] l: 1.0\n",
      "01:48:39 [INFO] render_mode: None\n",
      "01:48:39 [INFO] screen_dim: 500\n",
      "01:48:39 [INFO] screen: None\n",
      "01:48:39 [INFO] clock: None\n",
      "01:48:39 [INFO] isopen: True\n",
      "01:48:39 [INFO] action_space: Box(-2.0, 2.0, (1,), float32)\n",
      "01:48:39 [INFO] observation_space: Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n",
      "01:48:39 [INFO] spec: EnvSpec(id='Pendulum-v1', entry_point='gymnasium.envs.classic_control.pendulum:PendulumEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, disable_env_checker=True, kwargs={}, namespace=None, name='Pendulum', version=1, additional_wrappers=(), vector_entry_point=None)\n",
      "01:48:39 [INFO] === VectorEnv Created: 4 parallel environments ===\n"
     ]
    }
   ],
   "source": [
    "# 並列環境数\n",
    "num_envs = 4  # CPUコア数やタスクの重さに応じて調整\n",
    "\n",
    "# --- 1. スペック確認用 (単一環境) ---\n",
    "# VectorEnv越しだと内部パラメータ(m, l, gなど)が見にくいので、\n",
    "# 確認用に一時的に単一環境を作ってログを出力します。\n",
    "temp_env = gym.make(\"Pendulum-v1\")\n",
    "logging.info(\"=== Environment Specs (Reference) ===\")\n",
    "for key in vars(temp_env.spec):\n",
    "    logging.info('%s: %s', key, vars(temp_env.spec)[key])\n",
    "for key in vars(temp_env.unwrapped):\n",
    "    # すべて表示すると多すぎる場合は必要なものだけでもOK\n",
    "    logging.info('%s: %s', key, vars(temp_env.unwrapped)[key])\n",
    "temp_env.close()\n",
    "\n",
    "# --- 2. 学習用環境の立ち上げ (並列化) ---\n",
    "# make_vec を使うのが現代的なGymnasiumの書き方です。\n",
    "# vectorization_mode=\"async\": マルチプロセス (重い環境向け)\n",
    "# vectorization_mode=\"sync\": シングルプロセス (Pendulumのような軽い環境ならこれでも十分高速)\n",
    "env = gym.make_vec(\"Pendulum-v1\", num_envs=num_envs, vectorization_mode=\"async\")\n",
    "\n",
    "logging.info(f\"=== VectorEnv Created: {num_envs} parallel environments ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9794b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1071db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # ===== Pendulum-v1 specs =====\n",
    "    obs_dim: int = 3\n",
    "    act_dim: int = 1\n",
    "\n",
    "    # TRPOAgent 側は Config.u_llim / Config.u_ulim を参照\n",
    "    u_llim: list[float] = field(default_factory=lambda: [-2.0])\n",
    "    u_ulim: list[float] = field(default_factory=lambda: [ 2.0])\n",
    "\n",
    "    # ===== Network architecture =====\n",
    "    V_net_in: int = 3\n",
    "    P_net_in: int = 3\n",
    "\n",
    "    V_net_sizes: list[int] = field(default_factory=lambda: [64, 64])\n",
    "    P_net_sizes: list[int] = field(default_factory=lambda: [64, 64])\n",
    "\n",
    "    V_net_out: int = 1\n",
    "    P_net_out: int = 1  # = act_dim\n",
    "\n",
    "    # ===== Optimizer =====\n",
    "    V_lr: float = 1e-3\n",
    "    P_lr: float = 3e-4\n",
    "\n",
    "    # ===== GAE / discount =====\n",
    "    gamma: float = 0.99\n",
    "    lam: float = 0.97\n",
    "\n",
    "    # ===== PPO hyperparameters =====\n",
    "    clip_ratio: float = 0.2\n",
    "    policy_train_iters: int = 50\n",
    "    target_kl: float = 0.01\n",
    "    reward_scaling: float = 0.01\n",
    "\n",
    "    # ===== Value function training =====\n",
    "    value_train_iters: int = 50\n",
    "    value_l2_reg: float = 1e-3\n",
    "    v_clip_epsilon: float = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72d9007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from myAgent import PPOAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d0e9184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "def train_ppo_parallel(\n",
    "    env,\n",
    "    agent,\n",
    "    total_step: int = 2000, # ループを回す回数（フレーム数ではない点に注意）\n",
    "    batch_steps: int = 128,  # PPOの1回の更新で回すタイムステップ数 (T)\n",
    "    random_steps: int = 0,\n",
    "    bootstrap_on_timeout: bool = False,\n",
    "    log_interval_updates: int = 1,\n",
    "    eval_env = None, # 評価用には別途「単一環境」を渡すのがベスト\n",
    "):\n",
    "    \"\"\"\n",
    "    並列環境 (VectorEnv) 専用のPPO学習ループ\n",
    "    \"\"\"\n",
    "    print(f\"Start PPO Parallel Training: Device={agent.device}, Steps={batch_steps}, Envs={env.num_envs}\")\n",
    "\n",
    "    # 変数名修正に伴う変更\n",
    "    low_np = agent.u_low.detach().cpu().numpy()\n",
    "    high_np = agent.u_high.detach().cpu().numpy()\n",
    "\n",
    "    # 履歴保存用\n",
    "    loss_history = {\"policy_loss\": [], \"value_loss\": []}\n",
    "    train_reward_history = [] \n",
    "\n",
    "    # ★変更点1: 現在のスコア保存用変数を「配列」にする\n",
    "    # 各環境ごとの現在の報酬積み上げ\n",
    "    current_ep_returns = np.zeros(env.num_envs, dtype=np.float32)\n",
    "\n",
    "    # rollout buffer\n",
    "    rollout = {\"obs\": [], \"act\": [], \"logp\": [], \"rew\": [], \"obs_next\": [], \"done\": []}\n",
    "\n",
    "    def rollout_clear():\n",
    "        for k in rollout:\n",
    "            rollout[k].clear()\n",
    "\n",
    "    # ★変更点2: Resetは最初の一回だけ\n",
    "    obs, info = env.reset()\n",
    "    # obs は (N, obs_dim) の形\n",
    "\n",
    "    update_num = 0\n",
    "\n",
    "    # total_steps は「全環境合計のステップ数」ではなく「ループ回数」として扱います\n",
    "    # 実際の消化フレーム数は total_steps * num_envs になります\n",
    "    for t in range(total_step):\n",
    "        \n",
    "        # --- (1) 行動選択 ---\n",
    "        # 並列環境なので、obs は (N, dim) です。\n",
    "        # Agent側は (N, dim) を受け取って (N, act_dim) を返すようになっています。\n",
    "        if t < random_steps:\n",
    "            # ランダム\n",
    "            # vector env の sample は (N, act_dim) を返すはずですが、念のため確認推奨\n",
    "            # ここでは安全策で agent を通してサンプリングします\n",
    "            with torch.no_grad():\n",
    "                 a_t, logp_t = agent.get_action_and_log_prob(obs, deterministic=False)\n",
    "                 logp_val = logp_t.cpu().numpy() # (N,)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                a_t, logp_t = agent.get_action_and_log_prob(obs, deterministic=False)\n",
    "            \n",
    "            # Tensor -> Numpy\n",
    "            action_raw = a_t.detach().cpu().numpy() # (N, act_dim)\n",
    "            logp_val = logp_t.detach().cpu().numpy() # (N,)\n",
    "\n",
    "        # --- (2) env step ---\n",
    "        # 行動のクリップ (N, act_dim) に対して一括で行われます\n",
    "        action_env = np.clip(action_raw, low_np, high_np)\n",
    "        \n",
    "        # VectorEnv は step すると以下が返ってきます (全て配列)\n",
    "        # obs_next: (N, obs_dim)\n",
    "        # reward: (N,)\n",
    "        # terminated: (N,) bool\n",
    "        # truncated: (N,) bool\n",
    "        obs_next, reward, terminated, truncated, info = env.step(action_env)\n",
    "\n",
    "        # ★変更点3: 報酬の積み上げ処理\n",
    "        # ベクトル同士の足し算 (N,) += (N,)\n",
    "        current_ep_returns += reward\n",
    "\n",
    "        # GAE計算用のdoneフラグ\n",
    "        if bootstrap_on_timeout:\n",
    "            done_for_gae = terminated\n",
    "        else:\n",
    "            done_for_gae = np.logical_or(terminated, truncated)\n",
    "        \n",
    "        # AI学習用報酬のスケーリング (N,)\n",
    "        scaled_reward = reward * agent.Config.reward_scaling\n",
    "\n",
    "        # --- (3) バッファに保存 ---\n",
    "        # 全て (N, ...) の形のままリストに追加します\n",
    "        rollout[\"obs\"].append(obs)           # (N, dim)\n",
    "        rollout[\"act\"].append(action_raw)    # (N, act)\n",
    "        rollout[\"logp\"].append(logp_val)     # (N,)\n",
    "        rollout[\"rew\"].append(scaled_reward) # (N,)\n",
    "        rollout[\"obs_next\"].append(obs_next) # (N, dim)\n",
    "        rollout[\"done\"].append(done_for_gae.astype(np.float32)) # (N,)\n",
    "\n",
    "        # ★変更点4: 終了判定とリセット処理 (Auto-Reset対応)\n",
    "        # VectorEnvは「終了した環境だけ内部で勝手にリセット」され、\n",
    "        # obs_next には「新しいエピソードの初期状態」が入って返ってきます。\n",
    "        # なので手動の env.reset() は削除します。\n",
    "\n",
    "        # どこの環境が終わったかチェックしてログを残す\n",
    "        dones = np.logical_or(terminated, truncated) # (N,)\n",
    "        \n",
    "        # 終了した環境があれば、そのスコアを履歴に移して、カウンタを0に戻す\n",
    "        if dones.any():\n",
    "            for i, is_done in enumerate(dones):\n",
    "                if is_done:\n",
    "                    train_reward_history.append(current_ep_returns[i])\n",
    "                    current_ep_returns[i] = 0.0\n",
    "                    \n",
    "                    # 厳密なPPO実装メモ:\n",
    "                    # obs_next[i] はリセット後の初期状態になっています。\n",
    "                    # 正確な学習のためには「リセット前の到達地点(terminal state)」を使うべき場合があります。\n",
    "                    # Gymnasiumでは info[\"final_observation\"][i] にそれが入っています。\n",
    "                    # 今回はコードを複雑にしないため、このまま obs_next を使います。\n",
    "\n",
    "        # 次の状態へ更新\n",
    "        obs = obs_next\n",
    "\n",
    "        # --- (5) Update ---\n",
    "        # 指定ステップ数 (batch_steps) だけデータが溜まったら更新\n",
    "        if len(rollout[\"obs\"]) >= batch_steps:\n",
    "            update_num += 1\n",
    "\n",
    "            # list -> numpy stacking\n",
    "            # 結果は (T, N, dim) になります\n",
    "            states      = np.stack(rollout[\"obs\"], axis=0)\n",
    "            actions     = np.stack(rollout[\"act\"], axis=0)\n",
    "            log_probs   = np.stack(rollout[\"logp\"], axis=0)\n",
    "            rewards     = np.stack(rollout[\"rew\"], axis=0)\n",
    "            states_next = np.stack(rollout[\"obs_next\"], axis=0)\n",
    "            dones       = np.stack(rollout[\"done\"], axis=0)\n",
    "\n",
    "            # Update実行\n",
    "            # Agent側で (T, N, dim) を受け取れるように修正したのでそのまま渡す\n",
    "            loss_dict = agent.update_net(states, actions, log_probs, rewards, states_next, dones)\n",
    "            \n",
    "            rollout_clear()\n",
    "\n",
    "            loss_history[\"policy_loss\"].append(loss_dict[\"policy_loss\"])\n",
    "            loss_history[\"value_loss\"].append(loss_dict[\"value_loss\"])\n",
    "\n",
    "            if (update_num % log_interval_updates) == 0:\n",
    "                # 評価: 並列環境(VectorEnv)で evaluate を回すとバグる（終わらない）ので\n",
    "                # 評価用に別途「単一環境(eval_env)」を渡してある場合のみ実行するように変更\n",
    "                eval_score = 0.0\n",
    "                if eval_env is not None:\n",
    "                    eval_score = evaluate(eval_env, agent, n_episodes=3)\n",
    "                else:\n",
    "                    # 評価環境がない場合は、学習中の平均報酬を表示しておく\n",
    "                    if len(train_reward_history) > 0:\n",
    "                        eval_score = np.mean(train_reward_history[-10:])\n",
    "                \n",
    "                logging.info(\n",
    "                    f\"Update {update_num:4d} | Step {t:6d} | \"\n",
    "                    f\"Eval/MeanReward: {eval_score:8.2f} | \"\n",
    "                    f\"P_Loss: {loss_dict['policy_loss']:.4f} | \"\n",
    "                    f\"V_Loss: {loss_dict['value_loss']:.4f}\"\n",
    "                )\n",
    "\n",
    "    return loss_history, train_reward_history\n",
    "\n",
    "\n",
    "def evaluate(env, agent, n_episodes=3):\n",
    "    \"\"\"\n",
    "    並列環境 (VectorEnv) 専用の評価関数\n",
    "    指定した n_episodes 分のエピソードが完了するまで全環境を回し続け、平均スコアを返します。\n",
    "    \"\"\"\n",
    "    # 行動の制限値\n",
    "    low_np = agent.u_low.detach().cpu().numpy()\n",
    "    high_np = agent.u_high.detach().cpu().numpy()\n",
    "\n",
    "    # 結果保存用リスト\n",
    "    episode_scores = []\n",
    "    \n",
    "    # 各環境の現在の報酬を保持する配列 (サイズ: num_envs)\n",
    "    current_rewards = np.zeros(env.num_envs, dtype=np.float32)\n",
    "    \n",
    "    # 完了したエピソード数をカウント\n",
    "    finished_count = 0\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    # 指定数のエピソードが集まるまでループ\n",
    "    while finished_count < n_episodes:\n",
    "        # Agentは (N, dim) を受け取り (N, act) を返す\n",
    "        # 評価時は決定論的(deterministic=True)に振る舞う\n",
    "        action = agent.step(obs)\n",
    "        action = np.clip(action, low_np, high_np)\n",
    "        \n",
    "        # Step (VectorEnvなので全環境が一斉に進む)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        # 報酬を加算 (ベクトル演算)\n",
    "        current_rewards += reward\n",
    "        \n",
    "        # 終了判定\n",
    "        dones = np.logical_or(terminated, truncated)\n",
    "        \n",
    "        # 終了した環境があればスコアを回収\n",
    "        if dones.any():\n",
    "            for i, is_done in enumerate(dones):\n",
    "                if is_done:\n",
    "                    # まだ必要数に達していなければ記録\n",
    "                    if finished_count < n_episodes:\n",
    "                        episode_scores.append(current_rewards[i])\n",
    "                        finished_count += 1\n",
    "                    \n",
    "                    # カウンタリセット (次のエピソードの準備)\n",
    "                    current_rewards[i] = 0.0\n",
    "\n",
    "    return np.mean(episode_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "627661a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start PPO Parallel Training: Device=cuda, Steps=2048, Envs=4\n",
      "01:48:45 [INFO] Increased policy learning rate to 0.00045\n",
      "01:48:46 [INFO] Update    1 | Step   2047 | Eval/MeanReward: -1132.15 | P_Loss: -0.0025 | V_Loss: 1.4181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hirokazumura/benkyo_kojin/rl_python/myAgent.py:1113: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at /pytorch/aten/src/ATen/native/Scalar.cpp:22.)\n",
      "  return {\"policy_loss\": policy_loss.item(), \"value_loss\": value_loss.item()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:48:51 [INFO] Increased policy learning rate to 0.000675\n",
      "01:48:51 [INFO] Update    2 | Step   4095 | Eval/MeanReward: -1170.33 | P_Loss: -0.0045 | V_Loss: 1.4008\n",
      "01:48:54 [INFO] Increased policy learning rate to 0.0010125\n",
      "01:48:54 [INFO] Update    3 | Step   6143 | Eval/MeanReward: -1133.58 | P_Loss: -0.0033 | V_Loss: 1.2602\n",
      "01:49:00 [INFO] Increased policy learning rate to 0.00151875\n",
      "01:49:00 [INFO] Update    4 | Step   8191 | Eval/MeanReward: -1171.38 | P_Loss: -0.0037 | V_Loss: 1.1278\n",
      "01:49:05 [INFO] Increased policy learning rate to 0.0022781249999999998\n",
      "01:49:05 [INFO] Update    5 | Step  10239 | Eval/MeanReward: -1223.30 | P_Loss: -0.0041 | V_Loss: 0.8637\n",
      "01:49:11 [INFO] Increased policy learning rate to 0.0034171874999999997\n",
      "01:49:11 [INFO] Update    6 | Step  12287 | Eval/MeanReward: -1226.90 | P_Loss: -0.0061 | V_Loss: 0.8263\n",
      "01:49:16 [INFO] Increased policy learning rate to 0.005125781249999999\n",
      "01:49:16 [INFO] Update    7 | Step  14335 | Eval/MeanReward: -1125.10 | P_Loss: -0.0062 | V_Loss: 0.7597\n",
      "01:49:22 [INFO] Update    8 | Step  16383 | Eval/MeanReward: -1193.87 | P_Loss: -0.0053 | V_Loss: 0.7166\n",
      "01:49:26 [INFO] Increased policy learning rate to 0.007688671874999999\n",
      "01:49:26 [INFO] Update    9 | Step  18431 | Eval/MeanReward:  -895.30 | P_Loss: -0.0082 | V_Loss: 0.6491\n",
      "01:49:31 [INFO] Increased policy learning rate to 0.01\n",
      "01:49:31 [INFO] Update   10 | Step  20479 | Eval/MeanReward: -1014.16 | P_Loss: -0.0080 | V_Loss: 0.6387\n",
      "01:49:37 [INFO] Update   11 | Step  22527 | Eval/MeanReward:  -974.44 | P_Loss: -0.0091 | V_Loss: 0.6346\n",
      "01:49:42 [INFO] Update   12 | Step  24575 | Eval/MeanReward: -1027.74 | P_Loss: -0.0099 | V_Loss: 0.5939\n",
      "01:49:48 [INFO] Update   13 | Step  26623 | Eval/MeanReward:  -917.15 | P_Loss: -0.0095 | V_Loss: 0.5776\n",
      "01:49:52 [INFO] Update   14 | Step  28671 | Eval/MeanReward:  -710.71 | P_Loss: -0.0094 | V_Loss: 0.5077\n",
      "01:49:58 [INFO] Update   15 | Step  30719 | Eval/MeanReward:  -816.48 | P_Loss: -0.0063 | V_Loss: 0.4613\n",
      "01:50:03 [INFO] Increased policy learning rate to 0.01\n",
      "01:50:03 [INFO] Update   16 | Step  32767 | Eval/MeanReward:  -709.50 | P_Loss: -0.0130 | V_Loss: 0.4607\n",
      "01:50:09 [INFO] Update   17 | Step  34815 | Eval/MeanReward:  -625.90 | P_Loss: -0.0121 | V_Loss: 0.3196\n",
      "01:50:14 [INFO] Update   18 | Step  36863 | Eval/MeanReward:  -563.78 | P_Loss: -0.0117 | V_Loss: 0.2567\n",
      "01:50:20 [INFO] Update   19 | Step  38911 | Eval/MeanReward:  -363.60 | P_Loss: -0.0140 | V_Loss: 0.2051\n",
      "01:50:23 [INFO] Update   20 | Step  40959 | Eval/MeanReward:  -273.68 | P_Loss: -0.0104 | V_Loss: 0.1747\n",
      "01:50:29 [INFO] Update   21 | Step  43007 | Eval/MeanReward:  -212.54 | P_Loss: -0.0127 | V_Loss: 0.0983\n",
      "01:50:34 [INFO] Update   22 | Step  45055 | Eval/MeanReward:  -220.42 | P_Loss: -0.0107 | V_Loss: 0.0967\n",
      "01:50:40 [INFO] Update   23 | Step  47103 | Eval/MeanReward:  -195.24 | P_Loss: -0.0091 | V_Loss: 0.0767\n",
      "01:50:46 [INFO] Update   24 | Step  49151 | Eval/MeanReward:  -167.16 | P_Loss: -0.0103 | V_Loss: 0.0666\n",
      "01:50:51 [INFO] Update   25 | Step  51199 | Eval/MeanReward:  -222.38 | P_Loss: -0.0103 | V_Loss: 0.0606\n",
      "01:50:54 [INFO] Increased policy learning rate to 0.01\n",
      "01:50:54 [INFO] Update   26 | Step  53247 | Eval/MeanReward:  -220.47 | P_Loss: -0.0068 | V_Loss: 0.0528\n",
      "01:51:00 [INFO] Update   27 | Step  55295 | Eval/MeanReward:  -185.90 | P_Loss: -0.0078 | V_Loss: 0.0605\n",
      "01:51:05 [INFO] Decreased policy learning rate to 0.006666666666666667\n",
      "01:51:05 [INFO] Update   28 | Step  57343 | Eval/MeanReward:  -247.87 | P_Loss: -0.0109 | V_Loss: 0.0573\n",
      "01:51:11 [INFO] Update   29 | Step  59391 | Eval/MeanReward:  -191.73 | P_Loss: -0.0118 | V_Loss: 0.0571\n",
      "01:51:16 [INFO] Update   30 | Step  61439 | Eval/MeanReward:  -205.95 | P_Loss: -0.0135 | V_Loss: 0.0595\n",
      "01:51:22 [INFO] Update   31 | Step  63487 | Eval/MeanReward:  -187.97 | P_Loss: -0.0074 | V_Loss: 0.0707\n",
      "01:51:25 [INFO] Decreased policy learning rate to 0.0044444444444444444\n",
      "01:51:25 [INFO] Update   32 | Step  65535 | Eval/MeanReward:  -223.99 | P_Loss: -0.0128 | V_Loss: 0.0492\n",
      "01:51:30 [INFO] Update   33 | Step  67583 | Eval/MeanReward:  -171.01 | P_Loss: -0.0107 | V_Loss: 0.0493\n",
      "01:51:36 [INFO] Update   34 | Step  69631 | Eval/MeanReward:  -170.73 | P_Loss: -0.0103 | V_Loss: 0.0422\n",
      "01:51:41 [INFO] Update   35 | Step  71679 | Eval/MeanReward:  -194.27 | P_Loss: -0.0118 | V_Loss: 0.0475\n",
      "01:51:47 [INFO] Update   36 | Step  73727 | Eval/MeanReward:  -190.02 | P_Loss: -0.0103 | V_Loss: 0.0450\n",
      "01:51:52 [INFO] Update   37 | Step  75775 | Eval/MeanReward:  -159.31 | P_Loss: -0.0113 | V_Loss: 0.0365\n",
      "01:51:56 [INFO] Update   38 | Step  77823 | Eval/MeanReward:  -124.10 | P_Loss: -0.0107 | V_Loss: 0.0405\n",
      "01:52:01 [INFO] Update   39 | Step  79871 | Eval/MeanReward:  -137.41 | P_Loss: -0.0112 | V_Loss: 0.0367\n",
      "01:52:06 [INFO] Decreased policy learning rate to 0.002962962962962963\n",
      "01:52:06 [INFO] Update   40 | Step  81919 | Eval/MeanReward:  -194.94 | P_Loss: -0.0122 | V_Loss: 0.0359\n",
      "01:52:12 [INFO] Update   41 | Step  83967 | Eval/MeanReward:  -218.91 | P_Loss: -0.0118 | V_Loss: 0.0420\n",
      "01:52:17 [INFO] Update   42 | Step  86015 | Eval/MeanReward:  -182.03 | P_Loss: -0.0104 | V_Loss: 0.0393\n",
      "01:52:23 [INFO] Increased policy learning rate to 0.0044444444444444444\n",
      "01:52:23 [INFO] Update   43 | Step  88063 | Eval/MeanReward:  -148.50 | P_Loss: -0.0091 | V_Loss: 0.0372\n",
      "01:52:26 [INFO] Decreased policy learning rate to 0.002962962962962963\n",
      "01:52:26 [INFO] Update   44 | Step  90111 | Eval/MeanReward:  -139.01 | P_Loss: -0.0075 | V_Loss: 0.0338\n",
      "01:52:31 [INFO] Update   45 | Step  92159 | Eval/MeanReward:  -206.07 | P_Loss: -0.0117 | V_Loss: 0.0358\n",
      "01:52:37 [INFO] Update   46 | Step  94207 | Eval/MeanReward:  -108.70 | P_Loss: -0.0109 | V_Loss: 0.0334\n",
      "01:52:43 [INFO] Update   47 | Step  96255 | Eval/MeanReward:  -160.94 | P_Loss: -0.0108 | V_Loss: 0.0351\n",
      "01:52:49 [INFO] Update   48 | Step  98303 | Eval/MeanReward:  -170.90 | P_Loss: -0.0100 | V_Loss: 0.0329\n"
     ]
    }
   ],
   "source": [
    "agent = PPOAgent(Config=Config(),device=device)\n",
    "total_step= 100000\n",
    "\n",
    "lh, rh = train_ppo_parallel(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    total_step=total_step,\n",
    "    batch_steps=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44bd65c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "695d1439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to models/ppo_final_20260206_015253.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def make_unique_path(path: str | Path) -> Path:\n",
    "    \"\"\"\n",
    "    path が既に存在する場合、末尾に _1, _2, ... を付けて未使用のパスを返す。\n",
    "    例: ddpg_final_20251221_235959.pth -> ddpg_final_20251221_235959_1.pth -> ...\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "\n",
    "    # 存在しないならそのまま使う\n",
    "    if not p.exists():\n",
    "        return p\n",
    "\n",
    "    parent = p.parent\n",
    "    stem = p.stem      # 拡張子抜きファイル名\n",
    "    suffix = p.suffix  # \".pth\"\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        cand = parent / f\"{stem}_{i}{suffix}\"\n",
    "        if not cand.exists():\n",
    "            return cand\n",
    "        i += 1\n",
    "\n",
    "\n",
    "# 推論用に eval モードにしておく（保存自体は train のままでも可）\n",
    "agent.mode2eval()\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "models_dir = Path(\"./models\")\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "base_path = models_dir / f\"ppo_final_{stamp}.pth\"\n",
    "save_path = make_unique_path(base_path)\n",
    "\n",
    "agent.save_all(\n",
    "    save_path.as_posix(),\n",
    "    extra={\n",
    "        \"total_step\": int(total_step),\n",
    "        \"reward_history\": rh,  # 必要ならそのままでOK\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
