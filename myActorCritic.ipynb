{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86f0edc",
   "metadata": {},
   "source": [
    "# 自作のActor-Criticノートブック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7e829d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from dataclasses import dataclass, asdict, is_dataclass\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from myActivator import tanhAndScale\n",
    "from myFunction import make_squashed_gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7040df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "                    stream=sys.stdout, datefmt=\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "016d2cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:53:50 [INFO] id: Pendulum-v1\n",
      "22:53:50 [INFO] entry_point: gymnasium.envs.classic_control.pendulum:PendulumEnv\n",
      "22:53:50 [INFO] reward_threshold: None\n",
      "22:53:50 [INFO] nondeterministic: False\n",
      "22:53:50 [INFO] max_episode_steps: 200\n",
      "22:53:50 [INFO] order_enforce: True\n",
      "22:53:50 [INFO] disable_env_checker: False\n",
      "22:53:50 [INFO] kwargs: {'render_mode': 'human'}\n",
      "22:53:50 [INFO] additional_wrappers: ()\n",
      "22:53:50 [INFO] vector_entry_point: None\n",
      "22:53:50 [INFO] namespace: None\n",
      "22:53:50 [INFO] name: Pendulum\n",
      "22:53:50 [INFO] version: 1\n",
      "22:53:50 [INFO] max_speed: 8\n",
      "22:53:50 [INFO] max_torque: 2.0\n",
      "22:53:50 [INFO] dt: 0.05\n",
      "22:53:50 [INFO] g: 10.0\n",
      "22:53:50 [INFO] m: 1.0\n",
      "22:53:50 [INFO] l: 1.0\n",
      "22:53:50 [INFO] render_mode: human\n",
      "22:53:50 [INFO] screen_dim: 500\n",
      "22:53:50 [INFO] screen: None\n",
      "22:53:50 [INFO] clock: None\n",
      "22:53:50 [INFO] isopen: True\n",
      "22:53:50 [INFO] action_space: Box(-2.0, 2.0, (1,), float32)\n",
      "22:53:50 [INFO] observation_space: Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n",
      "22:53:50 [INFO] spec: EnvSpec(id='Pendulum-v1', entry_point='gymnasium.envs.classic_control.pendulum:PendulumEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, disable_env_checker=True, kwargs={'render_mode': 'human'}, namespace=None, name='Pendulum', version=1, additional_wrappers=(), vector_entry_point=None)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\",render_mode=\"human\")\n",
    "for key in vars(env.spec):\n",
    "    logging.info('%s: %s', key, vars(env.spec)[key])\n",
    "for key in vars(env.unwrapped):\n",
    "    logging.info('%s: %s', key, vars(env.unwrapped)[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8221af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # ネットワーク構造をシンプルに（2層 × 128ユニット）\n",
    "    V_net_sizes = [128, 128]\n",
    "    P_net_sizes = [128, 128]\n",
    "    V_net_in = 3\n",
    "    P_net_in = 3\n",
    "    V_net_out = 1\n",
    "    P_net_out = 2  # mu と log_std\n",
    "\n",
    "    V_lr = 3e-4    # 少し小さめに\n",
    "    P_lr = 3e-4\n",
    "\n",
    "    u_high = 2.0\n",
    "    u_low = -2.0\n",
    "\n",
    "    log_std_min = -20.0   # より広い範囲を許容\n",
    "    log_std_max = 2.0\n",
    "\n",
    "    gamma = 0.99          # 長期報酬を重視"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5807268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n",
    "    def __init__(self, Config, device=None):\n",
    "        # ... (既存のコードと同じ初期化部分) ...\n",
    "        self.Config = Config\n",
    "        self.device = torch.device(device) if device else torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        \n",
    "        self.u_high = torch.as_tensor(Config.u_high, dtype=torch.float32, device=self.device)\n",
    "        self.u_low = torch.as_tensor(Config.u_low, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        self.V_net = self.build_net(Config.V_net_in, Config.V_net_sizes, Config.V_net_out).to(self.device)\n",
    "        self.P_net = self.build_net(Config.P_net_in, Config.P_net_sizes, Config.P_net_out).to(self.device)\n",
    "        \n",
    "        self.V_optim = optim.Adam(self.V_net.parameters(), Config.V_lr)\n",
    "        self.P_optim = optim.Adam(self.P_net.parameters(), Config.P_lr)\n",
    "        \n",
    "        self.log_std_min = Config.log_std_min\n",
    "        self.log_std_max = Config.log_std_max\n",
    "\n",
    "    def build_net(self, input_size, hidden_sizes, output_size):\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = h\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def get_action_and_log_prob(self, state, deterministic=False):\n",
    "        \"\"\"行動と log_prob を同時に返す（収集時に使用）\"\"\"\n",
    "        state = torch.as_tensor(state, dtype=torch.float32, device=self.device)\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        \n",
    "        out = self.P_net(state)\n",
    "        mu, log_std = torch.chunk(out, 2, dim=-1)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        dist = make_squashed_gaussian(mu=mu, std=std, low=self.u_low, high=self.u_high)\n",
    "        \n",
    "        if deterministic:\n",
    "            # 評価時は平均を使う（tanh + affine変換を適用）\n",
    "            action = torch.tanh(mu) * (self.u_high - self.u_low) / 2 + (self.u_high + self.u_low) / 2\n",
    "            log_prob = None\n",
    "        else:\n",
    "            action = dist.rsample()\n",
    "            log_prob = dist.log_prob(action)  # ← ここで計算して保存！\n",
    "        \n",
    "        return action.squeeze(0), log_prob.squeeze(0) if log_prob is not None else None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, state):\n",
    "        \"\"\"推論用（勾配不要）\"\"\"\n",
    "        action, _ = self.get_action_and_log_prob(state, deterministic=False)\n",
    "        return action.cpu().numpy()\n",
    "\n",
    "    def update_net_batch(self, states, log_probs, rewards, states_next, dones):\n",
    "        \"\"\"\n",
    "        修正版：log_probs は収集時に計算済みのものを受け取る\n",
    "        \"\"\"\n",
    "        states = torch.as_tensor(states, dtype=torch.float32, device=self.device)\n",
    "        log_probs = torch.stack(log_probs).to(self.device)  # 収集時に保存したもの\n",
    "        rewards = torch.as_tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        states_next = torch.as_tensor(states_next, dtype=torch.float32, device=self.device)\n",
    "        dones = torch.as_tensor(dones, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        if rewards.dim() == 1:\n",
    "            rewards = rewards.unsqueeze(1)\n",
    "        if dones.dim() == 1:\n",
    "            dones = dones.unsqueeze(1)\n",
    "        if log_probs.dim() == 1:\n",
    "            log_probs = log_probs.unsqueeze(1)\n",
    "\n",
    "        # ========== Critic (V_net) の更新 ==========\n",
    "        with torch.no_grad():\n",
    "            V_next = self.V_net(states_next)\n",
    "            y_targets = rewards + self.Config.gamma * (1 - dones) * V_next\n",
    "\n",
    "        V_values = self.V_net(states)\n",
    "        V_loss = F.mse_loss(V_values, y_targets)\n",
    "        \n",
    "        self.V_optim.zero_grad()\n",
    "        V_loss.backward()\n",
    "        self.V_optim.step()\n",
    "\n",
    "        # ========== Actor (P_net) の更新 ==========\n",
    "        # Advantage を計算（更新後の V_net ではなく、更新前の値を使う）\n",
    "        with torch.no_grad():\n",
    "            # 注意：V_values は更新前に計算済みなのでそのまま使える\n",
    "            advantages = (y_targets - V_values)\n",
    "            # Advantage の正規化（学習安定化のため重要！）\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # log_probs は収集時に計算済みなので、そのまま使う\n",
    "        P_loss = -(advantages * log_probs).mean()\n",
    "        \n",
    "        self.P_optim.zero_grad()\n",
    "        P_loss.backward()\n",
    "        self.P_optim.step()\n",
    "\n",
    "        return float(V_loss.item()), float(P_loss.item())\n",
    "\n",
    "    \n",
    "    def to(self,device):\n",
    "        self.device = torch.device(device)\n",
    "        self.V_net.to(self.device)\n",
    "        self.P_net.to(self.device)\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def save_all(self,path:str,extra:dict|None=None):\n",
    "        cfg = asdict(self.Config) if is_dataclass(self.Config) else self.Config\n",
    "        ckpt = {\n",
    "            \"Config\":cfg,\n",
    "            \"V_net\":self.V_net.state_dict(),\n",
    "            \"P_net\":self.P_net.state_dict(),\n",
    "        }\n",
    "        if extra is not None:\n",
    "            ckpt[\"extra\"] = extra\n",
    "        \n",
    "        torch.save(ckpt,path)\n",
    "\n",
    "    \n",
    "    def load_all(self,path:str,map_location=None):\n",
    "        ckpt = torch.load(path,map_location=map_location)\n",
    "        self.V_net.load_state_dict(ckpt[\"V_net\"])\n",
    "        self.P_net.load_state_dict(ckpt[\"P_net\"])\n",
    "\n",
    "        return ckpt.get(\"extra\",None)\n",
    "    \n",
    "\n",
    "    def mode2eval(self):\n",
    "        self.V_net.eval()\n",
    "        self.P_net.eval()\n",
    "\n",
    "\n",
    "    def mode2train(self):\n",
    "        self.V_net.train()\n",
    "        self.P_net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62a5b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, rollout_num=200, rollout_len=2048):  # rollout_len を増やす\n",
    "    reward_history = []\n",
    "    reward_log = 0\n",
    "    episode = 0\n",
    "    state, info = env.reset()\n",
    "\n",
    "    for r in range(rollout_num):\n",
    "        states = []\n",
    "        log_probs = []  # actions の代わりに log_probs を保存！\n",
    "        rewards = []\n",
    "        states_next = []\n",
    "        dones = []\n",
    "\n",
    "        for t in range(rollout_len):\n",
    "            # 勾配計算が必要なので no_grad を外す\n",
    "            action, log_prob = agent.get_action_and_log_prob(state)\n",
    "            \n",
    "            state_next, reward, terminated, truncated, info = env.step(action.detach().cpu().numpy())\n",
    "            done = terminated\n",
    "\n",
    "            states.append(state.copy())\n",
    "            log_probs.append(log_prob)  # Tensor のまま保存\n",
    "            rewards.append(reward)\n",
    "            states_next.append(state_next.copy())\n",
    "            dones.append(float(done))\n",
    "\n",
    "            reward_log += reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                state, info = env.reset()\n",
    "                episode += 1\n",
    "                logging.info('episode %d: reward = %.2f', episode, reward_log)\n",
    "                reward_history.append(reward_log)\n",
    "                reward_log = 0\n",
    "            else:\n",
    "                state = state_next\n",
    "\n",
    "        V_loss, P_loss = agent.update_net_batch(states, log_probs, rewards, states_next, dones)\n",
    "        \n",
    "        # 勾配計算グラフをクリア（メモリリーク防止）\n",
    "        log_probs = None\n",
    "\n",
    "    return reward_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3a48d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98684/2747379279.py:64: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  states = torch.as_tensor(states, dtype=torch.float32, device=self.device)\n",
      "/tmp/ipykernel_98684/2747379279.py:104: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at /pytorch/aten/src/ATen/native/Scalar.cpp:22.)\n",
      "  return float(V_loss.item()), float(P_loss.item())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:54:00 [INFO] episode 1: reward = -1519.51\n",
      "22:54:07 [INFO] episode 2: reward = -1209.30\n",
      "22:54:13 [INFO] episode 3: reward = -1365.54\n",
      "22:54:20 [INFO] episode 4: reward = -1083.47\n",
      "22:54:27 [INFO] episode 5: reward = -981.90\n",
      "22:54:34 [INFO] episode 6: reward = -1349.46\n",
      "22:54:41 [INFO] episode 7: reward = -877.89\n",
      "22:54:48 [INFO] episode 8: reward = -1331.64\n",
      "22:54:55 [INFO] episode 9: reward = -1299.91\n",
      "22:55:02 [INFO] episode 10: reward = -947.95\n",
      "22:55:09 [INFO] episode 11: reward = -841.92\n",
      "22:55:16 [INFO] episode 12: reward = -1651.72\n",
      "22:55:23 [INFO] episode 13: reward = -899.33\n",
      "22:55:30 [INFO] episode 14: reward = -1175.74\n",
      "22:55:37 [INFO] episode 15: reward = -979.93\n",
      "22:55:44 [INFO] episode 16: reward = -1013.94\n",
      "22:55:51 [INFO] episode 17: reward = -1687.81\n",
      "22:55:58 [INFO] episode 18: reward = -870.64\n",
      "22:56:05 [INFO] episode 19: reward = -864.88\n",
      "22:56:12 [INFO] episode 20: reward = -867.84\n",
      "22:56:19 [INFO] episode 21: reward = -1008.77\n",
      "22:56:26 [INFO] episode 22: reward = -990.75\n",
      "22:56:33 [INFO] episode 23: reward = -1034.78\n",
      "22:56:40 [INFO] episode 24: reward = -1321.85\n",
      "22:56:47 [INFO] episode 25: reward = -1183.09\n",
      "22:56:54 [INFO] episode 26: reward = -1592.94\n",
      "22:57:01 [INFO] episode 27: reward = -1510.44\n",
      "22:57:08 [INFO] episode 28: reward = -1492.31\n",
      "22:57:15 [INFO] episode 29: reward = -1210.96\n",
      "22:57:22 [INFO] episode 30: reward = -1336.00\n",
      "22:57:29 [INFO] episode 31: reward = -1735.61\n",
      "22:57:36 [INFO] episode 32: reward = -1206.83\n",
      "22:57:43 [INFO] episode 33: reward = -1093.27\n",
      "22:57:50 [INFO] episode 34: reward = -1611.36\n",
      "22:57:57 [INFO] episode 35: reward = -1187.23\n",
      "22:58:04 [INFO] episode 36: reward = -1746.80\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (1, 1)) of distribution Normal(loc: tensor([[nan]], device='cuda:0', grad_fn=<SplitBackward0>), scale: tensor([[nan]], device='cuda:0', grad_fn=<ExpBackward0>)) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan]], device='cuda:0', grad_fn=<SplitBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m rollout_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m\n\u001b[1;32m      5\u001b[0m rollout_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m----> 7\u001b[0m rh \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrollout_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrollout_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrollout_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrollout_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, agent, rollout_num, rollout_len)\u001b[0m\n\u001b[1;32m     12\u001b[0m dones \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(rollout_len):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# 勾配計算が必要なので no_grad を外す\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action_and_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     state_next, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     19\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated\n",
      "Cell \u001b[0;32mIn[5], line 42\u001b[0m, in \u001b[0;36mActorCriticAgent.get_action_and_log_prob\u001b[0;34m(self, state, deterministic)\u001b[0m\n\u001b[1;32m     39\u001b[0m log_std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(log_std, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std_min, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std_max)\n\u001b[1;32m     40\u001b[0m std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(log_std)\n\u001b[0;32m---> 42\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[43mmake_squashed_gaussian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mu_low\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhigh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mu_high\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deterministic:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# 評価時は平均を使う（tanh + affine変換を適用）\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(mu) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu_high \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu_low) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu_high \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu_low) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/benkyo_kojin/rl_python/myFunction.py:8\u001b[0m, in \u001b[0;36mmake_squashed_gaussian\u001b[0;34m(mu, std, low, high)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_squashed_gaussian\u001b[39m(mu: torch\u001b[38;5;241m.\u001b[39mTensor, std: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m      6\u001b[0m                           low: torch\u001b[38;5;241m.\u001b[39mTensor, high: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# 1) ベース分布：多次元の対角ガウス（Independentで次元和を自動化）\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     base \u001b[38;5;241m=\u001b[39m Independent(\u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# 2) (-1,1) に押し込む tanh 変換（可逆変換として扱える）\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     tanh \u001b[38;5;241m=\u001b[39m TanhTransform(cache_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributions/normal.py:66\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributions/distribution.py:72\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     70\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_is_all_true(valid):\n\u001b[0;32m---> 72\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m             )\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (1, 1)) of distribution Normal(loc: tensor([[nan]], device='cuda:0', grad_fn=<SplitBackward0>), scale: tensor([[nan]], device='cuda:0', grad_fn=<ExpBackward0>)) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan]], device='cuda:0', grad_fn=<SplitBackward0>)"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = ActorCriticAgent(Config=Config(),device=device)\n",
    "\n",
    "rollout_num=500\n",
    "rollout_len=100\n",
    "\n",
    "rh = train(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    rollout_num=rollout_num,\n",
    "    rollout_len=rollout_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20337b6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774a60c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論用に eval モードにしておく（保存自体は train のままでも可）\n",
    "agent.mode2eval()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "agent.save_all(\n",
    "    \"./models/actor_critic_final_\" + stamp + \".pth\",\n",
    "    extra={\n",
    "        \"rollout_num\": rollout_num,\n",
    "        \"rollout_len\": rollout_len,\n",
    "        \"reward_history\": rh,  # 要らなければ外してOK\n",
    "    }\n",
    ")\n",
    "print(\"saved to actor_critic_final.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
