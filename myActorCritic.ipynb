{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86f0edc",
   "metadata": {},
   "source": [
    "# 自作のActor-Criticノートブック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7e829d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from dataclasses import dataclass, asdict, is_dataclass\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from myActivator import tanhAndScale\n",
    "from myFunction import make_squashed_gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7040df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "                    stream=sys.stdout, datefmt=\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "016d2cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:12:11 [INFO] id: Pendulum-v1\n",
      "22:12:11 [INFO] entry_point: gymnasium.envs.classic_control.pendulum:PendulumEnv\n",
      "22:12:11 [INFO] reward_threshold: None\n",
      "22:12:11 [INFO] nondeterministic: False\n",
      "22:12:11 [INFO] max_episode_steps: 200\n",
      "22:12:11 [INFO] order_enforce: True\n",
      "22:12:11 [INFO] disable_env_checker: False\n",
      "22:12:11 [INFO] kwargs: {'render_mode': 'human'}\n",
      "22:12:11 [INFO] additional_wrappers: ()\n",
      "22:12:11 [INFO] vector_entry_point: None\n",
      "22:12:11 [INFO] namespace: None\n",
      "22:12:11 [INFO] name: Pendulum\n",
      "22:12:11 [INFO] version: 1\n",
      "22:12:11 [INFO] max_speed: 8\n",
      "22:12:11 [INFO] max_torque: 2.0\n",
      "22:12:11 [INFO] dt: 0.05\n",
      "22:12:11 [INFO] g: 10.0\n",
      "22:12:11 [INFO] m: 1.0\n",
      "22:12:11 [INFO] l: 1.0\n",
      "22:12:11 [INFO] render_mode: human\n",
      "22:12:11 [INFO] screen_dim: 500\n",
      "22:12:11 [INFO] screen: None\n",
      "22:12:11 [INFO] clock: None\n",
      "22:12:11 [INFO] isopen: True\n",
      "22:12:11 [INFO] action_space: Box(-2.0, 2.0, (1,), float32)\n",
      "22:12:11 [INFO] observation_space: Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n",
      "22:12:11 [INFO] spec: EnvSpec(id='Pendulum-v1', entry_point='gymnasium.envs.classic_control.pendulum:PendulumEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, disable_env_checker=True, kwargs={'render_mode': 'human'}, namespace=None, name='Pendulum', version=1, additional_wrappers=(), vector_entry_point=None)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\",render_mode=\"human\")\n",
    "for key in vars(env.spec):\n",
    "    logging.info('%s: %s', key, vars(env.spec)[key])\n",
    "for key in vars(env.unwrapped):\n",
    "    logging.info('%s: %s', key, vars(env.unwrapped)[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8221af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    V_net_sizes = [6,12,24,12,6]\n",
    "    P_net_sizes = [6,12,24,24,12,6]\n",
    "    V_net_in = 3\n",
    "    P_net_in = 3\n",
    "    V_net_out = 1\n",
    "    P_net_out = 2\n",
    "\n",
    "    V_lr = 1e-3\n",
    "    P_lr = 1e-3\n",
    "\n",
    "    u_high = 2.0\n",
    "    u_low = -2.0\n",
    "\n",
    "    log_std_min = -10.0\n",
    "    log_std_max = 1.0\n",
    "\n",
    "    gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5807268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n",
    "    def __init__(self,Config,device=None):\n",
    "        if Config:\n",
    "            self.Config = Config\n",
    "        else:\n",
    "            raise ValueError(\"No Config!!\")\n",
    "        \n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "\n",
    "        self.u_high = torch.as_tensor(Config.u_high, dtype=torch.float32, device=self.device)\n",
    "        self.u_low = torch.as_tensor(Config.u_low, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        self.V_net = self.build_net(\n",
    "            Config.V_net_in,\n",
    "            Config.V_net_sizes,\n",
    "            Config.V_net_out\n",
    "        ).to(self.device)\n",
    "        self.V_net.train()\n",
    "\n",
    "        self.P_net = self.build_net(\n",
    "            Config.P_net_in,\n",
    "            Config.P_net_sizes,\n",
    "            Config.P_net_out\n",
    "        ).to(self.device)\n",
    "        self.P_net.train()\n",
    "\n",
    "        self.V_optim = optim.Adam(self.V_net.parameters(),Config.V_lr)\n",
    "        self.P_optim = optim.Adam(self.P_net.parameters(),Config.P_lr)\n",
    "\n",
    "        self.log_std_min = torch.as_tensor(Config.log_std_min,dtype=torch.float32,device=self.device)\n",
    "        self.log_std_max = torch.as_tensor(Config.log_std_max,dtype=torch.float32,device=self.device)\n",
    "\n",
    "    \n",
    "    def to(self,device):\n",
    "        self.device = torch.device(device)\n",
    "        self.V_net.to(self.device)\n",
    "        self.P_net.to(self.device)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def build_net(self,input_size,hidden_sizes,output_size=1,output_activator=None):\n",
    "        layers = []\n",
    "        for input_size, output_size in zip([input_size]+hidden_sizes, hidden_sizes+[output_size]):\n",
    "            layers.append(nn.Linear(input_size,output_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers = layers[:-1]\n",
    "        if output_activator:\n",
    "            layers.append(output_activator)\n",
    "        net = nn.Sequential(*layers)\n",
    "        return net\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self,state):\n",
    "        state = torch.as_tensor(state,dtype=torch.float32,device=self.device)\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        out = self.P_net(state)\n",
    "        mu, log_std = torch.chunk(out, 2, dim=-1)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        std = torch.exp(log_std)\n",
    "        dist = make_squashed_gaussian(mu=mu,std=std,low=self.u_low,high=self.u_high)\n",
    "        action = dist.rsample()\n",
    "        return action.squeeze(0).cpu().numpy()\n",
    "    \n",
    "\n",
    "    def save_all(self,path:str,extra:dict|None=None):\n",
    "        cfg = asdict(self.Config) if is_dataclass(self.Config) else self.Config\n",
    "        ckpt = {\n",
    "            \"Config\":cfg,\n",
    "            \"V_net\":self.V_net.state_dict(),\n",
    "            \"P_net\":self.P_net.state_dict(),\n",
    "        }\n",
    "        if extra is not None:\n",
    "            ckpt[\"extra\"] = extra\n",
    "        \n",
    "        torch.save(ckpt,path)\n",
    "\n",
    "    \n",
    "    def load_all(self,path:str,map_location=None):\n",
    "        ckpt = torch.load(path,map_location=map_location)\n",
    "        self.V_net.load_state_dict(ckpt[\"V_net\"])\n",
    "        self.P_net.load_state_dict(ckpt[\"P_net\"])\n",
    "\n",
    "        return ckpt.get(\"extra\",None)\n",
    "    \n",
    "\n",
    "    def mode2eval(self):\n",
    "        self.V_net.eval()\n",
    "        self.P_net.eval()\n",
    "\n",
    "\n",
    "    def mode2train(self):\n",
    "        self.V_net.train()\n",
    "        self.P_net.train()\n",
    "    \n",
    "\n",
    "    def update_net_batch(self,states,actions,rewards,states_next,dones):\n",
    "        states = torch.as_tensor(states,dtype=torch.float32,device=self.device)\n",
    "        actions = torch.as_tensor(actions,dtype=torch.float32,device=self.device)\n",
    "        rewards = torch.as_tensor(rewards,dtype=torch.float32,device=self.device)\n",
    "        states_next = torch.as_tensor(states_next,dtype=torch.float32,device=self.device)\n",
    "\n",
    "        if rewards.dim() == 1:\n",
    "            rewards = rewards.unsqueeze(1)\n",
    "        \n",
    "        if dones is None:\n",
    "            dones = torch.zeros((states.shape[0], 1), dtype=torch.float32, device=self.device)\n",
    "        else:\n",
    "            dones = torch.as_tensor(dones, dtype=torch.float32, device=self.device)\n",
    "            if dones.dim() == 1:\n",
    "                dones = dones.unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_targets = rewards+self.Config.gamma*(1-dones)*self.V_net(states_next)\n",
    "\n",
    "        V_values = self.V_net(states)\n",
    "        V_loss = F.mse_loss(y_targets,V_values)\n",
    "        self.V_optim.zero_grad()\n",
    "        V_loss.backward()\n",
    "        self.V_optim.step()\n",
    "\n",
    "        for p in self.V_net.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        outs = self.P_net(states)\n",
    "        mus, log_stds = torch.chunk(outs, 2, dim=-1)\n",
    "        log_stds = torch.clamp(log_stds, self.log_std_min, self.log_std_max)\n",
    "        stds = torch.exp(log_stds)\n",
    "        dists = make_squashed_gaussian(mu=mus,std=stds,low=self.u_low,high=self.u_high)\n",
    "        advantages = (y_targets - V_values).detach()\n",
    "        P_loss = -(advantages*dists.log_prob(actions).unsqueeze(-1)).mean()\n",
    "        self.P_optim.zero_grad()\n",
    "        P_loss.backward()\n",
    "        self.P_optim.step()\n",
    "\n",
    "        # print(\"logp:\", dists.log_prob(actions).unsqueeze(-1).shape, \"adv:\", advantages.shape,\n",
    "        #       \"std mean:\", stds.mean().item(),\n",
    "        #       \"logp mean:\", dists.log_prob(actions).unsqueeze(-1).mean().item())\n",
    "\n",
    "        for p in self.V_net.parameters():\n",
    "            p.requires_grad_(True)\n",
    "\n",
    "        return float(V_loss.item()), float(P_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62a5b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        env,\n",
    "        agent,\n",
    "        rollout_num=200,\n",
    "        rollout_len=256,\n",
    "):\n",
    "    print(\"cuda available:\", torch.cuda.is_available())\n",
    "    print(\"agent device:\", agent.device)\n",
    "    print(\"P_net device:\", next(agent.V_net.parameters()).device)\n",
    "    print(\"Q_net device:\", next(agent.P_net.parameters()).device)\n",
    "\n",
    "    reward_history = []\n",
    "    reward_log = 0\n",
    "    episode = 0\n",
    "\n",
    "    state, info = env.reset()\n",
    "\n",
    "    for r in range(rollout_num):\n",
    "        # ---- rollout バッファ（長さ rollout_len）を毎回作り直す ----\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        states_next = []\n",
    "        dones = []\n",
    "\n",
    "        for t in range(rollout_len):\n",
    "            with torch.no_grad():\n",
    "                action = agent.step(state)\n",
    "            state_next, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated\n",
    "\n",
    "            states.append(np.atleast_1d(state).tolist())\n",
    "            actions.append(np.atleast_1d(action).tolist())\n",
    "            rewards.append(np.atleast_1d(reward).tolist())\n",
    "            states_next.append(np.atleast_1d(state_next).tolist())\n",
    "            dones.append(np.atleast_1d(done).tolist())\n",
    "\n",
    "            reward_log += reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                state, info = env.reset()\n",
    "                episode += 1\n",
    "                logging.info('episode train %d: reward = %.2f', episode, reward_log)\n",
    "                reward_history.append(reward_log)\n",
    "                reward_log = 0\n",
    "            else:\n",
    "                state = state_next\n",
    "        \n",
    "        V_loss, P_loss = agent.update_net_batch(states,actions,rewards,states_next,dones)\n",
    "\n",
    "    return reward_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3a48d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "agent device: cuda\n",
      "P_net device: cuda:0\n",
      "Q_net device: cuda:0\n",
      "22:12:18 [INFO] episode train 1: reward = -1773.71\n",
      "22:12:24 [INFO] episode train 2: reward = -1795.53\n",
      "22:12:31 [INFO] episode train 3: reward = -1661.74\n",
      "22:12:38 [INFO] episode train 4: reward = -1167.24\n",
      "22:12:44 [INFO] episode train 5: reward = -1481.06\n",
      "22:12:51 [INFO] episode train 6: reward = -1035.51\n",
      "22:12:58 [INFO] episode train 7: reward = -1707.66\n",
      "22:13:04 [INFO] episode train 8: reward = -861.98\n",
      "22:13:11 [INFO] episode train 9: reward = -1075.98\n",
      "22:13:18 [INFO] episode train 10: reward = -1012.65\n",
      "22:13:24 [INFO] episode train 11: reward = -1061.22\n",
      "22:13:31 [INFO] episode train 12: reward = -1401.98\n",
      "22:13:38 [INFO] episode train 13: reward = -1045.42\n",
      "22:13:44 [INFO] episode train 14: reward = -1126.07\n",
      "22:13:51 [INFO] episode train 15: reward = -1230.00\n",
      "22:13:58 [INFO] episode train 16: reward = -939.40\n",
      "22:14:04 [INFO] episode train 17: reward = -986.38\n",
      "22:14:11 [INFO] episode train 18: reward = -1455.24\n",
      "22:14:18 [INFO] episode train 19: reward = -987.61\n",
      "22:14:24 [INFO] episode train 20: reward = -1020.72\n",
      "22:14:31 [INFO] episode train 21: reward = -1184.96\n",
      "22:14:37 [INFO] episode train 22: reward = -1271.11\n",
      "22:14:44 [INFO] episode train 23: reward = -1688.75\n",
      "22:14:51 [INFO] episode train 24: reward = -896.52\n",
      "22:14:57 [INFO] episode train 25: reward = -1154.73\n",
      "22:15:04 [INFO] episode train 26: reward = -1247.33\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m rollout_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m\n\u001b[1;32m      5\u001b[0m rollout_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m----> 7\u001b[0m rh \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrollout_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrollout_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrollout_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrollout_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, agent, rollout_num, rollout_len)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     28\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mstep(state)\n\u001b[0;32m---> 29\u001b[0m state_next, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated\n\u001b[1;32m     32\u001b[0m states\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39matleast_1d(state)\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/core.py:327\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/envs/classic_control/pendulum.py:146\u001b[0m, in \u001b[0;36mPendulumEnv.step\u001b[0;34m(self, u)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([newth, newthdot])\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_obs(), \u001b[38;5;241m-\u001b[39mcosts, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/envs/classic_control/pendulum.py:266\u001b[0m, in \u001b[0;36mPendulumEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# mode == \"rgb_array\":\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = ActorCriticAgent(Config=Config(),device=device)\n",
    "\n",
    "rollout_num=500\n",
    "rollout_len=100\n",
    "\n",
    "rh = train(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    rollout_num=rollout_num,\n",
    "    rollout_len=rollout_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20337b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774a60c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論用に eval モードにしておく（保存自体は train のままでも可）\n",
    "agent.mode2eval()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "agent.save_all(\n",
    "    \"./models/actor_critic_final_\" + stamp + \".pth\",\n",
    "    extra={\n",
    "        \"rollout_num\": rollout_num,\n",
    "        \"rollout_len\": rollout_len,\n",
    "        \"reward_history\": rh,  # 要らなければ外してOK\n",
    "    }\n",
    ")\n",
    "print(\"saved to actor_critic_final.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
