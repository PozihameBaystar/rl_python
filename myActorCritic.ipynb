{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86f0edc",
   "metadata": {},
   "source": [
    "# 自作のActor-Criticノートブック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e829d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from dataclasses import dataclass, asdict, is_dataclass\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from myActivator import tanhAndScale\n",
    "from myFunction import make_squashed_gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7040df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "                    stream=sys.stdout, datefmt=\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016d2cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\",render_mode=\"human\")\n",
    "for key in vars(env.spec):\n",
    "    logging.info('%s: %s', key, vars(env.spec)[key])\n",
    "for key in vars(env.unwrapped):\n",
    "    logging.info('%s: %s', key, vars(env.unwrapped)[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    V_net_sizes = [6,12,12,6]\n",
    "    P_net_sizes = [6,12,12,6]\n",
    "    V_net_in = 3\n",
    "    P_net_in = 3\n",
    "    V_net_out = 1\n",
    "    P_net_out = 2\n",
    "\n",
    "    V_lr = 1e-3\n",
    "    P_lr = 1e-3\n",
    "\n",
    "    u_high = 2.0\n",
    "    u_low = -2.0\n",
    "\n",
    "    gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5807268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n",
    "    def __init__(self,Config,device=None):\n",
    "        if Config:\n",
    "            self.Config = Config\n",
    "        else:\n",
    "            raise ValueError(\"No Config!!\")\n",
    "        \n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "\n",
    "        self.u_high = torch.as_tensor(Config.u_high, dtype=torch.float32, device=self.device)\n",
    "        self.u_low = torch.as_tensor(Config.u_low, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        self.V_net = self.build_net(\n",
    "            Config.V_net_in,\n",
    "            Config.V_net_sizes,\n",
    "            Config.V_net_out\n",
    "        ).to(self.device)\n",
    "        self.V_net.train()\n",
    "\n",
    "        self.P_net = self.build_net(\n",
    "            Config.P_net_in,\n",
    "            Config.P_net_sizes,\n",
    "            Config.P_net_out\n",
    "        ).to(self.device)\n",
    "        self.P_net.train()\n",
    "\n",
    "        self.V_optim = optim.Adam(self.V_net.parameters(),Config.V_lr)\n",
    "        self.P_optim = optim.Adam(self.P_net.parameters(),Config.P_lr)\n",
    "\n",
    "    \n",
    "    def to(self,device):\n",
    "        self.device = torch.device(device)\n",
    "        self.V_net.to(self.device)\n",
    "        self.P_net.to(self.device)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def build_net(self,input_size,hidden_sizes,output_size=1,output_activator=None):\n",
    "        layers = []\n",
    "        for input_size, output_size in zip([input_size]+hidden_sizes, hidden_sizes+[output_size]):\n",
    "            layers.append(nn.Linear(input_size,output_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers = layers[:-1]\n",
    "        if output_activator:\n",
    "            layers.append(output_activator)\n",
    "        net = nn.Sequential(*layers)\n",
    "        return net\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self,state):\n",
    "        state = torch.as_tensor(state,dtype=torch.float32,device=self.device)\n",
    "        out = self.P_net(state)\n",
    "        mu, log_std = torch.chunk(out, 2, dim=-1)\n",
    "        std = torch.exp(log_std)\n",
    "        dist = make_squashed_gaussian(mu=mu,std=std,low=self.u_low,high=self.u_high)\n",
    "        action = dist.rsample()\n",
    "        return action.squeeze(0).cpu().numpy()\n",
    "    \n",
    "\n",
    "    def save_all(self,path:str,extra:dict|None=None):\n",
    "        cfg = asdict(self.Config) if is_dataclass(self.Config) else self.Config\n",
    "        ckpt = {\n",
    "            \"Config\":cfg,\n",
    "            \"V_net\":self.V_net.state_dict(),\n",
    "            \"P_net\":self.P_net.state_dict(),\n",
    "        }\n",
    "        if extra is not None:\n",
    "            ckpt[\"extra\"] = extra\n",
    "        \n",
    "        torch.save(ckpt,path)\n",
    "\n",
    "    \n",
    "    def load_all(self,path:str,map_location=None):\n",
    "        ckpt = torch.load(path,map_location=map_location)\n",
    "        self.V_net.load_state_dict(ckpt[\"V_net\"])\n",
    "        self.P_net.load_state_dict(ckpt[\"P_net\"])\n",
    "\n",
    "        return ckpt.get(\"extra\",None)\n",
    "    \n",
    "\n",
    "    def mode2eval(self):\n",
    "        self.V_net.eval()\n",
    "        self.P_net.eval()\n",
    "\n",
    "\n",
    "    def mode2train(self):\n",
    "        self.V_net.train()\n",
    "        self.P_net.train()\n",
    "    \n",
    "\n",
    "    def update_net_batch(self,states,actions,rewards,states_next,dones):\n",
    "        states = torch.as_tensor(states,dtype=torch.float32,device=self.device)\n",
    "        actions = torch.as_tensor(actions,dtype=torch.float32,device=self.device)\n",
    "        rewards = torch.as_tensor(rewards,dtype=torch.float32,device=self.device)\n",
    "        states_next = torch.as_tensor(states_next,dtype=torch.float32,device=self.device)\n",
    "\n",
    "        if rewards.dim() == 1:\n",
    "            rewards = rewards.unsqueeze(1)\n",
    "        \n",
    "        if dones is None:\n",
    "            dones = torch.zeros((states.shape[0], 1), dtype=torch.float32, device=self.device)\n",
    "        else:\n",
    "            dones = torch.as_tensor(dones, dtype=torch.float32, device=self.device)\n",
    "            if dones.dim() == 1:\n",
    "                dones = dones.unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_targets = rewards+self.Config.gamma*(1-dones)*self.V_net(states_next)\n",
    "\n",
    "        V_values = self.V_net(states)\n",
    "        V_loss = F.mse_loss(y_targets,V_values)\n",
    "        self.V_optim.zero_grad()\n",
    "        V_loss.backward()\n",
    "        self.V_optim.step()\n",
    "\n",
    "        for p in self.V_net.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        outs = self.P_net(states)\n",
    "        mus, log_stds = torch.chunk(outs, 2, dim=-1)\n",
    "        stds = torch.exp(log_stds)\n",
    "        dists = make_squashed_gaussian(mu=mus,std=stds,low=self.u_low,high=self.u_high)\n",
    "        advantages = y_targets - V_values\n",
    "        P_loss = -(advantages*dists.log_prob(actions)).mean()\n",
    "        self.P_optim.zero_grad()\n",
    "        P_loss.backward()\n",
    "        self.P_optim.step()\n",
    "\n",
    "        for p in self.V_net.parameters():\n",
    "            p.requires_grad_(True)\n",
    "\n",
    "        return float(V_loss.item()), float(P_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a5b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        env,\n",
    "        agent,\n",
    "        rollout_num=200,\n",
    "        rollout_len=256,\n",
    "):\n",
    "    print(\"cuda available:\", torch.cuda.is_available())\n",
    "    print(\"agent device:\", agent.device)\n",
    "    print(\"P_net device:\", next(agent.V_net.parameters()).device)\n",
    "    print(\"Q_net device:\", next(agent.P_net.parameters()).device)\n",
    "\n",
    "    reward_history = []\n",
    "\n",
    "    state, info = env.reset()\n",
    "\n",
    "    for r in range(rollout_num):\n",
    "        # ---- rollout バッファ（長さ rollout_len）を毎回作り直す ----\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        states_next = []\n",
    "        dones = []\n",
    "        \n",
    "        reward_log = 0\n",
    "\n",
    "        for t in range(rollout_len):\n",
    "            with torch.no_grad():\n",
    "                action = agent.step(state)\n",
    "            state_next, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated\n",
    "\n",
    "            states.append(np.atleast_1d(state).tolist())\n",
    "            actions.append(np.atleast_1d(action).tolist())\n",
    "            rewards.append(np.atleast_1d(reward).tolist())\n",
    "            states_next.append(np.atleast_1d(state_next).tolist())\n",
    "            dones.append(np.atleast_1d(done).tolist())\n",
    "\n",
    "            if terminated or truncated:\n",
    "                state, info = env.reset()\n",
    "            else:\n",
    "                state = state_next\n",
    "\n",
    "            reward_log += reward\n",
    "        \n",
    "        V_loss, P_loss = agent.update_net_batch(states,actions,rewards,states_next,dones)\n",
    "\n",
    "        reward_history.append(reward_log)\n",
    "\n",
    "        logging.info('rollout %d: reward = %.2f', r, reward_log)\n",
    "\n",
    "    return reward_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3a48d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "agent device: cuda\n",
      "P_net device: cuda:0\n",
      "Q_net device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m rollout_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m\n\u001b[1;32m      5\u001b[0m rollout_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m----> 7\u001b[0m rh \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrollout_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrollout_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrollout_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrollout_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, agent, rollout_num, rollout_len)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ_net device:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mnext\u001b[39m(agent\u001b[38;5;241m.\u001b[39mP_net\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     12\u001b[0m reward_history \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 14\u001b[0m state, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(rollout_num):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# ---- rollout バッファ（長さ rollout_len）を毎回作り直す ----\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     states \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/common.py:146\u001b[0m, in \u001b[0;36mTimeLimit.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    The reset environment\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/core.py:333\u001b[0m, in \u001b[0;36mWrapper.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset\u001b[39m(\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, seed: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, options: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    332\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/common.py:400\u001b[0m, in \u001b[0;36mOrderEnforcing.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/core.py:333\u001b[0m, in \u001b[0;36mWrapper.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset\u001b[39m(\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, seed: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, options: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    332\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/common.py:293\u001b[0m, in \u001b[0;36mPassiveEnvChecker.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_reset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_reset_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(seed\u001b[38;5;241m=\u001b[39mseed, options\u001b[38;5;241m=\u001b[39moptions)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:185\u001b[0m, in \u001b[0;36menv_reset_passive_checker\u001b[0;34m(env, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdeprecation(\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m     )\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# Checks the result of env.reset with kwargs\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    188\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/envs/classic_control/pendulum.py:167\u001b[0m, in \u001b[0;36mPendulumEnv.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_obs(), {}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/envs/classic_control/pendulum.py:193\u001b[0m, in \u001b[0;36mPendulumEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpygame is not installed, run `pip install \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgymnasium[classic_control]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    190\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 193\u001b[0m     \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    195\u001b[0m         pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39minit()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = ActorCriticAgent(Config=Config(),device=device)\n",
    "\n",
    "rollout_num=200\n",
    "rollout_len=256\n",
    "\n",
    "rh = train(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    rollout_num=rollout_num,\n",
    "    rollout_len=rollout_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20337b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774a60c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論用に eval モードにしておく（保存自体は train のままでも可）\n",
    "agent.mode2eval()\n",
    "\n",
    "agent.save_all(\n",
    "    \"./models/actor_critic_final.pth\",\n",
    "    extra={\n",
    "        \"rollout_num\": rollout_num,\n",
    "        \"rollout_len\": rollout_len,\n",
    "        \"reward_history\": rh,  # 要らなければ外してOK\n",
    "    }\n",
    ")\n",
    "print(\"saved to actor_critic_final.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
