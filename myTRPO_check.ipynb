{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8437716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from dataclasses import dataclass, field, asdict, is_dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # ===== Pendulum-v1 specs =====\n",
    "    obs_dim: int = 3\n",
    "    act_dim: int = 1\n",
    "    u_llim: list[float] = field(default_factory=lambda: [-2.0])\n",
    "    u_ulim: list[float] = field(default_factory=lambda: [ 2.0])\n",
    "\n",
    "    # ===== Network architecture =====\n",
    "    V_net_in: int = 3\n",
    "    P_net_in: int = 3\n",
    "\n",
    "    # ★変更点1: Pendulum用にサイズを軽量化 (256->64)\n",
    "    # これで学習の立ち上がりが早くなり、検証しやすくなります\n",
    "    V_net_sizes: list[int] = field(default_factory=lambda: [64, 64])\n",
    "    P_net_sizes: list[int] = field(default_factory=lambda: [64, 64])\n",
    "\n",
    "    V_net_out: int = 1\n",
    "    P_net_out: int = 1\n",
    "\n",
    "    # ===== Optimizer =====\n",
    "    V_lr: float = 1e-3\n",
    "    P_lr: float = 3e-4  # TRPOでは未使用\n",
    "\n",
    "    # ===== GAE / discount =====\n",
    "    gamma: float = 0.99\n",
    "    lam: float = 0.97\n",
    "\n",
    "    # ===== TRPO hyperparameters =====\n",
    "    max_kl: float = 1e-2\n",
    "    cg_iters: int = 10\n",
    "    cg_damping: float = 0.1\n",
    "\n",
    "    ls_max_steps: int = 10\n",
    "    ls_backtrack: float = 0.8\n",
    "    ls_accept_ratio: float = 0.1\n",
    "\n",
    "    # ===== Value function training =====\n",
    "    value_train_iters: int = 20\n",
    "    value_l2_reg: float = 1e-3\n",
    "    \n",
    "    # ===== Training Loop =====\n",
    "    batch_steps: int = 5000\n",
    "    bootstrap_on_timeout: bool = False\n",
    "\n",
    "\n",
    "class TRPOAgent:\n",
    "    \"\"\"\n",
    "    v3/v4 検証用修正版\n",
    "    \n",
    "    変更点:\n",
    "    1. step() を deterministic=True (平均値使用) に変更\n",
    "    2. 念の為 update_net で actions の view(-1, 1) を追加 (事故防止)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, Config, device=None):\n",
    "        if Config is None:\n",
    "            raise ValueError(\"No Config!!\")\n",
    "        self.Config = Config\n",
    "\n",
    "        # device\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "\n",
    "        # action bounds\n",
    "        self.u_low = torch.as_tensor(Config.u_llim, dtype=torch.float32, device=self.device)\n",
    "        self.u_high = torch.as_tensor(Config.u_ulim, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # networks\n",
    "        self.V_net = self.build_net(Config.V_net_in, Config.V_net_sizes, Config.V_net_out).to(self.device)\n",
    "        self.P_net = self.build_net(Config.P_net_in, Config.P_net_sizes, Config.P_net_out).to(self.device)\n",
    "\n",
    "        self.V_net.train()\n",
    "        self.P_net.train()\n",
    "\n",
    "        # log_std (State-Independent Parameter)\n",
    "        action_dim = Config.P_net_out\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim, device=self.device))\n",
    "\n",
    "        # critic optimizer\n",
    "        self.V_optim = optim.Adam(self.V_net.parameters(), lr=Config.V_lr)\n",
    "\n",
    "        # hyperparams\n",
    "        self.gamma = float(Config.gamma)\n",
    "        self.tau = float(Config.lam)\n",
    "        self.max_kl = float(Config.max_kl)\n",
    "        self.cg_iters = int(Config.cg_iters)\n",
    "        self.cg_damping = float(Config.cg_damping)\n",
    "\n",
    "        self.value_train_iters = int(getattr(Config, \"value_train_iters\", 5))\n",
    "        self.value_l2_reg = float(getattr(Config, \"value_l2_reg\", 1e-3))\n",
    "        self.backtrack_coeff = float(getattr(Config, \"ls_backtrack\", 0.8))\n",
    "        self.backtrack_iters = int(getattr(Config, \"ls_max_steps\", 10))\n",
    "\n",
    "    def build_net(self, input_size, hidden_sizes, output_size):\n",
    "        layers = []\n",
    "        prev = input_size\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.Tanh())\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, output_size))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Policy helpers\n",
    "    # ------------------------------------------------------------\n",
    "    def _policy_mean(self, states: torch.Tensor) -> torch.Tensor:\n",
    "        return self.P_net(states)\n",
    "\n",
    "    def _policy_dist(self, states: torch.Tensor) -> Normal:\n",
    "        mean = self._policy_mean(states)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return Normal(mean, std)\n",
    "\n",
    "    def _log_prob(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n",
    "        dist = self._policy_dist(states)\n",
    "        return dist.log_prob(actions).sum(dim=-1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_action_and_log_prob(self, state, deterministic=False):\n",
    "        s = torch.as_tensor(state, dtype=torch.float32, device=self.device)\n",
    "        if s.dim() == 1:\n",
    "            s = s.unsqueeze(0)\n",
    "\n",
    "        dist = self._policy_dist(s)\n",
    "\n",
    "        if deterministic:\n",
    "            a = dist.mean\n",
    "            logp = None\n",
    "        else:\n",
    "            a = dist.sample()\n",
    "            logp = dist.log_prob(a).sum(dim=-1)\n",
    "\n",
    "        a = a.squeeze(0)\n",
    "        if logp is not None:\n",
    "            logp = logp.squeeze(0)\n",
    "        return a, logp\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, state):\n",
    "        \"\"\"\n",
    "        ★変更点2: ここを deterministic=True に変更。\n",
    "        これで「実力（平均値）」が出力されるようになります。\n",
    "        \"\"\"\n",
    "        a, _ = self.get_action_and_log_prob(state, deterministic=True)\n",
    "        return a.cpu().numpy()\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Flat params / grads\n",
    "    # ------------------------------------------------------------\n",
    "    def _flat_params(self) -> torch.Tensor:\n",
    "        return torch.cat([p.data.view(-1) for p in list(self.P_net.parameters()) + [self.log_std]])\n",
    "\n",
    "    def _set_flat_params(self, flat: torch.Tensor):\n",
    "        idx = 0\n",
    "        for p in self.P_net.parameters():\n",
    "            n = p.numel()\n",
    "            p.data.copy_(flat[idx:idx+n].view_as(p))\n",
    "            idx += n\n",
    "        n = self.log_std.numel()\n",
    "        self.log_std.data.copy_(flat[idx:idx+n].view_as(self.log_std))\n",
    "        idx += n\n",
    "\n",
    "    def _flat_grad(self, scalar: torch.Tensor, retain_graph=False, create_graph=False) -> torch.Tensor:\n",
    "        params = list(self.P_net.parameters()) + [self.log_std]\n",
    "        grads = torch.autograd.grad(\n",
    "            scalar, params,\n",
    "            retain_graph=retain_graph,\n",
    "            create_graph=create_graph,\n",
    "            allow_unused=False,\n",
    "        )\n",
    "        return torch.cat([g.contiguous().view(-1) for g in grads])\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # GAE\n",
    "    # ------------------------------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def _compute_gae(self, rewards, values, next_values, dones):\n",
    "        T = rewards.shape[0]\n",
    "        adv = torch.zeros_like(rewards)\n",
    "        gae = 0.0\n",
    "\n",
    "        for t in reversed(range(T)):\n",
    "            if t == T - 1:\n",
    "                nv = next_values[t]\n",
    "            else:\n",
    "                nv = values[t + 1]\n",
    "            delta = rewards[t] + self.gamma * nv * (1.0 - dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.tau * (1.0 - dones[t]) * gae\n",
    "            adv[t] = gae\n",
    "\n",
    "        ret = adv + values\n",
    "        return adv, ret\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # TRPO core\n",
    "    # ------------------------------------------------------------\n",
    "    def _conjugate_gradient(self, Avp_fn, b, n_iters=10, residual_tol=1e-10):\n",
    "        x = torch.zeros_like(b)\n",
    "        r = b.clone()\n",
    "        p = b.clone()\n",
    "        rdotr = torch.dot(r, r)\n",
    "\n",
    "        for _ in range(n_iters):\n",
    "            Ap = Avp_fn(p)\n",
    "            alpha = rdotr / (torch.dot(p, Ap) + 1e-8)\n",
    "            x = x + alpha * p\n",
    "            r = r - alpha * Ap\n",
    "            new_rdotr = torch.dot(r, r)\n",
    "            if new_rdotr < residual_tol:\n",
    "                break\n",
    "            beta = new_rdotr / (rdotr + 1e-12)\n",
    "            p = r + beta * p\n",
    "            rdotr = new_rdotr\n",
    "        return x\n",
    "\n",
    "    def _fisher_vector_product(self, states: torch.Tensor, v: torch.Tensor):\n",
    "        dist_new = self._policy_dist(states)\n",
    "        mean_old = dist_new.mean.detach()\n",
    "        std_old = dist_new.stddev.detach()\n",
    "        dist_old = Normal(mean_old, std_old)\n",
    "\n",
    "        kl = torch.distributions.kl_divergence(dist_old, dist_new).sum(dim=-1).mean()\n",
    "        kl_grad = self._flat_grad(kl, retain_graph=True, create_graph=True)\n",
    "        kl_grad_v = torch.dot(kl_grad, v)\n",
    "        hvp = self._flat_grad(kl_grad_v, retain_graph=True, create_graph=False)\n",
    "\n",
    "        return hvp + self.cg_damping * v\n",
    "\n",
    "    def _surrogate_loss(self, states, actions, advantages, old_log_probs):\n",
    "        new_logp = self._log_prob(states, actions)\n",
    "        ratio = torch.exp(new_logp - old_log_probs)\n",
    "        return -(ratio * advantages).mean()\n",
    "\n",
    "    def _trpo_step(self, states, actions, advantages, old_log_probs):\n",
    "        loss = self._surrogate_loss(states, actions, advantages, old_log_probs)\n",
    "        g = self._flat_grad(loss, retain_graph=True, create_graph=False)\n",
    "\n",
    "        def Fvp(v):\n",
    "            return self._fisher_vector_product(states, v)\n",
    "\n",
    "        step_dir = self._conjugate_gradient(Fvp, -g, n_iters=self.cg_iters)\n",
    "\n",
    "        shs = 0.5 * torch.dot(step_dir, Fvp(step_dir))\n",
    "        if shs.item() <= 0.0:\n",
    "            return False\n",
    "\n",
    "        lm = torch.sqrt(shs / self.max_kl)\n",
    "        full_step = step_dir / (lm + 1e-8)\n",
    "\n",
    "        old_params = self._flat_params()\n",
    "        old_loss = loss.item()\n",
    "\n",
    "        step_frac = 1.0\n",
    "        for _ in range(self.backtrack_iters):\n",
    "            new_params = old_params + step_frac * full_step\n",
    "            self._set_flat_params(new_params)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                new_loss = self._surrogate_loss(states, actions, advantages, old_log_probs).item()\n",
    "\n",
    "            if new_loss < old_loss:\n",
    "                return True\n",
    "\n",
    "            step_frac *= self.backtrack_coeff\n",
    "\n",
    "        self._set_flat_params(old_params)\n",
    "        return False\n",
    "\n",
    "    def _update_value_function(self, states, returns):\n",
    "        last_loss = None\n",
    "        for _ in range(self.value_train_iters):\n",
    "            v_pred = self.V_net(states).squeeze(-1)\n",
    "            v_loss = (v_pred - returns).pow(2).mean()\n",
    "\n",
    "            l2 = 0.0\n",
    "            for p in self.V_net.parameters():\n",
    "                l2 = l2 + p.pow(2).sum()\n",
    "            v_loss = v_loss + self.value_l2_reg * l2\n",
    "\n",
    "            self.V_optim.zero_grad()\n",
    "            v_loss.backward()\n",
    "            self.V_optim.step()\n",
    "\n",
    "            last_loss = v_loss.item()\n",
    "        return last_loss\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Update API\n",
    "    # ------------------------------------------------------------\n",
    "    def update_net(self, states, actions, log_probs, rewards, states_next, dones):\n",
    "        states = torch.as_tensor(states, dtype=torch.float32, device=self.device)\n",
    "        states_next = torch.as_tensor(states_next, dtype=torch.float32, device=self.device)\n",
    "        rewards = torch.as_tensor(rewards, dtype=torch.float32, device=self.device).view(-1)\n",
    "        dones = torch.as_tensor(dones, dtype=torch.float32, device=self.device).view(-1)\n",
    "\n",
    "        # ★念の為の保険: Broadcasting事故防止\n",
    "        actions = torch.as_tensor(actions, dtype=torch.float32, device=self.device).view(-1, self.Config.act_dim)\n",
    "\n",
    "        if isinstance(log_probs, (list, tuple)):\n",
    "            # 【ここを修正】torch.stack ではなく torch.tensor を使う\n",
    "            # torch.stack(log_probs) -> エラー (中身がfloatだから)\n",
    "            # torch.tensor(log_probs) -> 正解\n",
    "            old_log_probs = torch.tensor(log_probs, dtype=torch.float32, device=self.device).view(-1)\n",
    "        else:\n",
    "            old_log_probs = torch.as_tensor(log_probs, dtype=torch.float32, device=self.device).view(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            values = self.V_net(states).squeeze(-1)\n",
    "            next_values = self.V_net(states_next).squeeze(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            advantages, returns = self._compute_gae(rewards, values, next_values, dones)\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        trpo_ok = self._trpo_step(states, actions, advantages, old_log_probs)\n",
    "        v_loss = self._update_value_function(states, returns)\n",
    "\n",
    "        return {\"V_loss\": v_loss, \"trpo_ok\": trpo_ok}\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Misc\n",
    "    # ------------------------------------------------------------\n",
    "    def to(self, device):\n",
    "        self.device = torch.device(device)\n",
    "        self.V_net.to(self.device)\n",
    "        self.P_net.to(self.device)\n",
    "        self.log_std.data = self.log_std.data.to(self.device)\n",
    "        return self\n",
    "\n",
    "    def mode2eval(self):\n",
    "        self.V_net.eval()\n",
    "        self.P_net.eval()\n",
    "\n",
    "    def mode2train(self):\n",
    "        self.V_net.train()\n",
    "        self.P_net.train()\n",
    "\n",
    "    def save_all(self, path: str, extra: dict | None = None):\n",
    "        cfg = asdict(self.Config) if is_dataclass(self.Config) else self.Config\n",
    "        save_dict = {\n",
    "            \"Config\": cfg,\n",
    "            \"V_net_state_dict\": self.V_net.state_dict(),\n",
    "            \"P_net_state_dict\": self.P_net.state_dict(),\n",
    "            \"log_std\": self.log_std.data,\n",
    "        }\n",
    "        if extra is not None:\n",
    "            save_dict.update(extra)\n",
    "        torch.save(save_dict, path)\n",
    "        \n",
    "    def load_all(self, path: str, map_location=None):\n",
    "        load_dict = torch.load(path, map_location=map_location)\n",
    "        self.V_net.load_state_dict(load_dict[\"V_net_state_dict\"])\n",
    "        self.P_net.load_state_dict(load_dict[\"P_net_state_dict\"])\n",
    "        self.log_std.data = load_dict[\"log_std\"].to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbe2e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03:10:15 [INFO] Start Training: Env=Pendulum-v1, Batch=5000, Device=cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03:10:25 [INFO] Update   1 | Eval Score: -1272.93 (True Skill) | Train Score: -1526.99 (Noisy) | V_Loss: 33155.3477\n",
      "03:10:36 [INFO] Update   2 | Eval Score: -1308.81 (True Skill) | Train Score: -1255.34 (Noisy) | V_Loss: 26219.9844\n",
      "03:10:46 [INFO] Update   3 | Eval Score:  -998.92 (True Skill) | Train Score: -1168.16 (Noisy) | V_Loss: 25213.6934\n",
      "03:10:57 [INFO] Update   4 | Eval Score:  -868.22 (True Skill) | Train Score: -1210.66 (Noisy) | V_Loss: 19499.9453\n",
      "03:11:07 [INFO] Update   5 | Eval Score: -1047.40 (True Skill) | Train Score: -1190.00 (Noisy) | V_Loss: 21980.7754\n",
      "03:11:16 [INFO] Update   6 | Eval Score: -1106.48 (True Skill) | Train Score: -1070.02 (Noisy) | V_Loss: 16576.6719\n",
      "03:11:27 [INFO] Update   7 | Eval Score: -1019.76 (True Skill) | Train Score:  -996.95 (Noisy) | V_Loss: 16764.0293\n",
      "03:11:37 [INFO] Update   8 | Eval Score: -1120.99 (True Skill) | Train Score: -1083.86 (Noisy) | V_Loss: 18692.4316\n",
      "03:11:48 [INFO] Update   9 | Eval Score:  -901.69 (True Skill) | Train Score: -1235.30 (Noisy) | V_Loss: 19499.1934\n",
      "03:12:00 [INFO] Update  10 | Eval Score: -1020.30 (True Skill) | Train Score:  -959.57 (Noisy) | V_Loss: 14246.7188\n",
      "03:12:10 [INFO] Update  11 | Eval Score: -1055.85 (True Skill) | Train Score: -1002.14 (Noisy) | V_Loss: 16343.3535\n",
      "03:12:20 [INFO] Update  12 | Eval Score:  -861.07 (True Skill) | Train Score:  -995.61 (Noisy) | V_Loss: 13612.2783\n",
      "03:12:31 [INFO] Update  13 | Eval Score:  -914.75 (True Skill) | Train Score:  -894.25 (Noisy) | V_Loss: 14560.8926\n",
      "03:12:41 [INFO] Update  14 | Eval Score:  -902.11 (True Skill) | Train Score:  -881.30 (Noisy) | V_Loss: 11793.2832\n",
      "03:12:50 [INFO] Update  15 | Eval Score:  -816.46 (True Skill) | Train Score:  -799.51 (Noisy) | V_Loss: 10021.0039\n",
      "03:13:00 [INFO] Update  16 | Eval Score:  -839.04 (True Skill) | Train Score:  -900.86 (Noisy) | V_Loss: 11078.1699\n",
      "03:13:11 [INFO] Update  17 | Eval Score:  -784.19 (True Skill) | Train Score:  -875.42 (Noisy) | V_Loss: 11544.4492\n",
      "03:13:22 [INFO] Update  18 | Eval Score:  -718.33 (True Skill) | Train Score:  -889.21 (Noisy) | V_Loss: 11604.5068\n",
      "03:13:32 [INFO] Update  19 | Eval Score:  -697.40 (True Skill) | Train Score:  -816.82 (Noisy) | V_Loss: 9784.2969\n",
      "03:13:42 [INFO] Update  20 | Eval Score:  -601.11 (True Skill) | Train Score:  -746.07 (Noisy) | V_Loss: 8537.4004\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# ログ設定（標準出力に見やすく出す）\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    datefmt='%H:%M:%S',\n",
    "    stream=sys.stdout\n",
    ")\n",
    "\n",
    "def evaluate(env, agent, n_episodes=3):\n",
    "    \"\"\"\n",
    "    検証の本丸：決定論的（平均値）な行動で真の実力を測る\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        score = 0.0\n",
    "        while not done:\n",
    "            # 修正版Agentの step() は deterministic=True になっているので、\n",
    "            # ここを呼ぶだけで「ノイズなし」の行動が出る\n",
    "            action = agent.step(obs)\n",
    "            \n",
    "            # EnvのAction Spaceに合わせてクリップ\n",
    "            action = np.clip(action, -2.0, 2.0)\n",
    "            \n",
    "            obs, rew, term, trunc, _ = env.step(action)\n",
    "            score += rew\n",
    "            done = term or trunc\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "\n",
    "def train_trpo(\n",
    "    env_name,\n",
    "    agent,\n",
    "    total_steps=100_000,  # Pendulumなら10万ステップもあれば十分収束します\n",
    "    log_interval=1        # 何回の更新ごとにログを出すか\n",
    "):\n",
    "    # 環境作成\n",
    "    env = gym.make(env_name)\n",
    "    # 評価用環境（学習用とは別インスタンスにするのが作法）\n",
    "    eval_env = gym.make(env_name)\n",
    "\n",
    "    batch_steps = agent.Config.batch_steps\n",
    "    logging.info(f\"Start Training: Env={env_name}, Batch={batch_steps}, Device={agent.device}\")\n",
    "\n",
    "    # ロールアウトバッファ\n",
    "    rollout = {\"obs\": [], \"act\": [], \"logp\": [], \"rew\": [], \"obs_next\": [], \"done\": []}\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    episode_reward = 0.0\n",
    "    episodes_completed = 0\n",
    "    updates_completed = 0\n",
    "    \n",
    "    # 学習中の生スコア推移（ノイズあり）\n",
    "    train_score_history = []\n",
    "\n",
    "    for t in range(1, total_steps + 1):\n",
    "        \n",
    "        # --- (1) 行動選択（データ収集フェーズ） ---\n",
    "        # 重要：学習データ集めなので、あえて deterministic=False (確率的) にする。\n",
    "        # v3/v4はここが step() と混同されていたのが敗因でした。\n",
    "        with torch.no_grad():\n",
    "            action_tensor, logp_tensor = agent.get_action_and_log_prob(obs, deterministic=False)\n",
    "        \n",
    "        action_np = action_tensor.cpu().numpy()     # (1,)\n",
    "        logp_val = logp_tensor.item()               # float\n",
    "        \n",
    "        # クリップして環境へ\n",
    "        action_env = np.clip(action_np, -2.0, 2.0)\n",
    "        \n",
    "        # --- (2) 環境ステップ ---\n",
    "        obs_next, reward, terminated, truncated, _ = env.step(action_env)\n",
    "        done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "\n",
    "        # --- (3) データ保存 ---\n",
    "        rollout[\"obs\"].append(obs)\n",
    "        rollout[\"act\"].append(action_np) # クリップ前を保存\n",
    "        rollout[\"logp\"].append(logp_val)\n",
    "        rollout[\"rew\"].append(reward)\n",
    "        rollout[\"obs_next\"].append(obs_next)\n",
    "        rollout[\"done\"].append(float(terminated)) # GAE用（truncatedはdone扱いしないのが一般的）\n",
    "\n",
    "        # --- (4) エピソード完了処理 ---\n",
    "        if done:\n",
    "            episodes_completed += 1\n",
    "            train_score_history.append(episode_reward)\n",
    "            episode_reward = 0.0\n",
    "            obs, _ = env.reset()\n",
    "        else:\n",
    "            obs = obs_next\n",
    "\n",
    "        # --- (5) TRPO更新（バッチが溜まったら） ---\n",
    "        if len(rollout[\"obs\"]) >= batch_steps:\n",
    "            updates_completed += 1\n",
    "            \n",
    "            # NumPy配列化\n",
    "            states = np.array(rollout[\"obs\"], dtype=np.float32)\n",
    "            actions = np.array(rollout[\"act\"], dtype=np.float32)\n",
    "            # log_probs は float のリストなので、Agent側で Tensor化してもらう\n",
    "            log_probs = rollout[\"logp\"] \n",
    "            rewards = np.array(rollout[\"rew\"], dtype=np.float32)\n",
    "            states_next = np.array(rollout[\"obs_next\"], dtype=np.float32)\n",
    "            dones = np.array(rollout[\"done\"], dtype=np.float32)\n",
    "            \n",
    "            # 更新実行\n",
    "            result = agent.update_net(states, actions, log_probs, rewards, states_next, dones)\n",
    "            \n",
    "            # バッファクリア\n",
    "            for k in rollout: rollout[k].clear()\n",
    "            \n",
    "            # --- (6) 評価とログ ---\n",
    "            if updates_completed % log_interval == 0:\n",
    "                # ★ここが検証の肝！\n",
    "                # ノイズなしの決定論的ポリシーで評価する\n",
    "                eval_score = evaluate(eval_env, agent, n_episodes=3)\n",
    "                \n",
    "                # 直近の学習中スコア（ノイズあり）の平均\n",
    "                train_score_avg = np.mean(train_score_history[-10:]) if train_score_history else -9999\n",
    "                \n",
    "                logging.info(\n",
    "                    f\"Update {updates_completed:3d} | \"\n",
    "                    f\"Eval Score: {eval_score:8.2f} (True Skill) | \"\n",
    "                    f\"Train Score: {train_score_avg:8.2f} (Noisy) | \"\n",
    "                    f\"V_Loss: {result['V_loss']:.4f}\"\n",
    "                )\n",
    "                \n",
    "                # Pendulum-v1 は -200 以上なら概ねクリア\n",
    "                if eval_score > -200:\n",
    "                    logging.info(\">>> Solved! (Verification Successful)\")\n",
    "                    # break # 検証完了ならここで止めてもOK\n",
    "\n",
    "# ============================================================\n",
    "# 実行ブロック\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 前回の修正版 Agent クラスと Config クラスをここでインスタンス化\n",
    "    \n",
    "    # 1. 設定 (検証用にネットワークを軽量化済み)\n",
    "    cfg = Config() \n",
    "    \n",
    "    # 2. Agent作成\n",
    "    # ここには前回の回答にある「検証用修正版 TRPOAgent」クラスを使ってください\n",
    "    agent = TRPOAgent(cfg)\n",
    "    \n",
    "    # 3. 学習開始\n",
    "    train_trpo(\"Pendulum-v1\", agent, total_steps=100_000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
