{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed7508b",
   "metadata": {},
   "source": [
    "# 自作のPPOノートブック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "41b2c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, asdict, is_dataclass\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from myActivator import tanhAndScale\n",
    "from myFunction import make_squashed_gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "25772e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "                    stream=sys.stdout, datefmt=\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fc437463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:54:26 [INFO] id: Pendulum-v1\n",
      "22:54:26 [INFO] entry_point: gymnasium.envs.classic_control.pendulum:PendulumEnv\n",
      "22:54:26 [INFO] reward_threshold: None\n",
      "22:54:26 [INFO] nondeterministic: False\n",
      "22:54:26 [INFO] max_episode_steps: 200\n",
      "22:54:26 [INFO] order_enforce: True\n",
      "22:54:26 [INFO] disable_env_checker: False\n",
      "22:54:26 [INFO] kwargs: {}\n",
      "22:54:26 [INFO] additional_wrappers: ()\n",
      "22:54:26 [INFO] vector_entry_point: None\n",
      "22:54:26 [INFO] namespace: None\n",
      "22:54:26 [INFO] name: Pendulum\n",
      "22:54:26 [INFO] version: 1\n",
      "22:54:27 [INFO] max_speed: 8\n",
      "22:54:27 [INFO] max_torque: 2.0\n",
      "22:54:27 [INFO] dt: 0.05\n",
      "22:54:27 [INFO] g: 10.0\n",
      "22:54:27 [INFO] m: 1.0\n",
      "22:54:27 [INFO] l: 1.0\n",
      "22:54:27 [INFO] render_mode: None\n",
      "22:54:27 [INFO] screen_dim: 500\n",
      "22:54:27 [INFO] screen: None\n",
      "22:54:27 [INFO] clock: None\n",
      "22:54:27 [INFO] isopen: True\n",
      "22:54:27 [INFO] action_space: Box(-2.0, 2.0, (1,), float32)\n",
      "22:54:27 [INFO] observation_space: Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n",
      "22:54:27 [INFO] spec: EnvSpec(id='Pendulum-v1', entry_point='gymnasium.envs.classic_control.pendulum:PendulumEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, disable_env_checker=True, kwargs={}, namespace=None, name='Pendulum', version=1, additional_wrappers=(), vector_entry_point=None)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "for key in vars(env.spec):\n",
    "    logging.info('%s: %s', key, vars(env.spec)[key])\n",
    "for key in vars(env.unwrapped):\n",
    "    logging.info('%s: %s', key, vars(env.unwrapped)[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2e5818b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8dd64991",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # ===== Pendulum-v1 specs =====\n",
    "    obs_dim: int = 3\n",
    "    act_dim: int = 1\n",
    "\n",
    "    # TRPOAgent 側は Config.u_llim / Config.u_ulim を参照\n",
    "    u_llim: list[float] = field(default_factory=lambda: [-2.0])\n",
    "    u_ulim: list[float] = field(default_factory=lambda: [ 2.0])\n",
    "\n",
    "    # ===== Network architecture =====\n",
    "    V_net_in: int = 3\n",
    "    P_net_in: int = 3\n",
    "\n",
    "    V_net_sizes: list[int] = field(default_factory=lambda: [64, 64])\n",
    "    P_net_sizes: list[int] = field(default_factory=lambda: [64, 64])\n",
    "\n",
    "    V_net_out: int = 1\n",
    "    P_net_out: int = 1  # = act_dim\n",
    "\n",
    "    # ===== Optimizer =====\n",
    "    V_lr: float = 1e-3\n",
    "    P_lr: float = 3e-4\n",
    "\n",
    "    # ===== GAE / discount =====\n",
    "    gamma: float = 0.99\n",
    "    lam: float = 0.97\n",
    "\n",
    "    # ===== PPO hyperparameters =====\n",
    "    clip_ratio: float = 0.2\n",
    "    policy_train_iters: int = 50\n",
    "    target_kl: float = 0.01\n",
    "    reward_scaling: float = 0.01\n",
    "\n",
    "    # ===== Value function training =====\n",
    "    value_train_iters: int = 50\n",
    "    value_l2_reg: float = 1e-3\n",
    "    v_clip_epsilon: float = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "25aa0a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, Config, device=None):\n",
    "        if Config is None:\n",
    "            raise ValueError(\"No Config!!\")\n",
    "        self.Config = Config\n",
    "\n",
    "        # device\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "\n",
    "        self.u_low = torch.tensor(Config.u_llim, dtype=torch.float32, device=self.device)\n",
    "        self.u_high = torch.tensor(Config.u_ulim, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # networks\n",
    "        self.V_net = self.build_net(Config.V_net_in, Config.V_net_sizes, Config.V_net_out).to(self.device)\n",
    "        self.P_net = self.build_net(Config.P_net_in, Config.P_net_sizes, Config.P_net_out).to(self.device)\n",
    "\n",
    "        self.V_net.train()\n",
    "        self.P_net.train()\n",
    "\n",
    "        # log_std は状態に依存しないパラメータ\n",
    "        action_dim = Config.P_net_out\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim, device=self.device))\n",
    "\n",
    "        # optimizer（baselineはAdam）\n",
    "        self.V_optim = optim.Adam(self.V_net.parameters(), lr=Config.V_lr)\n",
    "        self.P_optim = optim.Adam(\n",
    "            list(self.P_net.parameters()) + [self.log_std], \n",
    "            lr=Config.P_lr\n",
    "        )\n",
    "\n",
    "        # hyperparams\n",
    "        self.gamma = float(Config.gamma)\n",
    "        self.tau = float(Config.lam)  # baselineの TAU (= GAE lambda)\n",
    "        self.target_kl = float(getattr(Config, \"target_kl\", 0.01))\n",
    "        self.reward_scaling = float(getattr(Config, \"reward_scaling\", 0.01))\n",
    "\n",
    "        self.policy_train_iters = int(getattr(Config, \"policy_train_iters\", 80))\n",
    "        self.value_train_iters = int(getattr(Config, \"value_train_iters\", 5))\n",
    "        self.value_l2_reg = float(getattr(Config, \"value_l2_reg\", 1e-3))\n",
    "        self.v_clip_epsilon = float(getattr(Config, \"v_clip_epsilon\", 0.2))\n",
    "\n",
    "    def build_net(self, input_size, hidden_sizes, output_size):\n",
    "        layers = []\n",
    "        in_size = input_size\n",
    "        for h_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_size, h_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_size = h_size\n",
    "        layers.append(nn.Linear(in_size, output_size))\n",
    "        net = nn.Sequential(*layers)\n",
    "        return net\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_action_and_log_prob(self, state, deterministic=False):\n",
    "        \"\"\"\n",
    "        deterministic: Trueなら平均値(mean)を返す。Falseならサンプリング。\n",
    "        \"\"\"\n",
    "        s = torch.as_tensor(state, dtype=torch.float32, device=self.device)\n",
    "        if s.dim() == 1:\n",
    "            s = s.unsqueeze(0)  # (1, obs_dim)\n",
    "\n",
    "        dist = self._policy_dist(s)  # Normal(mean, std)\n",
    "\n",
    "        if deterministic:\n",
    "            a = dist.mean  # (1, act_dim)\n",
    "            logp = None\n",
    "        else:\n",
    "            a = dist.sample()\n",
    "            logp = dist.log_prob(a).sum(axis=-1)  # (1,)\n",
    "\n",
    "        # 返すactionはclip前\n",
    "        # envに入れるときにnp.clipする\n",
    "        a = a.squeeze(0)\n",
    "        if logp is not None:\n",
    "            logp = logp.squeeze(0)\n",
    "        return a, logp\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self, state):\n",
    "        \"\"\"\n",
    "        推論時に使う用のwrapper関数\n",
    "        \"\"\"\n",
    "        a, _ = self.get_action_and_log_prob(state, deterministic=True)\n",
    "        return a.cpu().numpy()\n",
    "    \n",
    "    def _policy_mean(self, states):\n",
    "        \"\"\"\n",
    "        方策ネットから行動平均を計算するラッパー関数\n",
    "        states: (batch_size, obs_dim)\n",
    "        return: (batch_size, act_dim)\n",
    "        \"\"\"\n",
    "        mean = self.P_net(states)  # (batch_size, act_dim)\n",
    "        return mean\n",
    "    \n",
    "    def _policy_dist(self, states):\n",
    "        \"\"\"\n",
    "        方策ネットから平均を計算し、パラメータから分散を計算して、正規分布を返すラッパー関数\n",
    "        states: (batch_size, obs_dim)\n",
    "        return: Normal distribution\n",
    "        \"\"\"\n",
    "        mean = self._policy_mean(states)  # (batch_size, act_dim)\n",
    "        std = torch.exp(self.log_std)  # (act_dim,)\n",
    "        std = std.unsqueeze(0).expand_as(mean)  # (batch_size, act_dim)\n",
    "        dist = Normal(mean, std)\n",
    "        return dist\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _compute_gae(self, rewards, values, next_values, dones):\n",
    "        \"\"\"\n",
    "        GAEを計算する関数\n",
    "        rewards: (batch_size,)\n",
    "        values: (batch_size,)\n",
    "        next_values: (batch_size,)\n",
    "        dones: (batch_size,)\n",
    "        return: advantages: (batch_size,), returns: (batch_size,)\n",
    "        \"\"\"\n",
    "        batch_size = rewards.shape[0]\n",
    "        adv = torch.zeros_like(rewards, device=self.device)\n",
    "        gae = 0.0\n",
    "\n",
    "        for t in reversed(range(batch_size)):\n",
    "            if t == batch_size - 1:\n",
    "                nv = next_values[t]\n",
    "            else:\n",
    "                nv = values[t + 1]\n",
    "            delta = rewards[t] + self.gamma * nv * (1 - dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.tau * (1 - dones[t]) * gae\n",
    "            adv[t] = gae\n",
    "\n",
    "        ret = adv + values\n",
    "        return adv, ret\n",
    "    \n",
    "    def _ppo_step(self, states, actions, old_log_probs, advantages):\n",
    "        \"\"\"\n",
    "        PPOの方策ネット更新を行う関数\n",
    "        states: (batch_size, obs_dim)\n",
    "        actions: (batch_size, act_dim)\n",
    "        old_log_probs: (batch_size,)\n",
    "        advantages: (batch_size,)\n",
    "        return: policy_loss\n",
    "        \"\"\"\n",
    "\n",
    "        for _ in range(self.policy_train_iters):\n",
    "            dist = self._policy_dist(states)  # Normal(mean, std)\n",
    "            log_probs = dist.log_prob(actions).sum(axis=-1)  # (batch_size,)\n",
    "\n",
    "            ratios = torch.exp(log_probs - old_log_probs)  # (batch_size,)\n",
    "\n",
    "            surr1 = ratios * advantages  # (batch_size,)\n",
    "            surr2 = torch.clamp(ratios, 1.0 - self.Config.clip_ratio, 1.0 + self.Config.clip_ratio) * advantages  # (batch_size,)\n",
    "\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            self.P_optim.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.P_optim.step()\n",
    "\n",
    "        # どれくらい変化したかを確認する\n",
    "        change = (old_log_probs - log_probs).mean()\n",
    "\n",
    "        return policy_loss, change\n",
    "    \n",
    "    def _update_value_function(self, states, returns, old_values):\n",
    "        \"\"\"\n",
    "        価値関数ネットワークの更新を行う関数\n",
    "        states: (batch_size, obs_dim)\n",
    "        returns: (batch_size,)\n",
    "        return: value_loss\n",
    "        \"\"\"\n",
    "        for _ in range(self.value_train_iters):\n",
    "            values = self.V_net(states).squeeze(-1)  # (batch_size,)\n",
    "            value_loss = F.mse_loss(values, returns)\n",
    "\n",
    "            # クリッピング版の価値関数損失（Vの計算が暴走するのを防ぐため）\n",
    "            v_clip = old_values + torch.clamp(values - old_values, self.v_clip_epsilon, self.v_clip_epsilon)\n",
    "            v_clip_loss = F.mse_loss(v_clip, returns)\n",
    "\n",
    "            # L2正則化\n",
    "            l2_reg = torch.tensor(0., device=self.device)\n",
    "            for param in self.V_net.parameters():\n",
    "                l2_reg += torch.norm(param)**2\n",
    "            loss = torch.max(value_loss, v_clip_loss) + self.value_l2_reg * l2_reg\n",
    "            # loss = value_loss + self.value_l2_reg * l2_reg\n",
    "\n",
    "            self.V_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            self.V_optim.step()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def update_net(self, states, actions, log_probs, rewards, next_states, dones):\n",
    "        \"\"\"\n",
    "        ネットワークを更新する関数\n",
    "        states: (batch_size, obs_dim)\n",
    "        actions: (batch_size, act_dim)\n",
    "        log_probs: (batch_size,)\n",
    "        rewards: (batch_size,)\n",
    "        next_states: (batch_size, obs_dim)\n",
    "        dones: (batch_size,)\n",
    "        return: dict of losses\n",
    "        \"\"\"\n",
    "        states = torch.as_tensor(states, dtype=torch.float32, device=self.device)\n",
    "        actions = torch.as_tensor(actions, dtype=torch.float32, device=self.device)\n",
    "        log_probs = torch.as_tensor(log_probs, dtype=torch.float32, device=self.device)\n",
    "        rewards = torch.as_tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        next_states = torch.as_tensor(next_states, dtype=torch.float32, device=self.device)\n",
    "        dones = torch.as_tensor(dones, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # 一応shapeを揃えておく\n",
    "        actions = torch.as_tensor(actions, dtype=torch.float32, device=self.device).view(-1, self.Config.act_dim)\n",
    "\n",
    "        # old_log_probs も念のためshapeをそろえてdetachしておく\n",
    "        old_log_probs = torch.as_tensor(log_probs, dtype=torch.float32, device=self.device).view(-1).detach()\n",
    "\n",
    "        # GAEの計算\n",
    "        with torch.no_grad():\n",
    "            values = self.V_net(states).squeeze(-1)  # (batch_size,)\n",
    "            next_values = self.V_net(next_states).squeeze(-1)  # (batch_size,)\n",
    "\n",
    "            advantages, returns = self._compute_gae(rewards, values, next_values, dones)\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)  # 正規化\n",
    "\n",
    "        # 方策ネットワークの更新\n",
    "        policy_loss, kl_change = self._ppo_step(states, actions, old_log_probs, advantages)\n",
    "\n",
    "        # 変化量に応じて学習率を調整\n",
    "        if kl_change > 1.5 * self.target_kl:\n",
    "            for param_group in self.P_optim.param_groups:\n",
    "                param_group['lr'] = max(param_group['lr'] / 1.5, 1e-5)\n",
    "                logging.info(f\"Decreased policy learning rate to {param_group['lr']}\")\n",
    "        elif kl_change < self.target_kl / 1.5:\n",
    "            for param_group in self.P_optim.param_groups:\n",
    "                param_group['lr'] = min(param_group['lr'] * 1.5, 1e-2)\n",
    "                logging.info(f\"Increased policy learning rate to {param_group['lr']}\")\n",
    "\n",
    "        # 価値観数ネットワークの更新\n",
    "        value_loss = self._update_value_function(states, returns, values)\n",
    "\n",
    "        return {\"policy_loss\": policy_loss.item(), \"value_loss\": value_loss.item()}\n",
    "    \n",
    "    def to(self, device):\n",
    "        self.device = torch.device(device)\n",
    "        self.V_net.to(self.device)\n",
    "        self.P_net.to(self.device)\n",
    "        self.log_std.data = self.log_std.data.to(self.device)\n",
    "        self.u_low = self.u_low.to(self.device)\n",
    "        self.u_high = self.u_high.to(self.device)\n",
    "        return self\n",
    "\n",
    "    def mode2eval(self):\n",
    "        self.V_net.eval()\n",
    "        self.P_net.eval()\n",
    "\n",
    "    def mode2train(self):\n",
    "        self.V_net.train()\n",
    "        self.P_net.train()\n",
    "\n",
    "    def save_all(self, path: str, extra: dict | None = None):\n",
    "        cfg = asdict(self.Config) if is_dataclass(self.Config) else self.Config\n",
    "        save_dict = {\n",
    "            \"config\": cfg,\n",
    "            \"V_net_state_dict\": self.V_net.state_dict(),\n",
    "            \"P_net_state_dict\": self.P_net.state_dict(),\n",
    "            \"log_std\": self.log_std.data,\n",
    "        }\n",
    "        if extra is not None:\n",
    "            save_dict.update(extra)\n",
    "        torch.save(save_dict, path)\n",
    "        \n",
    "    def load_all(self, path: str, map_location=None):\n",
    "        load_dict = torch.load(path, map_location=map_location)\n",
    "        self.V_net.load_state_dict(load_dict[\"V_net_state_dict\"])\n",
    "        self.P_net.load_state_dict(load_dict[\"P_net_state_dict\"])\n",
    "        self.log_std.data = load_dict[\"log_std\"].to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "71815088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "def train_ppo(\n",
    "    env,\n",
    "    agent,\n",
    "    total_step: int = 200_000,\n",
    "    batch_steps: int | None = None,\n",
    "    random_steps: int = 0,\n",
    "    bootstrap_on_timeout: bool | None = None,\n",
    "    log_interval_updates: int = 1,\n",
    "):\n",
    "    \"\"\"\n",
    "    PPO Agentのための学習ループ\n",
    "    \"\"\"\n",
    "    if batch_steps is None:\n",
    "        batch_steps = int(getattr(agent.Config, \"batch_steps\", 2048)) # PPOはバッチサイズ大きめが一般的\n",
    "    if bootstrap_on_timeout is None:\n",
    "        bootstrap_on_timeout = bool(getattr(agent.Config, \"bootstrap_on_timeout\", False))\n",
    "\n",
    "    print(f\"Start PPO Training: Device={agent.device}, Batch={batch_steps}\")\n",
    "\n",
    "    # 変数名修正に伴う変更\n",
    "    low_np = agent.u_low.detach().cpu().numpy()\n",
    "    high_np = agent.u_high.detach().cpu().numpy()\n",
    "\n",
    "    # 履歴保存用\n",
    "    loss_history = {\"policy_loss\": [], \"value_loss\": []}\n",
    "    train_reward_history = [] # ノイズあり（学習中の報酬）\n",
    "\n",
    "    # rollout buffer\n",
    "    rollout = {\"obs\": [], \"act\": [], \"logp\": [], \"rew\": [], \"obs_next\": [], \"done\": []}\n",
    "\n",
    "    def rollout_clear():\n",
    "        for k in rollout:\n",
    "            rollout[k].clear()\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    ep_return = 0.0\n",
    "    episode_num = 0\n",
    "    update_num = 0\n",
    "\n",
    "    for t in range(total_step):\n",
    "        # --- (1) 行動選択 ---\n",
    "        # ランダムステップ期間、もしくは学習初期\n",
    "        if t < random_steps:\n",
    "            action_raw = np.atleast_1d(env.action_space.sample()).astype(np.float32)\n",
    "            # ランダム行動の場合のlogpは適当(0.0)あるいは計算不要だが、\n",
    "            # PPOの更新で使うなら整合性を取るためにAgentからサンプリングした方が無難。\n",
    "            # ここではあくまで「完全ランダム」として扱うため、logp=0としてUpdateに使わない手もあるが、\n",
    "            # 実装を単純にするため、random_steps期間はバッファに入れないか、\n",
    "            # もしくはagentを使ってサンプリングする形が推奨されます。\n",
    "            # 今回は「agentを使う」形に倒します。\n",
    "            with torch.no_grad():\n",
    "                a_t, logp_t = agent.get_action_and_log_prob(obs, deterministic=False)\n",
    "                logp_val = logp_t.item()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # deterministic=False (確率的方策に従って探索)\n",
    "                a_t, logp_t = agent.get_action_and_log_prob(obs, deterministic=False)\n",
    "            \n",
    "            action_raw = np.atleast_1d(a_t.detach().cpu().numpy()).astype(np.float32)\n",
    "            logp_val = logp_t.item()\n",
    "\n",
    "        # --- (2) env step ---\n",
    "        # クリップして環境に入力\n",
    "        action_env = np.clip(action_raw, low_np, high_np)\n",
    "        obs_next, reward, terminated, truncated, info = env.step(action_env)\n",
    "\n",
    "        ep_return += float(reward)\n",
    "\n",
    "        # GAE計算用のdoneフラグ (TimeLimitによる打ち切りはFalse扱いにする場合が多い)\n",
    "        if bootstrap_on_timeout:\n",
    "            done_for_gae = float(terminated)\n",
    "        else:\n",
    "            done_for_gae = float(terminated or truncated)\n",
    "        \n",
    "        # 環境リセット用のdoneフラグ\n",
    "        done_for_reset = (terminated or truncated)\n",
    "\n",
    "        # 【AI用】学習用の変数は「スケーリングした reward」を作る\n",
    "        scaled_reward = float(reward) * agent.Config.reward_scaling  # Pendulum用に 1/100 にする\n",
    "        # scaled_reward = float(reward)\n",
    "\n",
    "        # --- (3) バッファに保存 ---\n",
    "        rollout[\"obs\"].append(np.asarray(obs, dtype=np.float32))\n",
    "        rollout[\"act\"].append(np.asarray(action_raw, dtype=np.float32))\n",
    "        rollout[\"logp\"].append(logp_val)\n",
    "        # rollout[\"rew\"].append(float(reward))\n",
    "        rollout[\"rew\"].append(float(scaled_reward))\n",
    "        rollout[\"obs_next\"].append(np.asarray(obs_next, dtype=np.float32))\n",
    "        rollout[\"done\"].append(float(done_for_gae))\n",
    "\n",
    "        # --- (4) Reset判定 ---\n",
    "        if done_for_reset:\n",
    "            train_reward_history.append(ep_return)\n",
    "            episode_num += 1\n",
    "            ep_return = 0.0\n",
    "            obs, info = env.reset()\n",
    "        else:\n",
    "            obs = obs_next\n",
    "\n",
    "        # --- (5) Update ---\n",
    "        # バッチサイズ分たまったら更新\n",
    "        if len(rollout[\"obs\"]) >= batch_steps:\n",
    "            update_num += 1\n",
    "\n",
    "            # list -> numpy\n",
    "            states      = np.stack(rollout[\"obs\"], axis=0)\n",
    "            actions     = np.stack(rollout[\"act\"], axis=0)\n",
    "            log_probs   = np.array(rollout[\"logp\"], dtype=np.float32)\n",
    "            rewards     = np.array(rollout[\"rew\"], dtype=np.float32)\n",
    "            states_next = np.stack(rollout[\"obs_next\"], axis=0)\n",
    "            dones       = np.array(rollout[\"done\"], dtype=np.float32)\n",
    "\n",
    "            # Update実行\n",
    "            loss_dict = agent.update_net(states, actions, log_probs, rewards, states_next, dones)\n",
    "            \n",
    "            # バッファクリア\n",
    "            rollout_clear()\n",
    "\n",
    "            # ログ記録\n",
    "            loss_history[\"policy_loss\"].append(loss_dict[\"policy_loss\"])\n",
    "            loss_history[\"value_loss\"].append(loss_dict[\"value_loss\"])\n",
    "\n",
    "            # 評価とログ出力\n",
    "            if (update_num % log_interval_updates) == 0:\n",
    "                # 決定論的モードで評価\n",
    "                eval_score = evaluate(env, agent, n_episodes=3)\n",
    "                \n",
    "                logging.info(\n",
    "                    f\"Update {update_num:4d} | Step {t:6d} | \"\n",
    "                    f\"Eval: {eval_score:8.2f} | \"\n",
    "                    f\"P_Loss: {loss_dict['policy_loss']:.4f} | \"\n",
    "                    f\"V_Loss: {loss_dict['value_loss']:.4f}\"\n",
    "                )\n",
    "\n",
    "    return loss_history, train_reward_history\n",
    "\n",
    "# 評価用関数（元のコードと同じものでOKですが、念のため再掲）\n",
    "def evaluate(env, agent, n_episodes=3):\n",
    "    scores = []\n",
    "    # 変数名修正対応\n",
    "    low_np = agent.u_low.detach().cpu().numpy()\n",
    "    high_np = agent.u_high.detach().cpu().numpy()\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        score = 0.0\n",
    "        while not done:\n",
    "            # step() は deterministic=True になっているはず\n",
    "            action = agent.step(obs)\n",
    "            action = np.clip(action, low_np, high_np)\n",
    "            \n",
    "            obs, rew, term, trunc, _ = env.step(action)\n",
    "            score += rew\n",
    "            done = term or trunc\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "25c1a565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start PPO Training: Device=cuda, Batch=2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:54:32 [INFO] Increased policy learning rate to 0.00045\n",
      "22:54:33 [INFO] Update    1 | Step   2047 | Eval: -1066.38 | P_Loss: -0.0055 | V_Loss: 2.4054\n",
      "22:54:37 [INFO] Increased policy learning rate to 0.000675\n",
      "22:54:38 [INFO] Update    2 | Step   4095 | Eval: -1107.56 | P_Loss: -0.0076 | V_Loss: 2.2814\n",
      "22:54:43 [INFO] Increased policy learning rate to 0.0010125\n",
      "22:54:43 [INFO] Update    3 | Step   6143 | Eval: -1207.60 | P_Loss: -0.0088 | V_Loss: 2.5116\n",
      "22:54:49 [INFO] Update    4 | Step   8191 | Eval: -1150.87 | P_Loss: -0.0119 | V_Loss: 2.3314\n",
      "22:54:53 [INFO] Increased policy learning rate to 0.00151875\n",
      "22:54:54 [INFO] Update    5 | Step  10239 | Eval: -1072.44 | P_Loss: -0.0110 | V_Loss: 2.6153\n",
      "22:54:57 [INFO] Increased policy learning rate to 0.0022781249999999998\n",
      "22:54:58 [INFO] Update    6 | Step  12287 | Eval: -1173.62 | P_Loss: -0.0118 | V_Loss: 2.4338\n",
      "22:55:03 [INFO] Update    7 | Step  14335 | Eval: -1307.88 | P_Loss: -0.0127 | V_Loss: 2.2117\n",
      "22:55:08 [INFO] Increased policy learning rate to 0.0034171874999999997\n",
      "22:55:09 [INFO] Update    8 | Step  16383 | Eval:  -921.15 | P_Loss: -0.0168 | V_Loss: 2.6857\n",
      "22:55:14 [INFO] Update    9 | Step  18431 | Eval: -1495.14 | P_Loss: -0.0177 | V_Loss: 2.4224\n",
      "22:55:19 [INFO] Decreased policy learning rate to 0.0022781249999999998\n",
      "22:55:20 [INFO] Update   10 | Step  20479 | Eval: -1054.09 | P_Loss: -0.0162 | V_Loss: 1.8842\n",
      "22:55:25 [INFO] Update   11 | Step  22527 | Eval:  -753.15 | P_Loss: -0.0259 | V_Loss: 2.1431\n",
      "22:55:28 [INFO] Decreased policy learning rate to 0.00151875\n",
      "22:55:29 [INFO] Update   12 | Step  24575 | Eval:  -981.18 | P_Loss: -0.0199 | V_Loss: 2.0434\n",
      "22:55:34 [INFO] Update   13 | Step  26623 | Eval: -1125.07 | P_Loss: -0.0187 | V_Loss: 2.0129\n",
      "22:55:39 [INFO] Update   14 | Step  28671 | Eval: -1226.69 | P_Loss: -0.0212 | V_Loss: 2.0996\n",
      "22:55:44 [INFO] Decreased policy learning rate to 0.0010125\n",
      "22:55:45 [INFO] Update   15 | Step  30719 | Eval: -1005.56 | P_Loss: -0.0205 | V_Loss: 1.5743\n",
      "22:55:50 [INFO] Update   16 | Step  32767 | Eval:  -886.09 | P_Loss: -0.0206 | V_Loss: 1.9819\n",
      "22:55:55 [INFO] Update   17 | Step  34815 | Eval:  -941.82 | P_Loss: -0.0222 | V_Loss: 2.0596\n",
      "22:55:59 [INFO] Update   18 | Step  36863 | Eval: -1068.92 | P_Loss: -0.0197 | V_Loss: 1.6595\n",
      "22:56:04 [INFO] Decreased policy learning rate to 0.0006749999999999999\n",
      "22:56:05 [INFO] Update   19 | Step  38911 | Eval: -1111.35 | P_Loss: -0.0197 | V_Loss: 1.8441\n",
      "22:56:09 [INFO] Increased policy learning rate to 0.0010125\n",
      "22:56:10 [INFO] Update   20 | Step  40959 | Eval:  -839.82 | P_Loss: -0.0166 | V_Loss: 1.3835\n",
      "22:56:14 [INFO] Decreased policy learning rate to 0.0006749999999999999\n",
      "22:56:15 [INFO] Update   21 | Step  43007 | Eval: -1015.31 | P_Loss: -0.0197 | V_Loss: 2.1736\n",
      "22:56:20 [INFO] Update   22 | Step  45055 | Eval:  -717.04 | P_Loss: -0.0174 | V_Loss: 1.7101\n",
      "22:56:25 [INFO] Update   23 | Step  47103 | Eval:  -697.18 | P_Loss: -0.0193 | V_Loss: 1.8221\n",
      "22:56:29 [INFO] Update   24 | Step  49151 | Eval:  -784.52 | P_Loss: -0.0198 | V_Loss: 1.6758\n",
      "22:56:34 [INFO] Decreased policy learning rate to 0.00044999999999999993\n",
      "22:56:35 [INFO] Update   25 | Step  51199 | Eval:  -723.70 | P_Loss: -0.0214 | V_Loss: 1.4951\n",
      "22:56:42 [INFO] Update   26 | Step  53247 | Eval:  -671.12 | P_Loss: -0.0166 | V_Loss: 1.3774\n",
      "22:56:47 [INFO] Update   27 | Step  55295 | Eval:  -877.83 | P_Loss: -0.0161 | V_Loss: 1.4763\n",
      "22:56:51 [INFO] Decreased policy learning rate to 0.0003\n",
      "22:56:52 [INFO] Update   28 | Step  57343 | Eval: -1023.15 | P_Loss: -0.0155 | V_Loss: 1.3831\n",
      "22:56:58 [INFO] Update   29 | Step  59391 | Eval:  -757.90 | P_Loss: -0.0177 | V_Loss: 1.7334\n",
      "22:57:02 [INFO] Update   30 | Step  61439 | Eval:  -868.70 | P_Loss: -0.0149 | V_Loss: 1.5354\n",
      "22:57:07 [INFO] Update   31 | Step  63487 | Eval: -1086.02 | P_Loss: -0.0139 | V_Loss: 1.1619\n",
      "22:57:12 [INFO] Increased policy learning rate to 0.00045\n",
      "22:57:13 [INFO] Update   32 | Step  65535 | Eval:  -845.99 | P_Loss: -0.0159 | V_Loss: 1.8185\n",
      "22:57:17 [INFO] Decreased policy learning rate to 0.0003\n",
      "22:57:18 [INFO] Update   33 | Step  67583 | Eval:  -683.92 | P_Loss: -0.0176 | V_Loss: 1.2774\n",
      "22:57:23 [INFO] Update   34 | Step  69631 | Eval:  -621.59 | P_Loss: -0.0147 | V_Loss: 1.2001\n",
      "22:57:28 [INFO] Update   35 | Step  71679 | Eval:  -668.03 | P_Loss: -0.0137 | V_Loss: 1.0673\n",
      "22:57:31 [INFO] Update   36 | Step  73727 | Eval:  -713.21 | P_Loss: -0.0154 | V_Loss: 1.4392\n",
      "22:57:36 [INFO] Decreased policy learning rate to 0.00019999999999999998\n",
      "22:57:36 [INFO] Update   37 | Step  75775 | Eval:  -712.72 | P_Loss: -0.0163 | V_Loss: 1.0303\n",
      "22:57:42 [INFO] Update   38 | Step  77823 | Eval:  -803.69 | P_Loss: -0.0137 | V_Loss: 1.1220\n",
      "22:57:47 [INFO] Update   39 | Step  79871 | Eval:  -655.78 | P_Loss: -0.0163 | V_Loss: 1.3153\n",
      "22:57:52 [INFO] Update   40 | Step  81919 | Eval:  -587.24 | P_Loss: -0.0176 | V_Loss: 1.3211\n",
      "22:57:56 [INFO] Decreased policy learning rate to 0.0001333333333333333\n",
      "22:57:57 [INFO] Update   41 | Step  83967 | Eval:  -826.57 | P_Loss: -0.0159 | V_Loss: 1.1083\n",
      "22:58:01 [INFO] Update   42 | Step  86015 | Eval:  -685.59 | P_Loss: -0.0108 | V_Loss: 1.3020\n",
      "22:58:06 [INFO] Update   43 | Step  88063 | Eval:  -720.40 | P_Loss: -0.0140 | V_Loss: 1.3382\n",
      "22:58:11 [INFO] Update   44 | Step  90111 | Eval:  -581.30 | P_Loss: -0.0114 | V_Loss: 1.2160\n",
      "22:58:16 [INFO] Decreased policy learning rate to 8.888888888888888e-05\n",
      "22:58:17 [INFO] Update   45 | Step  92159 | Eval:  -544.91 | P_Loss: -0.0141 | V_Loss: 1.0279\n",
      "22:58:22 [INFO] Update   46 | Step  94207 | Eval:  -548.97 | P_Loss: -0.0122 | V_Loss: 0.8215\n",
      "22:58:27 [INFO] Increased policy learning rate to 0.0001333333333333333\n",
      "22:58:27 [INFO] Update   47 | Step  96255 | Eval:  -553.15 | P_Loss: -0.0110 | V_Loss: 1.3077\n",
      "22:58:31 [INFO] Update   48 | Step  98303 | Eval:  -546.46 | P_Loss: -0.0131 | V_Loss: 1.2561\n",
      "22:58:37 [INFO] Update   49 | Step 100351 | Eval:  -463.10 | P_Loss: -0.0122 | V_Loss: 0.9695\n",
      "22:58:42 [INFO] Update   50 | Step 102399 | Eval:  -519.32 | P_Loss: -0.0128 | V_Loss: 0.9305\n",
      "22:58:47 [INFO] Update   51 | Step 104447 | Eval:  -335.47 | P_Loss: -0.0114 | V_Loss: 0.7985\n",
      "22:58:52 [INFO] Decreased policy learning rate to 8.888888888888888e-05\n",
      "22:58:52 [INFO] Update   52 | Step 106495 | Eval:  -343.93 | P_Loss: -0.0159 | V_Loss: 0.7454\n",
      "22:58:58 [INFO] Update   53 | Step 108543 | Eval:  -307.30 | P_Loss: -0.0103 | V_Loss: 1.0268\n",
      "22:59:02 [INFO] Update   54 | Step 110591 | Eval:  -391.46 | P_Loss: -0.0113 | V_Loss: 0.5699\n",
      "22:59:07 [INFO] Update   55 | Step 112639 | Eval:  -363.97 | P_Loss: -0.0143 | V_Loss: 0.4494\n",
      "22:59:12 [INFO] Update   56 | Step 114687 | Eval:  -486.72 | P_Loss: -0.0140 | V_Loss: 0.4713\n",
      "22:59:17 [INFO] Increased policy learning rate to 0.0001333333333333333\n",
      "22:59:18 [INFO] Update   57 | Step 116735 | Eval:  -489.40 | P_Loss: -0.0095 | V_Loss: 0.6925\n",
      "22:59:23 [INFO] Update   58 | Step 118783 | Eval:  -562.41 | P_Loss: -0.0105 | V_Loss: 0.1993\n",
      "22:59:28 [INFO] Update   59 | Step 120831 | Eval:  -467.58 | P_Loss: -0.0136 | V_Loss: 0.6735\n",
      "22:59:33 [INFO] Update   60 | Step 122879 | Eval:  -210.47 | P_Loss: -0.0127 | V_Loss: 0.6533\n",
      "22:59:37 [INFO] Update   61 | Step 124927 | Eval:  -210.09 | P_Loss: -0.0107 | V_Loss: 0.6692\n",
      "22:59:42 [INFO] Increased policy learning rate to 0.00019999999999999998\n",
      "22:59:43 [INFO] Update   62 | Step 126975 | Eval:  -257.68 | P_Loss: -0.0114 | V_Loss: 0.2418\n",
      "22:59:48 [INFO] Update   63 | Step 129023 | Eval:  -403.26 | P_Loss: -0.0155 | V_Loss: 0.2734\n",
      "22:59:53 [INFO] Update   64 | Step 131071 | Eval:  -235.68 | P_Loss: -0.0179 | V_Loss: 0.4121\n",
      "22:59:59 [INFO] Update   65 | Step 133119 | Eval:  -328.81 | P_Loss: -0.0144 | V_Loss: 0.2978\n",
      "23:00:04 [INFO] Update   66 | Step 135167 | Eval:  -255.66 | P_Loss: -0.0120 | V_Loss: 0.5048\n",
      "23:00:08 [INFO] Update   67 | Step 137215 | Eval:  -286.22 | P_Loss: -0.0158 | V_Loss: 0.3220\n",
      "23:00:13 [INFO] Update   68 | Step 139263 | Eval:  -172.43 | P_Loss: -0.0150 | V_Loss: 0.2849\n",
      "23:00:19 [INFO] Update   69 | Step 141311 | Eval:  -404.54 | P_Loss: -0.0136 | V_Loss: 0.4558\n",
      "23:00:26 [INFO] Update   70 | Step 143359 | Eval:  -167.53 | P_Loss: -0.0112 | V_Loss: 0.4250\n",
      "23:00:32 [INFO] Increased policy learning rate to 0.0003\n",
      "23:00:32 [INFO] Update   71 | Step 145407 | Eval:  -280.36 | P_Loss: -0.0141 | V_Loss: 0.2890\n",
      "23:00:37 [INFO] Update   72 | Step 147455 | Eval:  -124.65 | P_Loss: -0.0128 | V_Loss: 0.2921\n",
      "23:00:44 [INFO] Update   73 | Step 149503 | Eval:  -122.64 | P_Loss: -0.0183 | V_Loss: 0.2719\n",
      "23:00:50 [INFO] Update   74 | Step 151551 | Eval:  -246.80 | P_Loss: -0.0140 | V_Loss: 0.7169\n",
      "23:00:56 [INFO] Update   75 | Step 153599 | Eval:  -124.00 | P_Loss: -0.0140 | V_Loss: 0.6477\n",
      "23:01:01 [INFO] Update   76 | Step 155647 | Eval:  -228.75 | P_Loss: -0.0162 | V_Loss: 0.4475\n",
      "23:01:07 [INFO] Update   77 | Step 157695 | Eval:  -260.89 | P_Loss: -0.0167 | V_Loss: 0.4052\n",
      "23:01:11 [INFO] Update   78 | Step 159743 | Eval:  -166.88 | P_Loss: -0.0155 | V_Loss: 0.2501\n",
      "23:01:17 [INFO] Update   79 | Step 161791 | Eval:  -487.56 | P_Loss: -0.0155 | V_Loss: 0.2275\n",
      "23:01:23 [INFO] Update   80 | Step 163839 | Eval:  -222.33 | P_Loss: -0.0154 | V_Loss: 0.3291\n",
      "23:01:28 [INFO] Decreased policy learning rate to 0.00019999999999999998\n",
      "23:01:28 [INFO] Update   81 | Step 165887 | Eval:  -523.87 | P_Loss: -0.0160 | V_Loss: 0.2071\n",
      "23:01:34 [INFO] Update   82 | Step 167935 | Eval:  -165.12 | P_Loss: -0.0128 | V_Loss: 0.3616\n",
      "23:01:38 [INFO] Update   83 | Step 169983 | Eval:  -125.69 | P_Loss: -0.0118 | V_Loss: 0.5979\n",
      "23:01:43 [INFO] Update   84 | Step 172031 | Eval:  -175.26 | P_Loss: -0.0152 | V_Loss: 0.3337\n",
      "23:01:48 [INFO] Update   85 | Step 174079 | Eval:  -757.86 | P_Loss: -0.0106 | V_Loss: 0.4134\n",
      "23:01:53 [INFO] Increased policy learning rate to 0.0003\n",
      "23:01:53 [INFO] Update   86 | Step 176127 | Eval:  -484.50 | P_Loss: -0.0120 | V_Loss: 0.2499\n",
      "23:01:59 [INFO] Update   87 | Step 178175 | Eval:  -676.20 | P_Loss: -0.0152 | V_Loss: 0.2219\n",
      "23:02:03 [INFO] Decreased policy learning rate to 0.00019999999999999998\n",
      "23:02:03 [INFO] Update   88 | Step 180223 | Eval:  -370.37 | P_Loss: -0.0225 | V_Loss: 0.4326\n",
      "23:02:08 [INFO] Increased policy learning rate to 0.0003\n",
      "23:02:08 [INFO] Update   89 | Step 182271 | Eval:   -39.47 | P_Loss: -0.0097 | V_Loss: 0.5686\n",
      "23:02:13 [INFO] Update   90 | Step 184319 | Eval:   -83.32 | P_Loss: -0.0098 | V_Loss: 0.8675\n",
      "23:02:18 [INFO] Increased policy learning rate to 0.00045\n",
      "23:02:18 [INFO] Update   91 | Step 186367 | Eval:  -719.30 | P_Loss: -0.0076 | V_Loss: 1.6330\n",
      "23:02:24 [INFO] Update   92 | Step 188415 | Eval:  -212.41 | P_Loss: -0.0119 | V_Loss: 0.7965\n",
      "23:02:26 [INFO] Decreased policy learning rate to 0.0003\n",
      "23:02:27 [INFO] Update   93 | Step 190463 | Eval:  -670.60 | P_Loss: -0.0251 | V_Loss: 0.1645\n",
      "23:02:32 [INFO] Update   94 | Step 192511 | Eval:  -886.48 | P_Loss: -0.0134 | V_Loss: 0.5695\n",
      "23:02:37 [INFO] Decreased policy learning rate to 0.00019999999999999998\n",
      "23:02:37 [INFO] Update   95 | Step 194559 | Eval:  -820.88 | P_Loss: -0.0156 | V_Loss: 0.2389\n",
      "23:02:43 [INFO] Update   96 | Step 196607 | Eval:  -208.91 | P_Loss: -0.0126 | V_Loss: 0.1227\n",
      "23:02:47 [INFO] Increased policy learning rate to 0.0003\n",
      "23:02:48 [INFO] Update   97 | Step 198655 | Eval:  -721.72 | P_Loss: -0.0061 | V_Loss: 0.7375\n",
      "23:02:53 [INFO] Update   98 | Step 200703 | Eval:  -243.03 | P_Loss: -0.0172 | V_Loss: 0.2834\n",
      "23:02:57 [INFO] Update   99 | Step 202751 | Eval:  -599.88 | P_Loss: -0.0134 | V_Loss: 0.4137\n",
      "23:03:02 [INFO] Increased policy learning rate to 0.00045\n",
      "23:03:03 [INFO] Update  100 | Step 204799 | Eval:   -86.39 | P_Loss: -0.0075 | V_Loss: 0.8116\n",
      "23:03:08 [INFO] Update  101 | Step 206847 | Eval:  -540.17 | P_Loss: -0.0164 | V_Loss: 0.3659\n",
      "23:03:12 [INFO] Decreased policy learning rate to 0.0003\n",
      "23:03:13 [INFO] Update  102 | Step 208895 | Eval:  -584.52 | P_Loss: -0.0202 | V_Loss: 0.2273\n",
      "23:03:19 [INFO] Update  103 | Step 210943 | Eval:  -125.30 | P_Loss: -0.0155 | V_Loss: 0.4885\n",
      "23:03:24 [INFO] Decreased policy learning rate to 0.00019999999999999998\n",
      "23:03:24 [INFO] Update  104 | Step 212991 | Eval:   -81.50 | P_Loss: -0.0205 | V_Loss: 0.1842\n",
      "23:03:28 [INFO] Increased policy learning rate to 0.0003\n",
      "23:03:28 [INFO] Update  105 | Step 215039 | Eval:  -695.54 | P_Loss: -0.0072 | V_Loss: 0.5700\n",
      "23:03:34 [INFO] Update  106 | Step 217087 | Eval:  -170.80 | P_Loss: -0.0090 | V_Loss: 1.7968\n",
      "23:03:39 [INFO] Increased policy learning rate to 0.00045\n",
      "23:03:40 [INFO] Update  107 | Step 219135 | Eval:  -267.06 | P_Loss: -0.0069 | V_Loss: 0.9429\n",
      "23:03:45 [INFO] Increased policy learning rate to 0.000675\n",
      "23:03:46 [INFO] Update  108 | Step 221183 | Eval:  -129.09 | P_Loss: -0.0052 | V_Loss: 0.9296\n",
      "23:03:52 [INFO] Decreased policy learning rate to 0.00045000000000000004\n",
      "23:03:53 [INFO] Update  109 | Step 223231 | Eval:   -84.95 | P_Loss: -0.0215 | V_Loss: 0.1693\n",
      "23:03:57 [INFO] Increased policy learning rate to 0.000675\n",
      "23:03:57 [INFO] Update  110 | Step 225279 | Eval:  -126.08 | P_Loss: -0.0046 | V_Loss: 1.0187\n",
      "23:04:03 [INFO] Update  111 | Step 227327 | Eval:  -518.25 | P_Loss: -0.0083 | V_Loss: 1.7101\n",
      "23:04:07 [INFO] Decreased policy learning rate to 0.00045000000000000004\n",
      "23:04:08 [INFO] Update  112 | Step 229375 | Eval:  -169.07 | P_Loss: -0.0236 | V_Loss: 0.1447\n",
      "23:04:14 [INFO] Update  113 | Step 231423 | Eval:  -837.64 | P_Loss: -0.0098 | V_Loss: 0.7836\n",
      "23:04:19 [INFO] Update  114 | Step 233471 | Eval:  -179.55 | P_Loss: -0.0069 | V_Loss: 2.4974\n",
      "23:04:24 [INFO] Update  115 | Step 235519 | Eval: -1318.78 | P_Loss: -0.0099 | V_Loss: 1.7998\n",
      "23:04:28 [INFO] Update  116 | Step 237567 | Eval:  -823.19 | P_Loss: -0.0081 | V_Loss: 1.6597\n",
      "23:04:34 [INFO] Update  117 | Step 239615 | Eval:  -127.27 | P_Loss: -0.0093 | V_Loss: 1.2455\n",
      "23:04:39 [INFO] Update  118 | Step 241663 | Eval:  -664.75 | P_Loss: -0.0077 | V_Loss: 1.2484\n",
      "23:04:44 [INFO] Increased policy learning rate to 0.000675\n",
      "23:04:44 [INFO] Update  119 | Step 243711 | Eval:  -125.33 | P_Loss: -0.0077 | V_Loss: 1.7579\n",
      "23:04:49 [INFO] Increased policy learning rate to 0.0010125\n",
      "23:04:50 [INFO] Update  120 | Step 245759 | Eval:  -691.33 | P_Loss: -0.0079 | V_Loss: 1.7852\n",
      "23:04:55 [INFO] Update  121 | Step 247807 | Eval:  -866.01 | P_Loss: -0.0087 | V_Loss: 1.6023\n",
      "23:04:58 [INFO] Increased policy learning rate to 0.00151875\n",
      "23:04:59 [INFO] Update  122 | Step 249855 | Eval: -1279.36 | P_Loss: -0.0065 | V_Loss: 3.2005\n",
      "23:05:04 [INFO] Decreased policy learning rate to 0.0010125\n",
      "23:05:04 [INFO] Update  123 | Step 251903 | Eval:   -84.92 | P_Loss: -0.0168 | V_Loss: 1.3555\n",
      "23:05:10 [INFO] Update  124 | Step 253951 | Eval:  -205.74 | P_Loss: -0.0193 | V_Loss: 0.2434\n",
      "23:05:14 [INFO] Decreased policy learning rate to 0.0006749999999999999\n",
      "23:05:15 [INFO] Update  125 | Step 255999 | Eval:  -676.32 | P_Loss: -0.0141 | V_Loss: 0.9942\n",
      "23:05:20 [INFO] Increased policy learning rate to 0.0010125\n",
      "23:05:21 [INFO] Update  126 | Step 258047 | Eval:  -709.35 | P_Loss: -0.0092 | V_Loss: 1.7469\n",
      "23:05:26 [INFO] Update  127 | Step 260095 | Eval:  -168.86 | P_Loss: -0.0116 | V_Loss: 1.6810\n",
      "23:05:30 [INFO] Update  128 | Step 262143 | Eval:  -126.76 | P_Loss: -0.0080 | V_Loss: 1.7260\n",
      "23:05:34 [INFO] Increased policy learning rate to 0.00151875\n",
      "23:05:35 [INFO] Update  129 | Step 264191 | Eval:  -129.57 | P_Loss: -0.0090 | V_Loss: 1.1825\n",
      "23:05:40 [INFO] Increased policy learning rate to 0.0022781249999999998\n",
      "23:05:41 [INFO] Update  130 | Step 266239 | Eval:  -951.91 | P_Loss: -0.0104 | V_Loss: 2.0584\n",
      "23:05:45 [INFO] Increased policy learning rate to 0.0034171874999999997\n",
      "23:05:46 [INFO] Update  131 | Step 268287 | Eval:  -671.03 | P_Loss: -0.0094 | V_Loss: 2.7776\n",
      "23:05:50 [INFO] Decreased policy learning rate to 0.0022781249999999998\n",
      "23:05:51 [INFO] Update  132 | Step 270335 | Eval:  -302.07 | P_Loss: -0.0125 | V_Loss: 0.5983\n",
      "23:05:56 [INFO] Update  133 | Step 272383 | Eval:  -619.14 | P_Loss: -0.0122 | V_Loss: 1.8680\n",
      "23:06:01 [INFO] Decreased policy learning rate to 0.00151875\n",
      "23:06:01 [INFO] Update  134 | Step 274431 | Eval: -1275.11 | P_Loss: -0.0134 | V_Loss: 1.2520\n",
      "23:06:05 [INFO] Increased policy learning rate to 0.0022781249999999998\n",
      "23:06:06 [INFO] Update  135 | Step 276479 | Eval: -1190.96 | P_Loss: -0.0097 | V_Loss: 3.1885\n",
      "23:06:10 [INFO] Decreased policy learning rate to 0.00151875\n",
      "23:06:11 [INFO] Update  136 | Step 278527 | Eval:  -689.46 | P_Loss: -0.0121 | V_Loss: 1.7187\n",
      "23:06:16 [INFO] Update  137 | Step 280575 | Eval:  -164.42 | P_Loss: -0.0124 | V_Loss: 2.0789\n",
      "23:06:21 [INFO] Decreased policy learning rate to 0.0010125\n",
      "23:06:22 [INFO] Update  138 | Step 282623 | Eval:  -655.07 | P_Loss: -0.0235 | V_Loss: 0.4519\n",
      "23:06:26 [INFO] Decreased policy learning rate to 0.0006749999999999999\n",
      "23:06:27 [INFO] Update  139 | Step 284671 | Eval:  -175.55 | P_Loss: -0.0193 | V_Loss: 0.7170\n",
      "23:06:32 [INFO] Decreased policy learning rate to 0.00044999999999999993\n",
      "23:06:32 [INFO] Update  140 | Step 286719 | Eval:  -263.34 | P_Loss: -0.0255 | V_Loss: 0.2962\n",
      "23:06:35 [INFO] Increased policy learning rate to 0.0006749999999999999\n",
      "23:06:36 [INFO] Update  141 | Step 288767 | Eval:   -83.62 | P_Loss: -0.0081 | V_Loss: 1.3235\n",
      "23:06:41 [INFO] Update  142 | Step 290815 | Eval:  -136.88 | P_Loss: -0.0094 | V_Loss: 1.0477\n",
      "23:06:46 [INFO] Decreased policy learning rate to 0.00044999999999999993\n",
      "23:06:47 [INFO] Update  143 | Step 292863 | Eval:  -596.62 | P_Loss: -0.0176 | V_Loss: 0.8667\n",
      "23:06:52 [INFO] Update  144 | Step 294911 | Eval:  -654.47 | P_Loss: -0.0214 | V_Loss: 0.1442\n",
      "23:06:57 [INFO] Update  145 | Step 296959 | Eval:  -127.43 | P_Loss: -0.0161 | V_Loss: 0.6795\n",
      "23:07:02 [INFO] Update  146 | Step 299007 | Eval:  -580.68 | P_Loss: -0.0101 | V_Loss: 1.5234\n",
      "23:07:06 [INFO] Update  147 | Step 301055 | Eval:  -279.37 | P_Loss: -0.0119 | V_Loss: 0.7883\n",
      "23:07:12 [INFO] Update  148 | Step 303103 | Eval:  -301.47 | P_Loss: -0.0159 | V_Loss: 0.4949\n",
      "23:07:16 [INFO] Decreased policy learning rate to 0.0003\n",
      "23:07:17 [INFO] Update  149 | Step 305151 | Eval:  -242.09 | P_Loss: -0.0191 | V_Loss: 0.3035\n",
      "23:07:22 [INFO] Update  150 | Step 307199 | Eval: -1098.92 | P_Loss: -0.0124 | V_Loss: 0.8341\n",
      "23:07:28 [INFO] Update  151 | Step 309247 | Eval:  -127.89 | P_Loss: -0.0183 | V_Loss: 0.1978\n",
      "23:07:33 [INFO] Update  152 | Step 311295 | Eval:  -123.58 | P_Loss: -0.0158 | V_Loss: 0.3603\n",
      "23:07:37 [INFO] Update  153 | Step 313343 | Eval: -1046.80 | P_Loss: -0.0192 | V_Loss: 0.3833\n",
      "23:07:43 [INFO] Update  154 | Step 315391 | Eval:   -87.15 | P_Loss: -0.0083 | V_Loss: 0.8273\n",
      "23:07:48 [INFO] Update  155 | Step 317439 | Eval:  -360.66 | P_Loss: -0.0075 | V_Loss: 0.9865\n",
      "23:07:52 [INFO] Increased policy learning rate to 0.00045\n",
      "23:07:53 [INFO] Update  156 | Step 319487 | Eval:   -81.92 | P_Loss: -0.0068 | V_Loss: 1.8005\n",
      "23:07:58 [INFO] Update  157 | Step 321535 | Eval:   -41.79 | P_Loss: -0.0120 | V_Loss: 0.7915\n",
      "23:08:04 [INFO] Update  158 | Step 323583 | Eval:  -586.79 | P_Loss: -0.0100 | V_Loss: 0.6459\n",
      "23:08:09 [INFO] Increased policy learning rate to 0.000675\n",
      "23:08:09 [INFO] Update  159 | Step 325631 | Eval: -1055.24 | P_Loss: -0.0080 | V_Loss: 1.6584\n",
      "23:08:14 [INFO] Increased policy learning rate to 0.0010125\n",
      "23:08:15 [INFO] Update  160 | Step 327679 | Eval:  -624.61 | P_Loss: -0.0087 | V_Loss: 1.4353\n",
      "23:08:20 [INFO] Decreased policy learning rate to 0.0006749999999999999\n",
      "23:08:21 [INFO] Update  161 | Step 329727 | Eval:  -172.30 | P_Loss: -0.0131 | V_Loss: 0.9363\n",
      "23:08:26 [INFO] Decreased policy learning rate to 0.00044999999999999993\n",
      "23:08:27 [INFO] Update  162 | Step 331775 | Eval:  -125.45 | P_Loss: -0.0154 | V_Loss: 1.1952\n",
      "23:08:32 [INFO] Update  163 | Step 333823 | Eval:  -630.89 | P_Loss: -0.0158 | V_Loss: 0.3016\n",
      "23:08:37 [INFO] Update  164 | Step 335871 | Eval:  -323.38 | P_Loss: -0.0165 | V_Loss: 0.1388\n",
      "23:08:42 [INFO] Update  165 | Step 337919 | Eval:  -586.70 | P_Loss: -0.0158 | V_Loss: 0.2635\n",
      "23:08:47 [INFO] Decreased policy learning rate to 0.0003\n",
      "23:08:48 [INFO] Update  166 | Step 339967 | Eval: -1004.29 | P_Loss: -0.0142 | V_Loss: 0.6645\n",
      "23:08:54 [INFO] Update  167 | Step 342015 | Eval:  -126.19 | P_Loss: -0.0155 | V_Loss: 0.2992\n",
      "23:08:59 [INFO] Update  168 | Step 344063 | Eval:  -128.41 | P_Loss: -0.0118 | V_Loss: 0.5680\n",
      "23:09:04 [INFO] Increased policy learning rate to 0.00045\n",
      "23:09:05 [INFO] Update  169 | Step 346111 | Eval:  -208.78 | P_Loss: -0.0048 | V_Loss: 0.9385\n",
      "23:09:09 [INFO] Increased policy learning rate to 0.000675\n",
      "23:09:10 [INFO] Update  170 | Step 348159 | Eval:  -544.23 | P_Loss: -0.0098 | V_Loss: 0.7061\n",
      "23:09:15 [INFO] Update  171 | Step 350207 | Eval:  -267.01 | P_Loss: -0.0153 | V_Loss: 0.3312\n",
      "23:09:21 [INFO] Update  172 | Step 352255 | Eval:  -126.50 | P_Loss: -0.0198 | V_Loss: 0.1465\n",
      "23:09:26 [INFO] Decreased policy learning rate to 0.00045000000000000004\n",
      "23:09:27 [INFO] Update  173 | Step 354303 | Eval:  -596.14 | P_Loss: -0.0233 | V_Loss: 0.3064\n",
      "23:09:32 [INFO] Decreased policy learning rate to 0.00030000000000000003\n",
      "23:09:32 [INFO] Update  174 | Step 356351 | Eval:  -546.46 | P_Loss: -0.0157 | V_Loss: 0.4721\n",
      "23:09:38 [INFO] Update  175 | Step 358399 | Eval:   -44.76 | P_Loss: -0.0079 | V_Loss: 0.9466\n",
      "23:09:41 [INFO] Increased policy learning rate to 0.00045000000000000004\n",
      "23:09:42 [INFO] Update  176 | Step 360447 | Eval: -1003.58 | P_Loss: -0.0098 | V_Loss: 0.8643\n",
      "23:09:47 [INFO] Update  177 | Step 362495 | Eval:  -594.59 | P_Loss: -0.0197 | V_Loss: 0.2512\n",
      "23:09:53 [INFO] Update  178 | Step 364543 | Eval:  -217.93 | P_Loss: -0.0099 | V_Loss: 0.8811\n",
      "23:09:58 [INFO] Update  179 | Step 366591 | Eval:  -168.07 | P_Loss: -0.0084 | V_Loss: 1.3323\n",
      "23:10:03 [INFO] Update  180 | Step 368639 | Eval:  -719.37 | P_Loss: -0.0102 | V_Loss: 0.5885\n",
      "23:10:09 [INFO] Update  181 | Step 370687 | Eval:  -328.50 | P_Loss: -0.0130 | V_Loss: 0.4952\n",
      "23:10:12 [INFO] Decreased policy learning rate to 0.00030000000000000003\n",
      "23:10:13 [INFO] Update  182 | Step 372735 | Eval:  -180.13 | P_Loss: -0.0194 | V_Loss: 0.1928\n",
      "23:10:17 [INFO] Increased policy learning rate to 0.00045000000000000004\n",
      "23:10:18 [INFO] Update  183 | Step 374783 | Eval:  -244.16 | P_Loss: -0.0067 | V_Loss: 0.9025\n",
      "23:10:23 [INFO] Update  184 | Step 376831 | Eval:  -729.59 | P_Loss: -0.0144 | V_Loss: 0.4175\n",
      "23:10:29 [INFO] Update  185 | Step 378879 | Eval:  -329.86 | P_Loss: -0.0146 | V_Loss: 0.5363\n",
      "23:10:37 [INFO] Update  186 | Step 380927 | Eval:  -125.92 | P_Loss: -0.0130 | V_Loss: 0.8644\n",
      "23:10:42 [INFO] Update  187 | Step 382975 | Eval:  -169.20 | P_Loss: -0.0180 | V_Loss: 0.4308\n",
      "23:10:48 [INFO] Update  188 | Step 385023 | Eval:  -173.84 | P_Loss: -0.0133 | V_Loss: 0.3377\n",
      "23:10:54 [INFO] Decreased policy learning rate to 0.00030000000000000003\n",
      "23:10:55 [INFO] Update  189 | Step 387071 | Eval:  -216.30 | P_Loss: -0.0189 | V_Loss: 0.3231\n",
      "23:11:01 [INFO] Update  190 | Step 389119 | Eval:   -85.25 | P_Loss: -0.0108 | V_Loss: 0.5772\n",
      "23:11:08 [INFO] Update  191 | Step 391167 | Eval:  -221.75 | P_Loss: -0.0133 | V_Loss: 0.4463\n",
      "23:11:13 [INFO] Update  192 | Step 393215 | Eval:  -213.49 | P_Loss: -0.0135 | V_Loss: 0.3387\n",
      "23:11:19 [INFO] Update  193 | Step 395263 | Eval:  -180.41 | P_Loss: -0.0092 | V_Loss: 0.4438\n",
      "23:11:25 [INFO] Update  194 | Step 397311 | Eval:  -309.98 | P_Loss: -0.0155 | V_Loss: 0.4207\n",
      "23:11:32 [INFO] Update  195 | Step 399359 | Eval:  -224.27 | P_Loss: -0.0140 | V_Loss: 0.2514\n",
      "23:11:37 [INFO] Update  196 | Step 401407 | Eval:   -84.22 | P_Loss: -0.0113 | V_Loss: 0.3679\n",
      "23:11:41 [INFO] Increased policy learning rate to 0.00045000000000000004\n",
      "23:11:42 [INFO] Update  197 | Step 403455 | Eval:  -172.55 | P_Loss: -0.0159 | V_Loss: 0.1767\n",
      "23:11:48 [INFO] Update  198 | Step 405503 | Eval:  -170.86 | P_Loss: -0.0153 | V_Loss: 0.3785\n",
      "23:11:54 [INFO] Update  199 | Step 407551 | Eval:  -583.19 | P_Loss: -0.0166 | V_Loss: 0.2817\n",
      "23:12:00 [INFO] Increased policy learning rate to 0.000675\n",
      "23:12:00 [INFO] Update  200 | Step 409599 | Eval:   -85.22 | P_Loss: -0.0161 | V_Loss: 0.2155\n",
      "23:12:07 [INFO] Update  201 | Step 411647 | Eval:  -124.18 | P_Loss: -0.0174 | V_Loss: 0.2368\n",
      "23:12:13 [INFO] Update  202 | Step 413695 | Eval:   -84.80 | P_Loss: -0.0162 | V_Loss: 0.1783\n",
      "23:12:18 [INFO] Update  203 | Step 415743 | Eval:  -247.44 | P_Loss: -0.0172 | V_Loss: 0.1963\n",
      "23:12:24 [INFO] Update  204 | Step 417791 | Eval:  -349.37 | P_Loss: -0.0125 | V_Loss: 0.1363\n",
      "23:12:31 [INFO] Update  205 | Step 419839 | Eval:  -542.15 | P_Loss: -0.0154 | V_Loss: 0.2007\n",
      "23:12:37 [INFO] Update  206 | Step 421887 | Eval:  -177.14 | P_Loss: -0.0190 | V_Loss: 0.3092\n",
      "23:12:42 [INFO] Decreased policy learning rate to 0.00045000000000000004\n",
      "23:12:43 [INFO] Update  207 | Step 423935 | Eval:  -181.82 | P_Loss: -0.0139 | V_Loss: 0.3656\n",
      "23:12:47 [INFO] Update  208 | Step 425983 | Eval:  -170.48 | P_Loss: -0.0102 | V_Loss: 0.3089\n",
      "23:12:53 [INFO] Update  209 | Step 428031 | Eval:   -86.03 | P_Loss: -0.0135 | V_Loss: 0.2858\n",
      "23:12:59 [INFO] Update  210 | Step 430079 | Eval:  -583.33 | P_Loss: -0.0182 | V_Loss: 0.2915\n",
      "23:13:05 [INFO] Update  211 | Step 432127 | Eval:  -316.89 | P_Loss: -0.0130 | V_Loss: 0.3231\n",
      "23:13:11 [INFO] Update  212 | Step 434175 | Eval:  -130.25 | P_Loss: -0.0185 | V_Loss: 0.2699\n",
      "23:13:16 [INFO] Update  213 | Step 436223 | Eval:  -277.68 | P_Loss: -0.0135 | V_Loss: 0.2653\n",
      "23:13:22 [INFO] Update  214 | Step 438271 | Eval:  -207.84 | P_Loss: -0.0161 | V_Loss: 0.2566\n",
      "23:13:27 [INFO] Decreased policy learning rate to 0.00030000000000000003\n",
      "23:13:28 [INFO] Update  215 | Step 440319 | Eval:  -663.53 | P_Loss: -0.0222 | V_Loss: 0.2202\n",
      "23:13:34 [INFO] Update  216 | Step 442367 | Eval:  -309.30 | P_Loss: -0.0123 | V_Loss: 0.4098\n",
      "23:13:39 [INFO] Update  217 | Step 444415 | Eval:  -214.44 | P_Loss: -0.0106 | V_Loss: 0.5145\n",
      "23:13:45 [INFO] Update  218 | Step 446463 | Eval:  -168.53 | P_Loss: -0.0144 | V_Loss: 0.5418\n",
      "23:13:49 [INFO] Increased policy learning rate to 0.00045000000000000004\n",
      "23:13:50 [INFO] Update  219 | Step 448511 | Eval:  -299.90 | P_Loss: -0.0156 | V_Loss: 0.1960\n",
      "23:13:55 [INFO] Increased policy learning rate to 0.000675\n",
      "23:13:56 [INFO] Update  220 | Step 450559 | Eval:  -268.83 | P_Loss: -0.0146 | V_Loss: 0.1355\n",
      "23:14:02 [INFO] Update  221 | Step 452607 | Eval:  -308.25 | P_Loss: -0.0170 | V_Loss: 0.2527\n",
      "23:14:08 [INFO] Update  222 | Step 454655 | Eval:  -169.73 | P_Loss: -0.0134 | V_Loss: 0.2618\n",
      "23:14:14 [INFO] Decreased policy learning rate to 0.00045000000000000004\n",
      "23:14:14 [INFO] Update  223 | Step 456703 | Eval:  -130.25 | P_Loss: -0.0206 | V_Loss: 0.3240\n",
      "23:14:19 [INFO] Update  224 | Step 458751 | Eval:  -213.07 | P_Loss: -0.0133 | V_Loss: 0.3600\n",
      "23:14:25 [INFO] Update  225 | Step 460799 | Eval:  -248.03 | P_Loss: -0.0128 | V_Loss: 0.2756\n",
      "23:14:32 [INFO] Update  226 | Step 462847 | Eval:  -585.41 | P_Loss: -0.0143 | V_Loss: 0.2897\n",
      "23:14:38 [INFO] Update  227 | Step 464895 | Eval:  -173.06 | P_Loss: -0.0087 | V_Loss: 0.4976\n",
      "23:14:44 [INFO] Update  228 | Step 466943 | Eval:  -369.96 | P_Loss: -0.0168 | V_Loss: 0.2766\n",
      "23:14:48 [INFO] Increased policy learning rate to 0.000675\n",
      "23:14:49 [INFO] Update  229 | Step 468991 | Eval:  -126.64 | P_Loss: -0.0104 | V_Loss: 0.5702\n",
      "23:14:55 [INFO] Update  230 | Step 471039 | Eval:  -202.39 | P_Loss: -0.0129 | V_Loss: 0.5338\n",
      "23:15:01 [INFO] Update  231 | Step 473087 | Eval:  -345.39 | P_Loss: -0.0127 | V_Loss: 0.4507\n",
      "23:15:07 [INFO] Decreased policy learning rate to 0.00045000000000000004\n",
      "23:15:08 [INFO] Update  232 | Step 475135 | Eval:  -215.07 | P_Loss: -0.0164 | V_Loss: 0.3932\n",
      "23:15:14 [INFO] Update  233 | Step 477183 | Eval:  -256.47 | P_Loss: -0.0154 | V_Loss: 0.2091\n",
      "23:15:20 [INFO] Update  234 | Step 479231 | Eval:  -125.29 | P_Loss: -0.0157 | V_Loss: 0.3277\n",
      "23:15:25 [INFO] Update  235 | Step 481279 | Eval:  -178.40 | P_Loss: -0.0129 | V_Loss: 0.4043\n",
      "23:15:31 [INFO] Update  236 | Step 483327 | Eval:  -224.88 | P_Loss: -0.0134 | V_Loss: 0.6296\n",
      "23:15:36 [INFO] Increased policy learning rate to 0.000675\n",
      "23:15:37 [INFO] Update  237 | Step 485375 | Eval:  -780.79 | P_Loss: -0.0092 | V_Loss: 0.6992\n",
      "23:15:43 [INFO] Update  238 | Step 487423 | Eval:  -168.20 | P_Loss: -0.0126 | V_Loss: 0.5575\n",
      "23:15:49 [INFO] Update  239 | Step 489471 | Eval:  -129.47 | P_Loss: -0.0172 | V_Loss: 0.6110\n",
      "23:15:53 [INFO] Update  240 | Step 491519 | Eval:  -229.66 | P_Loss: -0.0139 | V_Loss: 0.5282\n",
      "23:15:59 [INFO] Increased policy learning rate to 0.0010125\n",
      "23:15:59 [INFO] Update  241 | Step 493567 | Eval:  -223.81 | P_Loss: -0.0135 | V_Loss: 0.5791\n",
      "23:16:07 [INFO] Update  242 | Step 495615 | Eval:  -231.00 | P_Loss: -0.0197 | V_Loss: 0.3267\n",
      "23:16:13 [INFO] Update  243 | Step 497663 | Eval:   -84.99 | P_Loss: -0.0165 | V_Loss: 0.4449\n",
      "23:16:19 [INFO] Update  244 | Step 499711 | Eval:  -582.18 | P_Loss: -0.0142 | V_Loss: 0.4658\n",
      "23:16:24 [INFO] Decreased policy learning rate to 0.0006749999999999999\n",
      "23:16:25 [INFO] Update  245 | Step 501759 | Eval:  -167.05 | P_Loss: -0.0226 | V_Loss: 0.3219\n",
      "23:16:31 [INFO] Update  246 | Step 503807 | Eval:    -0.47 | P_Loss: -0.0165 | V_Loss: 0.2114\n",
      "23:16:37 [INFO] Update  247 | Step 505855 | Eval:  -221.36 | P_Loss: -0.0177 | V_Loss: 0.3812\n",
      "23:16:43 [INFO] Update  248 | Step 507903 | Eval:  -286.25 | P_Loss: -0.0166 | V_Loss: 0.3691\n",
      "23:16:49 [INFO] Update  249 | Step 509951 | Eval:  -579.22 | P_Loss: -0.0124 | V_Loss: 0.5787\n",
      "23:16:53 [INFO] Update  250 | Step 511999 | Eval: -1131.38 | P_Loss: -0.0164 | V_Loss: 0.3894\n",
      "23:17:00 [INFO] Update  251 | Step 514047 | Eval:   -41.70 | P_Loss: -0.0165 | V_Loss: 0.2032\n",
      "23:17:06 [INFO] Update  252 | Step 516095 | Eval:  -177.73 | P_Loss: -0.0123 | V_Loss: 0.2882\n",
      "23:17:13 [INFO] Update  253 | Step 518143 | Eval:   -43.05 | P_Loss: -0.0119 | V_Loss: 0.4854\n",
      "23:17:19 [INFO] Update  254 | Step 520191 | Eval:  -200.50 | P_Loss: -0.0183 | V_Loss: 0.2193\n",
      "23:17:23 [INFO] Decreased policy learning rate to 0.00044999999999999993\n",
      "23:17:24 [INFO] Update  255 | Step 522239 | Eval:  -540.78 | P_Loss: -0.0178 | V_Loss: 0.2995\n",
      "23:17:30 [INFO] Update  256 | Step 524287 | Eval:   -86.68 | P_Loss: -0.0136 | V_Loss: 0.2916\n",
      "23:17:36 [INFO] Update  257 | Step 526335 | Eval:  -206.08 | P_Loss: -0.0104 | V_Loss: 0.3451\n",
      "23:17:42 [INFO] Update  258 | Step 528383 | Eval:  -121.77 | P_Loss: -0.0170 | V_Loss: 0.4214\n",
      "23:17:48 [INFO] Update  259 | Step 530431 | Eval:  -126.57 | P_Loss: -0.0142 | V_Loss: 0.2724\n",
      "23:17:53 [INFO] Increased policy learning rate to 0.0006749999999999999\n",
      "23:17:54 [INFO] Update  260 | Step 532479 | Eval:  -579.45 | P_Loss: -0.0127 | V_Loss: 0.3158\n",
      "23:17:59 [INFO] Update  261 | Step 534527 | Eval:  -163.21 | P_Loss: -0.0161 | V_Loss: 0.2142\n",
      "23:18:04 [INFO] Decreased policy learning rate to 0.00044999999999999993\n",
      "23:18:05 [INFO] Update  262 | Step 536575 | Eval:  -720.08 | P_Loss: -0.0175 | V_Loss: 0.3459\n",
      "23:18:12 [INFO] Update  263 | Step 538623 | Eval:  -202.31 | P_Loss: -0.0133 | V_Loss: 0.6592\n",
      "23:18:18 [INFO] Update  264 | Step 540671 | Eval:  -541.74 | P_Loss: -0.0122 | V_Loss: 0.2163\n",
      "23:18:24 [INFO] Update  265 | Step 542719 | Eval:  -124.75 | P_Loss: -0.0110 | V_Loss: 0.4988\n",
      "23:18:29 [INFO] Update  266 | Step 544767 | Eval:  -330.96 | P_Loss: -0.0115 | V_Loss: 0.3800\n",
      "23:18:35 [INFO] Increased policy learning rate to 0.0006749999999999999\n",
      "23:18:35 [INFO] Update  267 | Step 546815 | Eval:  -165.50 | P_Loss: -0.0142 | V_Loss: 0.1765\n",
      "23:18:42 [INFO] Update  268 | Step 548863 | Eval:  -220.74 | P_Loss: -0.0131 | V_Loss: 0.2425\n",
      "23:18:48 [INFO] Update  269 | Step 550911 | Eval:  -127.40 | P_Loss: -0.0120 | V_Loss: 0.2333\n",
      "23:18:54 [INFO] Update  270 | Step 552959 | Eval: -1005.11 | P_Loss: -0.0125 | V_Loss: 0.2745\n",
      "23:18:59 [INFO] Decreased policy learning rate to 0.00044999999999999993\n",
      "23:18:59 [INFO] Update  271 | Step 555007 | Eval:  -164.08 | P_Loss: -0.0190 | V_Loss: 0.3000\n",
      "23:19:06 [INFO] Update  272 | Step 557055 | Eval:  -214.30 | P_Loss: -0.0098 | V_Loss: 0.5414\n",
      "23:19:13 [INFO] Update  273 | Step 559103 | Eval:  -119.11 | P_Loss: -0.0124 | V_Loss: 0.3557\n",
      "23:19:19 [INFO] Update  274 | Step 561151 | Eval:  -579.43 | P_Loss: -0.0091 | V_Loss: 0.3392\n",
      "23:19:26 [INFO] Update  275 | Step 563199 | Eval:  -214.51 | P_Loss: -0.0118 | V_Loss: 0.3009\n",
      "23:19:32 [INFO] Update  276 | Step 565247 | Eval:  -164.66 | P_Loss: -0.0138 | V_Loss: 0.3022\n",
      "23:19:38 [INFO] Update  277 | Step 567295 | Eval:  -124.75 | P_Loss: -0.0114 | V_Loss: 0.3312\n",
      "23:19:43 [INFO] Increased policy learning rate to 0.0006749999999999999\n",
      "23:19:44 [INFO] Update  278 | Step 569343 | Eval:  -123.71 | P_Loss: -0.0109 | V_Loss: 0.2646\n",
      "23:19:49 [INFO] Decreased policy learning rate to 0.00044999999999999993\n",
      "23:19:50 [INFO] Update  279 | Step 571391 | Eval:   -85.66 | P_Loss: -0.0161 | V_Loss: 0.1712\n",
      "23:19:56 [INFO] Update  280 | Step 573439 | Eval:  -500.50 | P_Loss: -0.0103 | V_Loss: 0.3884\n",
      "23:20:01 [INFO] Update  281 | Step 575487 | Eval:   -82.87 | P_Loss: -0.0147 | V_Loss: 0.1891\n",
      "23:20:07 [INFO] Update  282 | Step 577535 | Eval:  -188.57 | P_Loss: -0.0127 | V_Loss: 0.2568\n",
      "23:20:13 [INFO] Update  283 | Step 579583 | Eval:  -171.62 | P_Loss: -0.0116 | V_Loss: 0.2749\n",
      "23:20:19 [INFO] Increased policy learning rate to 0.0006749999999999999\n",
      "23:20:20 [INFO] Update  284 | Step 581631 | Eval:  -211.61 | P_Loss: -0.0107 | V_Loss: 0.2560\n",
      "23:20:26 [INFO] Update  285 | Step 583679 | Eval:  -269.29 | P_Loss: -0.0168 | V_Loss: 0.2216\n",
      "23:20:32 [INFO] Update  286 | Step 585727 | Eval:  -124.44 | P_Loss: -0.0128 | V_Loss: 0.2356\n",
      "23:20:39 [INFO] Update  287 | Step 587775 | Eval:  -123.68 | P_Loss: -0.0123 | V_Loss: 0.2495\n",
      "23:20:46 [INFO] Update  288 | Step 589823 | Eval:  -123.52 | P_Loss: -0.0134 | V_Loss: 0.3138\n",
      "23:20:52 [INFO] Update  289 | Step 591871 | Eval:  -165.05 | P_Loss: -0.0159 | V_Loss: 0.2955\n",
      "23:20:58 [INFO] Update  290 | Step 593919 | Eval:   -43.41 | P_Loss: -0.0104 | V_Loss: 0.3262\n",
      "23:21:04 [INFO] Update  291 | Step 595967 | Eval:  -123.32 | P_Loss: -0.0135 | V_Loss: 0.2435\n",
      "23:21:11 [INFO] Update  292 | Step 598015 | Eval:  -120.53 | P_Loss: -0.0176 | V_Loss: 0.1928\n",
      "23:21:17 [INFO] Update  293 | Step 600063 | Eval:  -241.15 | P_Loss: -0.0118 | V_Loss: 0.2636\n",
      "23:21:23 [INFO] Update  294 | Step 602111 | Eval:  -258.42 | P_Loss: -0.0150 | V_Loss: 0.2394\n",
      "23:21:30 [INFO] Update  295 | Step 604159 | Eval:  -189.53 | P_Loss: -0.0095 | V_Loss: 0.2663\n",
      "23:21:35 [INFO] Update  296 | Step 606207 | Eval:   -41.01 | P_Loss: -0.0159 | V_Loss: 0.2613\n",
      "23:21:41 [INFO] Update  297 | Step 608255 | Eval:  -298.86 | P_Loss: -0.0091 | V_Loss: 0.2077\n",
      "23:21:46 [INFO] Decreased policy learning rate to 0.00044999999999999993\n",
      "23:21:47 [INFO] Update  298 | Step 610303 | Eval:  -230.72 | P_Loss: -0.0165 | V_Loss: 0.3468\n",
      "23:21:53 [INFO] Update  299 | Step 612351 | Eval:   -83.62 | P_Loss: -0.0129 | V_Loss: 0.2524\n",
      "23:21:59 [INFO] Update  300 | Step 614399 | Eval:  -198.38 | P_Loss: -0.0116 | V_Loss: 0.3852\n",
      "23:22:04 [INFO] Update  301 | Step 616447 | Eval:  -199.90 | P_Loss: -0.0152 | V_Loss: 0.2736\n",
      "23:22:10 [INFO] Update  302 | Step 618495 | Eval:  -164.34 | P_Loss: -0.0106 | V_Loss: 0.3618\n",
      "23:22:15 [INFO] Increased policy learning rate to 0.0006749999999999999\n",
      "23:22:16 [INFO] Update  303 | Step 620543 | Eval:  -126.19 | P_Loss: -0.0088 | V_Loss: 0.1497\n",
      "23:22:22 [INFO] Increased policy learning rate to 0.0010125\n",
      "23:22:23 [INFO] Update  304 | Step 622591 | Eval:  -215.16 | P_Loss: -0.0095 | V_Loss: 0.2702\n",
      "23:22:29 [INFO] Update  305 | Step 624639 | Eval:  -195.14 | P_Loss: -0.0141 | V_Loss: 0.2361\n",
      "23:22:34 [INFO] Decreased policy learning rate to 0.0006749999999999999\n",
      "23:22:35 [INFO] Update  306 | Step 626687 | Eval:  -487.16 | P_Loss: -0.0194 | V_Loss: 0.1221\n",
      "23:22:41 [INFO] Update  307 | Step 628735 | Eval:  -715.12 | P_Loss: -0.0108 | V_Loss: 0.3125\n",
      "23:22:46 [INFO] Decreased policy learning rate to 0.00044999999999999993\n",
      "23:22:47 [INFO] Update  308 | Step 630783 | Eval:  -128.55 | P_Loss: -0.0170 | V_Loss: 0.5257\n",
      "23:22:52 [INFO] Increased policy learning rate to 0.0006749999999999999\n",
      "23:22:53 [INFO] Update  309 | Step 632831 | Eval:   -82.52 | P_Loss: -0.0093 | V_Loss: 0.2346\n",
      "23:22:59 [INFO] Update  310 | Step 634879 | Eval:  -198.27 | P_Loss: -0.0091 | V_Loss: 0.3551\n",
      "23:23:04 [INFO] Update  311 | Step 636927 | Eval:   -91.50 | P_Loss: -0.0156 | V_Loss: 0.3688\n",
      "23:23:11 [INFO] Update  312 | Step 638975 | Eval:   -85.47 | P_Loss: -0.0159 | V_Loss: 0.2004\n",
      "23:23:17 [INFO] Update  313 | Step 641023 | Eval:  -209.01 | P_Loss: -0.0099 | V_Loss: 0.4085\n",
      "23:23:23 [INFO] Update  314 | Step 643071 | Eval:   -85.82 | P_Loss: -0.0123 | V_Loss: 0.2887\n",
      "23:23:30 [INFO] Update  315 | Step 645119 | Eval:  -542.06 | P_Loss: -0.0138 | V_Loss: 0.2988\n",
      "23:23:35 [INFO] Increased policy learning rate to 0.0010125\n",
      "23:23:36 [INFO] Update  316 | Step 647167 | Eval:  -620.37 | P_Loss: -0.0099 | V_Loss: 0.3159\n",
      "23:23:41 [INFO] Update  317 | Step 649215 | Eval:  -126.69 | P_Loss: -0.0114 | V_Loss: 0.3576\n",
      "23:23:46 [INFO] Increased policy learning rate to 0.00151875\n",
      "23:23:47 [INFO] Update  318 | Step 651263 | Eval:  -172.89 | P_Loss: -0.0103 | V_Loss: 0.4967\n",
      "23:23:53 [INFO] Update  319 | Step 653311 | Eval:  -205.47 | P_Loss: -0.0131 | V_Loss: 0.3088\n",
      "23:23:58 [INFO] Increased policy learning rate to 0.0022781249999999998\n",
      "23:23:59 [INFO] Update  320 | Step 655359 | Eval: -1046.90 | P_Loss: -0.0148 | V_Loss: 0.4229\n",
      "23:24:06 [INFO] Update  321 | Step 657407 | Eval:  -583.69 | P_Loss: -0.0133 | V_Loss: 0.6116\n",
      "23:24:09 [INFO] Decreased policy learning rate to 0.00151875\n",
      "23:24:10 [INFO] Update  322 | Step 659455 | Eval:  -129.40 | P_Loss: -0.0160 | V_Loss: 0.3010\n",
      "23:24:16 [INFO] Decreased policy learning rate to 0.0010125\n",
      "23:24:17 [INFO] Update  323 | Step 661503 | Eval:  -542.03 | P_Loss: -0.0175 | V_Loss: 0.3878\n",
      "23:24:22 [INFO] Increased policy learning rate to 0.00151875\n",
      "23:24:23 [INFO] Update  324 | Step 663551 | Eval:  -164.43 | P_Loss: -0.0107 | V_Loss: 0.3479\n",
      "23:24:30 [INFO] Update  325 | Step 665599 | Eval:  -186.78 | P_Loss: -0.0148 | V_Loss: 0.3542\n",
      "23:24:36 [INFO] Update  326 | Step 667647 | Eval:  -906.48 | P_Loss: -0.0138 | V_Loss: 0.2799\n",
      "23:24:41 [INFO] Update  327 | Step 669695 | Eval:  -125.90 | P_Loss: -0.0102 | V_Loss: 0.4447\n",
      "23:24:46 [INFO] Decreased policy learning rate to 0.0010125\n",
      "23:24:47 [INFO] Update  328 | Step 671743 | Eval:   -84.25 | P_Loss: -0.0160 | V_Loss: 0.2294\n",
      "23:24:53 [INFO] Increased policy learning rate to 0.00151875\n",
      "23:24:53 [INFO] Update  329 | Step 673791 | Eval:    -2.49 | P_Loss: -0.0101 | V_Loss: 0.4173\n",
      "23:24:59 [INFO] Update  330 | Step 675839 | Eval:  -213.28 | P_Loss: -0.0108 | V_Loss: 0.4678\n",
      "23:25:06 [INFO] Update  331 | Step 677887 | Eval:  -590.37 | P_Loss: -0.0142 | V_Loss: 0.4605\n",
      "23:25:11 [INFO] Update  332 | Step 679935 | Eval:   -43.42 | P_Loss: -0.0143 | V_Loss: 0.4471\n",
      "23:25:17 [INFO] Decreased policy learning rate to 0.0010125\n",
      "23:25:18 [INFO] Update  333 | Step 681983 | Eval:  -127.86 | P_Loss: -0.0159 | V_Loss: 0.3923\n",
      "23:25:24 [INFO] Increased policy learning rate to 0.00151875\n",
      "23:25:25 [INFO] Update  334 | Step 684031 | Eval: -1097.20 | P_Loss: -0.0096 | V_Loss: 0.2497\n",
      "23:25:31 [INFO] Update  335 | Step 686079 | Eval:   -86.38 | P_Loss: -0.0145 | V_Loss: 0.4165\n",
      "23:25:36 [INFO] Decreased policy learning rate to 0.0010125\n",
      "23:25:37 [INFO] Update  336 | Step 688127 | Eval:  -582.01 | P_Loss: -0.0167 | V_Loss: 0.3750\n",
      "23:25:42 [INFO] Update  337 | Step 690175 | Eval:   -90.04 | P_Loss: -0.0111 | V_Loss: 0.4036\n",
      "23:25:48 [INFO] Update  338 | Step 692223 | Eval:  -582.70 | P_Loss: -0.0161 | V_Loss: 0.5098\n",
      "23:25:54 [INFO] Update  339 | Step 694271 | Eval:   -82.96 | P_Loss: -0.0139 | V_Loss: 0.3835\n",
      "23:26:00 [INFO] Increased policy learning rate to 0.00151875\n",
      "23:26:01 [INFO] Update  340 | Step 696319 | Eval:  -260.46 | P_Loss: -0.0109 | V_Loss: 0.4463\n",
      "23:26:06 [INFO] Update  341 | Step 698367 | Eval:  -170.72 | P_Loss: -0.0110 | V_Loss: 0.8036\n",
      "23:26:10 [INFO] Decreased policy learning rate to 0.0010125\n",
      "23:26:11 [INFO] Update  342 | Step 700415 | Eval:  -172.42 | P_Loss: -0.0202 | V_Loss: 0.3371\n",
      "23:26:17 [INFO] Update  343 | Step 702463 | Eval:  -166.24 | P_Loss: -0.0090 | V_Loss: 0.5704\n",
      "23:26:23 [INFO] Update  344 | Step 704511 | Eval:  -176.28 | P_Loss: -0.0117 | V_Loss: 0.5741\n",
      "23:26:29 [INFO] Update  345 | Step 706559 | Eval:  -165.21 | P_Loss: -0.0119 | V_Loss: 0.2269\n",
      "23:26:35 [INFO] Update  346 | Step 708607 | Eval:  -227.62 | P_Loss: -0.0104 | V_Loss: 0.7923\n",
      "23:26:41 [INFO] Update  347 | Step 710655 | Eval:  -130.73 | P_Loss: -0.0122 | V_Loss: 0.4362\n",
      "23:26:46 [INFO] Update  348 | Step 712703 | Eval:  -123.10 | P_Loss: -0.0108 | V_Loss: 0.8917\n",
      "23:26:52 [INFO] Update  349 | Step 714751 | Eval:  -618.59 | P_Loss: -0.0103 | V_Loss: 0.6717\n",
      "23:26:58 [INFO] Update  350 | Step 716799 | Eval:  -255.81 | P_Loss: -0.0130 | V_Loss: 0.2954\n",
      "23:27:05 [INFO] Update  351 | Step 718847 | Eval:  -260.92 | P_Loss: -0.0130 | V_Loss: 0.5400\n",
      "23:27:11 [INFO] Update  352 | Step 720895 | Eval:  -617.78 | P_Loss: -0.0104 | V_Loss: 0.6264\n",
      "23:27:17 [INFO] Update  353 | Step 722943 | Eval:  -124.46 | P_Loss: -0.0125 | V_Loss: 0.8744\n",
      "23:27:22 [INFO] Decreased policy learning rate to 0.0006749999999999999\n",
      "23:27:23 [INFO] Update  354 | Step 724991 | Eval:  -120.17 | P_Loss: -0.0159 | V_Loss: 0.5528\n",
      "23:27:29 [INFO] Update  355 | Step 727039 | Eval:  -123.20 | P_Loss: -0.0080 | V_Loss: 0.8830\n",
      "23:27:35 [INFO] Increased policy learning rate to 0.0010125\n",
      "23:27:36 [INFO] Update  356 | Step 729087 | Eval:  -176.63 | P_Loss: -0.0100 | V_Loss: 0.5177\n",
      "23:27:41 [INFO] Increased policy learning rate to 0.00151875\n",
      "23:27:42 [INFO] Update  357 | Step 731135 | Eval:  -540.15 | P_Loss: -0.0098 | V_Loss: 0.3427\n",
      "23:27:47 [INFO] Increased policy learning rate to 0.0022781249999999998\n",
      "23:27:48 [INFO] Update  358 | Step 733183 | Eval:  -127.10 | P_Loss: -0.0084 | V_Loss: 1.1607\n",
      "23:27:54 [INFO] Update  359 | Step 735231 | Eval:  -628.84 | P_Loss: -0.0138 | V_Loss: 0.6722\n",
      "23:28:00 [INFO] Update  360 | Step 737279 | Eval:  -243.05 | P_Loss: -0.0143 | V_Loss: 0.3128\n",
      "23:28:06 [INFO] Update  361 | Step 739327 | Eval:  -780.34 | P_Loss: -0.0118 | V_Loss: 0.9125\n",
      "23:28:12 [INFO] Update  362 | Step 741375 | Eval:   -82.06 | P_Loss: -0.0133 | V_Loss: 0.8039\n",
      "23:28:18 [INFO] Update  363 | Step 743423 | Eval:  -259.53 | P_Loss: -0.0109 | V_Loss: 1.0031\n",
      "23:28:23 [INFO] Decreased policy learning rate to 0.00151875\n",
      "23:28:24 [INFO] Update  364 | Step 745471 | Eval:  -175.35 | P_Loss: -0.0160 | V_Loss: 1.0795\n",
      "23:28:31 [INFO] Update  365 | Step 747519 | Eval:   -85.13 | P_Loss: -0.0098 | V_Loss: 1.1357\n",
      "23:28:37 [INFO] Update  366 | Step 749567 | Eval:  -348.46 | P_Loss: -0.0191 | V_Loss: 0.3581\n",
      "23:28:43 [INFO] Update  367 | Step 751615 | Eval:   -82.51 | P_Loss: -0.0140 | V_Loss: 0.6191\n",
      "23:28:48 [INFO] Update  368 | Step 753663 | Eval:  -574.04 | P_Loss: -0.0155 | V_Loss: 0.3824\n",
      "23:28:54 [INFO] Update  369 | Step 755711 | Eval:  -232.26 | P_Loss: -0.0106 | V_Loss: 0.6947\n",
      "23:29:00 [INFO] Update  370 | Step 757759 | Eval:  -667.85 | P_Loss: -0.0084 | V_Loss: 1.4351\n",
      "23:29:07 [INFO] Update  371 | Step 759807 | Eval:  -663.74 | P_Loss: -0.0122 | V_Loss: 0.7385\n",
      "23:29:13 [INFO] Update  372 | Step 761855 | Eval:  -582.07 | P_Loss: -0.0118 | V_Loss: 0.5885\n",
      "23:29:18 [INFO] Update  373 | Step 763903 | Eval:  -580.39 | P_Loss: -0.0119 | V_Loss: 0.7666\n",
      "23:29:24 [INFO] Update  374 | Step 765951 | Eval:  -171.54 | P_Loss: -0.0138 | V_Loss: 0.3922\n",
      "23:29:30 [INFO] Update  375 | Step 767999 | Eval:  -172.02 | P_Loss: -0.0144 | V_Loss: 0.4097\n",
      "23:29:37 [INFO] Update  376 | Step 770047 | Eval:  -123.86 | P_Loss: -0.0118 | V_Loss: 1.6852\n",
      "23:29:43 [INFO] Update  377 | Step 772095 | Eval: -1048.32 | P_Loss: -0.0104 | V_Loss: 0.8487\n",
      "23:29:48 [INFO] Update  378 | Step 774143 | Eval:  -164.98 | P_Loss: -0.0163 | V_Loss: 0.4366\n",
      "23:29:55 [INFO] Update  379 | Step 776191 | Eval:  -621.55 | P_Loss: -0.0101 | V_Loss: 0.7423\n",
      "23:30:00 [INFO] Decreased policy learning rate to 0.0010125\n",
      "23:30:01 [INFO] Update  380 | Step 778239 | Eval:  -577.76 | P_Loss: -0.0182 | V_Loss: 0.3258\n",
      "23:30:07 [INFO] Decreased policy learning rate to 0.0006749999999999999\n",
      "23:30:07 [INFO] Update  381 | Step 780287 | Eval:  -520.41 | P_Loss: -0.0128 | V_Loss: 0.6305\n",
      "23:30:14 [INFO] Update  382 | Step 782335 | Eval:  -118.32 | P_Loss: -0.0072 | V_Loss: 0.8446\n",
      "23:30:19 [INFO] Update  383 | Step 784383 | Eval: -1014.78 | P_Loss: -0.0082 | V_Loss: 0.6772\n",
      "23:30:25 [INFO] Increased policy learning rate to 0.0010125\n",
      "23:30:25 [INFO] Update  384 | Step 786431 | Eval:  -630.08 | P_Loss: -0.0085 | V_Loss: 0.4679\n",
      "23:30:31 [INFO] Update  385 | Step 788479 | Eval:  -591.29 | P_Loss: -0.0106 | V_Loss: 0.4280\n",
      "23:30:38 [INFO] Update  386 | Step 790527 | Eval:  -125.86 | P_Loss: -0.0094 | V_Loss: 0.9440\n",
      "23:30:44 [INFO] Update  387 | Step 792575 | Eval: -1037.45 | P_Loss: -0.0111 | V_Loss: 0.4369\n",
      "23:30:49 [INFO] Increased policy learning rate to 0.00151875\n",
      "23:30:50 [INFO] Update  388 | Step 794623 | Eval: -1040.11 | P_Loss: -0.0073 | V_Loss: 1.0644\n",
      "23:30:55 [INFO] Update  389 | Step 796671 | Eval:  -578.79 | P_Loss: -0.0094 | V_Loss: 0.7822\n",
      "23:31:02 [INFO] Update  390 | Step 798719 | Eval:  -126.10 | P_Loss: -0.0121 | V_Loss: 0.3638\n",
      "23:31:08 [INFO] Update  391 | Step 800767 | Eval:  -588.96 | P_Loss: -0.0099 | V_Loss: 0.9041\n",
      "23:31:14 [INFO] Update  392 | Step 802815 | Eval:  -633.13 | P_Loss: -0.0058 | V_Loss: 1.6665\n",
      "23:31:21 [INFO] Increased policy learning rate to 0.0022781249999999998\n",
      "23:31:21 [INFO] Update  393 | Step 804863 | Eval:   -86.00 | P_Loss: -0.0082 | V_Loss: 1.6743\n",
      "23:31:27 [INFO] Update  394 | Step 806911 | Eval:  -176.46 | P_Loss: -0.0127 | V_Loss: 0.7665\n",
      "23:31:32 [INFO] Decreased policy learning rate to 0.00151875\n",
      "23:31:33 [INFO] Update  395 | Step 808959 | Eval:   -86.17 | P_Loss: -0.0212 | V_Loss: 0.2943\n",
      "23:31:39 [INFO] Update  396 | Step 811007 | Eval:  -215.53 | P_Loss: -0.0154 | V_Loss: 0.2654\n",
      "23:31:45 [INFO] Update  397 | Step 813055 | Eval:  -191.06 | P_Loss: -0.0109 | V_Loss: 0.8476\n",
      "23:31:52 [INFO] Update  398 | Step 815103 | Eval: -1000.52 | P_Loss: -0.0164 | V_Loss: 0.4381\n",
      "23:31:57 [INFO] Update  399 | Step 817151 | Eval:  -215.93 | P_Loss: -0.0146 | V_Loss: 0.8951\n",
      "23:32:03 [INFO] Update  400 | Step 819199 | Eval:   -85.63 | P_Loss: -0.0131 | V_Loss: 0.2656\n",
      "23:32:09 [INFO] Update  401 | Step 821247 | Eval:  -126.86 | P_Loss: -0.0104 | V_Loss: 0.6380\n",
      "23:32:14 [INFO] Increased policy learning rate to 0.0022781249999999998\n",
      "23:32:15 [INFO] Update  402 | Step 823295 | Eval:  -690.51 | P_Loss: -0.0133 | V_Loss: 0.4704\n",
      "23:32:21 [INFO] Update  403 | Step 825343 | Eval:  -418.66 | P_Loss: -0.0156 | V_Loss: 0.3142\n",
      "23:32:27 [INFO] Update  404 | Step 827391 | Eval:  -665.33 | P_Loss: -0.0155 | V_Loss: 0.5316\n",
      "23:32:33 [INFO] Update  405 | Step 829439 | Eval:  -283.96 | P_Loss: -0.0151 | V_Loss: 0.4685\n",
      "23:32:39 [INFO] Update  406 | Step 831487 | Eval:  -176.20 | P_Loss: -0.0135 | V_Loss: 0.7692\n",
      "23:32:45 [INFO] Decreased policy learning rate to 0.00151875\n",
      "23:32:46 [INFO] Update  407 | Step 833535 | Eval:  -246.06 | P_Loss: -0.0127 | V_Loss: 0.8112\n",
      "23:32:52 [INFO] Update  408 | Step 835583 | Eval:  -133.96 | P_Loss: -0.0119 | V_Loss: 0.4891\n",
      "23:32:57 [INFO] Update  409 | Step 837631 | Eval:  -162.56 | P_Loss: -0.0100 | V_Loss: 0.5762\n",
      "23:33:04 [INFO] Update  410 | Step 839679 | Eval:  -185.44 | P_Loss: -0.0124 | V_Loss: 0.4131\n",
      "23:33:10 [INFO] Update  411 | Step 841727 | Eval:  -542.08 | P_Loss: -0.0144 | V_Loss: 0.5243\n",
      "23:33:16 [INFO] Update  412 | Step 843775 | Eval:  -177.17 | P_Loss: -0.0145 | V_Loss: 0.6443\n",
      "23:33:22 [INFO] Update  413 | Step 845823 | Eval:  -219.80 | P_Loss: -0.0137 | V_Loss: 0.4062\n",
      "23:33:27 [INFO] Update  414 | Step 847871 | Eval:  -583.66 | P_Loss: -0.0131 | V_Loss: 0.4133\n",
      "23:33:33 [INFO] Update  415 | Step 849919 | Eval:  -689.36 | P_Loss: -0.0125 | V_Loss: 0.3825\n",
      "23:33:39 [INFO] Update  416 | Step 851967 | Eval:  -547.41 | P_Loss: -0.0118 | V_Loss: 0.2657\n",
      "23:33:45 [INFO] Update  417 | Step 854015 | Eval:  -588.59 | P_Loss: -0.0099 | V_Loss: 0.4940\n",
      "23:33:50 [INFO] Increased policy learning rate to 0.0022781249999999998\n",
      "23:33:51 [INFO] Update  418 | Step 856063 | Eval:  -186.03 | P_Loss: -0.0113 | V_Loss: 0.4596\n",
      "23:33:55 [INFO] Update  419 | Step 858111 | Eval:  -585.77 | P_Loss: -0.0184 | V_Loss: 0.4256\n",
      "23:34:02 [INFO] Update  420 | Step 860159 | Eval:  -648.55 | P_Loss: -0.0161 | V_Loss: 0.2747\n",
      "23:34:08 [INFO] Update  421 | Step 862207 | Eval:  -176.85 | P_Loss: -0.0128 | V_Loss: 0.5381\n",
      "23:34:14 [INFO] Decreased policy learning rate to 0.00151875\n",
      "23:34:15 [INFO] Update  422 | Step 864255 | Eval:  -127.45 | P_Loss: -0.0160 | V_Loss: 0.5406\n",
      "23:34:21 [INFO] Decreased policy learning rate to 0.0010125\n",
      "23:34:22 [INFO] Update  423 | Step 866303 | Eval:  -140.83 | P_Loss: -0.0125 | V_Loss: 0.5729\n",
      "23:34:27 [INFO] Update  424 | Step 868351 | Eval:  -177.15 | P_Loss: -0.0122 | V_Loss: 0.4381\n",
      "23:34:33 [INFO] Update  425 | Step 870399 | Eval: -1131.19 | P_Loss: -0.0126 | V_Loss: 0.5049\n",
      "23:34:40 [INFO] Update  426 | Step 872447 | Eval: -1501.22 | P_Loss: -0.0114 | V_Loss: 0.4591\n",
      "23:34:46 [INFO] Update  427 | Step 874495 | Eval:    -2.18 | P_Loss: -0.0077 | V_Loss: 0.7120\n",
      "23:34:51 [INFO] Increased policy learning rate to 0.00151875\n",
      "23:34:52 [INFO] Update  428 | Step 876543 | Eval:  -176.35 | P_Loss: -0.0094 | V_Loss: 0.9195\n",
      "23:34:59 [INFO] Update  429 | Step 878591 | Eval:  -542.07 | P_Loss: -0.0092 | V_Loss: 0.5630\n",
      "23:35:04 [INFO] Update  430 | Step 880639 | Eval:   -88.17 | P_Loss: -0.0131 | V_Loss: 0.2537\n",
      "23:35:10 [INFO] Update  431 | Step 882687 | Eval:  -594.36 | P_Loss: -0.0102 | V_Loss: 0.5123\n",
      "23:35:16 [INFO] Update  432 | Step 884735 | Eval:  -586.26 | P_Loss: -0.0134 | V_Loss: 0.6344\n",
      "23:35:23 [INFO] Update  433 | Step 886783 | Eval:  -217.50 | P_Loss: -0.0120 | V_Loss: 0.5011\n",
      "23:35:29 [INFO] Update  434 | Step 888831 | Eval:   -84.26 | P_Loss: -0.0131 | V_Loss: 0.3172\n",
      "23:35:33 [INFO] Decreased policy learning rate to 0.0010125\n",
      "23:35:34 [INFO] Update  435 | Step 890879 | Eval:  -215.54 | P_Loss: -0.0148 | V_Loss: 0.4116\n",
      "23:35:40 [INFO] Increased policy learning rate to 0.00151875\n",
      "23:35:41 [INFO] Update  436 | Step 892927 | Eval:  -261.87 | P_Loss: -0.0085 | V_Loss: 0.4211\n",
      "23:35:47 [INFO] Update  437 | Step 894975 | Eval:   -87.29 | P_Loss: -0.0122 | V_Loss: 0.3987\n",
      "23:35:54 [INFO] Update  438 | Step 897023 | Eval:  -634.97 | P_Loss: -0.0121 | V_Loss: 0.5012\n",
      "23:36:00 [INFO] Update  439 | Step 899071 | Eval:  -172.39 | P_Loss: -0.0097 | V_Loss: 0.3802\n",
      "23:36:04 [INFO] Decreased policy learning rate to 0.0010125\n",
      "23:36:05 [INFO] Update  440 | Step 901119 | Eval:  -666.28 | P_Loss: -0.0183 | V_Loss: 0.2882\n",
      "23:36:11 [INFO] Increased policy learning rate to 0.00151875\n",
      "23:36:12 [INFO] Update  441 | Step 903167 | Eval:  -190.57 | P_Loss: -0.0064 | V_Loss: 0.5428\n",
      "23:36:18 [INFO] Increased policy learning rate to 0.0022781249999999998\n",
      "23:36:19 [INFO] Update  442 | Step 905215 | Eval:  -631.18 | P_Loss: -0.0082 | V_Loss: 0.6909\n",
      "23:36:25 [INFO] Decreased policy learning rate to 0.00151875\n",
      "23:36:26 [INFO] Update  443 | Step 907263 | Eval:  -194.58 | P_Loss: -0.0131 | V_Loss: 0.5745\n",
      "23:36:32 [INFO] Update  444 | Step 909311 | Eval:  -631.28 | P_Loss: -0.0119 | V_Loss: 0.4092\n",
      "23:36:39 [INFO] Update  445 | Step 911359 | Eval:  -175.53 | P_Loss: -0.0080 | V_Loss: 1.0116\n",
      "23:36:44 [INFO] Decreased policy learning rate to 0.0010125\n",
      "23:36:45 [INFO] Update  446 | Step 913407 | Eval:  -132.53 | P_Loss: -0.0121 | V_Loss: 0.6629\n",
      "23:36:51 [INFO] Increased policy learning rate to 0.00151875\n",
      "23:36:52 [INFO] Update  447 | Step 915455 | Eval: -1048.04 | P_Loss: -0.0062 | V_Loss: 0.9209\n",
      "23:36:58 [INFO] Update  448 | Step 917503 | Eval:  -130.38 | P_Loss: -0.0082 | V_Loss: 0.4893\n",
      "23:37:03 [INFO] Update  449 | Step 919551 | Eval:   -84.55 | P_Loss: -0.0097 | V_Loss: 0.3807\n",
      "23:37:09 [INFO] Update  450 | Step 921599 | Eval:  -270.90 | P_Loss: -0.0091 | V_Loss: 0.6211\n",
      "23:37:15 [INFO] Increased policy learning rate to 0.0022781249999999998\n",
      "23:37:16 [INFO] Update  451 | Step 923647 | Eval:  -633.03 | P_Loss: -0.0105 | V_Loss: 0.4123\n",
      "23:37:22 [INFO] Update  452 | Step 925695 | Eval: -1104.16 | P_Loss: -0.0130 | V_Loss: 0.4214\n",
      "23:37:28 [INFO] Increased policy learning rate to 0.0034171874999999997\n",
      "23:37:29 [INFO] Update  453 | Step 927743 | Eval:  -753.76 | P_Loss: -0.0095 | V_Loss: 1.3636\n",
      "23:37:34 [INFO] Update  454 | Step 929791 | Eval: -1029.38 | P_Loss: -0.0109 | V_Loss: 0.8861\n",
      "23:37:41 [INFO] Update  455 | Step 931839 | Eval: -1027.26 | P_Loss: -0.0127 | V_Loss: 0.4976\n",
      "23:37:47 [INFO] Update  456 | Step 933887 | Eval:   -88.13 | P_Loss: -0.0087 | V_Loss: 0.8400\n",
      "23:37:53 [INFO] Update  457 | Step 935935 | Eval:  -128.79 | P_Loss: -0.0125 | V_Loss: 0.5562\n",
      "23:37:59 [INFO] Update  458 | Step 937983 | Eval:   -44.72 | P_Loss: -0.0139 | V_Loss: 0.7342\n",
      "23:38:05 [INFO] Update  459 | Step 940031 | Eval:  -585.99 | P_Loss: -0.0115 | V_Loss: 0.9397\n",
      "23:38:11 [INFO] Update  460 | Step 942079 | Eval: -1087.80 | P_Loss: -0.0088 | V_Loss: 1.1573\n",
      "23:38:17 [INFO] Increased policy learning rate to 0.005125781249999999\n",
      "23:38:17 [INFO] Update  461 | Step 944127 | Eval:   -88.83 | P_Loss: -0.0127 | V_Loss: 0.2878\n",
      "23:38:23 [INFO] Increased policy learning rate to 0.007688671874999999\n",
      "23:38:24 [INFO] Update  462 | Step 946175 | Eval:  -613.48 | P_Loss: -0.0113 | V_Loss: 1.8541\n",
      "23:38:29 [INFO] Increased policy learning rate to 0.01\n",
      "23:38:30 [INFO] Update  463 | Step 948223 | Eval: -1096.03 | P_Loss: -0.0119 | V_Loss: 1.6137\n",
      "23:38:35 [INFO] Update  464 | Step 950271 | Eval:   -48.23 | P_Loss: -0.0164 | V_Loss: 0.9230\n",
      "23:38:41 [INFO] Decreased policy learning rate to 0.006666666666666667\n",
      "23:38:42 [INFO] Update  465 | Step 952319 | Eval:  -678.50 | P_Loss: -0.0160 | V_Loss: 1.0624\n",
      "23:38:48 [INFO] Update  466 | Step 954367 | Eval:  -636.46 | P_Loss: -0.0176 | V_Loss: 0.5818\n",
      "23:38:54 [INFO] Update  467 | Step 956415 | Eval:  -274.47 | P_Loss: -0.0164 | V_Loss: 0.5198\n",
      "23:39:01 [INFO] Update  468 | Step 958463 | Eval: -1006.85 | P_Loss: -0.0155 | V_Loss: 1.0195\n",
      "23:39:06 [INFO] Decreased policy learning rate to 0.0044444444444444444\n",
      "23:39:07 [INFO] Update  469 | Step 960511 | Eval:  -581.78 | P_Loss: -0.0203 | V_Loss: 0.5991\n",
      "23:39:11 [INFO] Increased policy learning rate to 0.006666666666666666\n",
      "23:39:12 [INFO] Update  470 | Step 962559 | Eval:  -582.03 | P_Loss: -0.0103 | V_Loss: 2.0190\n",
      "23:39:18 [INFO] Update  471 | Step 964607 | Eval:  -746.25 | P_Loss: -0.0170 | V_Loss: 0.6993\n",
      "23:39:24 [INFO] Decreased policy learning rate to 0.0044444444444444444\n",
      "23:39:25 [INFO] Update  472 | Step 966655 | Eval:  -680.42 | P_Loss: -0.0139 | V_Loss: 0.8702\n",
      "23:39:31 [INFO] Update  473 | Step 968703 | Eval:  -530.84 | P_Loss: -0.0097 | V_Loss: 1.2646\n",
      "23:39:36 [INFO] Decreased policy learning rate to 0.002962962962962963\n",
      "23:39:37 [INFO] Update  474 | Step 970751 | Eval:  -573.28 | P_Loss: -0.0170 | V_Loss: 0.4888\n",
      "23:39:43 [INFO] Update  475 | Step 972799 | Eval:  -551.81 | P_Loss: -0.0132 | V_Loss: 1.0737\n",
      "23:39:50 [INFO] Update  476 | Step 974847 | Eval:  -634.25 | P_Loss: -0.0117 | V_Loss: 0.9594\n",
      "23:39:54 [INFO] Update  477 | Step 976895 | Eval:  -273.72 | P_Loss: -0.0091 | V_Loss: 1.1116\n",
      "23:40:00 [INFO] Update  478 | Step 978943 | Eval:  -176.91 | P_Loss: -0.0104 | V_Loss: 1.1630\n",
      "23:40:06 [INFO] Update  479 | Step 980991 | Eval:  -154.10 | P_Loss: -0.0138 | V_Loss: 0.7915\n",
      "23:40:13 [INFO] Update  480 | Step 983039 | Eval:   -85.23 | P_Loss: -0.0084 | V_Loss: 1.5567\n",
      "23:40:19 [INFO] Increased policy learning rate to 0.0044444444444444444\n",
      "23:40:19 [INFO] Update  481 | Step 985087 | Eval:  -299.29 | P_Loss: -0.0072 | V_Loss: 1.4104\n",
      "23:40:25 [INFO] Update  482 | Step 987135 | Eval:  -556.81 | P_Loss: -0.0149 | V_Loss: 1.5917\n",
      "23:40:31 [INFO] Update  483 | Step 989183 | Eval:  -529.43 | P_Loss: -0.0105 | V_Loss: 0.5919\n",
      "23:40:38 [INFO] Update  484 | Step 991231 | Eval:  -587.92 | P_Loss: -0.0146 | V_Loss: 0.6347\n",
      "23:40:44 [INFO] Update  485 | Step 993279 | Eval:  -137.08 | P_Loss: -0.0091 | V_Loss: 1.2168\n",
      "23:40:51 [INFO] Update  486 | Step 995327 | Eval: -1012.66 | P_Loss: -0.0118 | V_Loss: 0.9389\n",
      "23:40:56 [INFO] Update  487 | Step 997375 | Eval: -1089.86 | P_Loss: -0.0098 | V_Loss: 0.6837\n",
      "23:41:02 [INFO] Update  488 | Step 999423 | Eval:  -273.24 | P_Loss: -0.0125 | V_Loss: 0.2445\n"
     ]
    }
   ],
   "source": [
    "agent = PPOAgent(Config=Config(),device=device)\n",
    "total_step= 1000000\n",
    "\n",
    "lh, rh = train_ppo(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    total_step=total_step,\n",
    "    batch_steps=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4b24ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2bc1c0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to models/ppo_final_20260201_234103.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def make_unique_path(path: str | Path) -> Path:\n",
    "    \"\"\"\n",
    "    path が既に存在する場合、末尾に _1, _2, ... を付けて未使用のパスを返す。\n",
    "    例: ddpg_final_20251221_235959.pth -> ddpg_final_20251221_235959_1.pth -> ...\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "\n",
    "    # 存在しないならそのまま使う\n",
    "    if not p.exists():\n",
    "        return p\n",
    "\n",
    "    parent = p.parent\n",
    "    stem = p.stem      # 拡張子抜きファイル名\n",
    "    suffix = p.suffix  # \".pth\"\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        cand = parent / f\"{stem}_{i}{suffix}\"\n",
    "        if not cand.exists():\n",
    "            return cand\n",
    "        i += 1\n",
    "\n",
    "\n",
    "# 推論用に eval モードにしておく（保存自体は train のままでも可）\n",
    "agent.mode2eval()\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "models_dir = Path(\"./models\")\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "base_path = models_dir / f\"ppo_final_{stamp}.pth\"\n",
    "save_path = make_unique_path(base_path)\n",
    "\n",
    "agent.save_all(\n",
    "    save_path.as_posix(),\n",
    "    extra={\n",
    "        \"total_step\": int(total_step),\n",
    "        \"reward_history\": rh,  # 必要ならそのままでOK\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
