{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed7508b",
   "metadata": {},
   "source": [
    "# 自作のPPOノートブック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b2c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, asdict, is_dataclass\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from myActivator import tanhAndScale\n",
    "from myFunction import make_squashed_gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25772e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "                    stream=sys.stdout, datefmt=\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc437463",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "for key in vars(env.spec):\n",
    "    logging.info('%s: %s', key, vars(env.spec)[key])\n",
    "for key in vars(env.unwrapped):\n",
    "    logging.info('%s: %s', key, vars(env.unwrapped)[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5818b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd64991",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # ===== Pendulum-v1 specs =====\n",
    "    obs_dim: int = 3\n",
    "    act_dim: int = 1\n",
    "\n",
    "    # TRPOAgent 側は Config.u_llim / Config.u_ulim を参照\n",
    "    u_llim: list[float] = field(default_factory=lambda: [-2.0])\n",
    "    u_ulim: list[float] = field(default_factory=lambda: [ 2.0])\n",
    "\n",
    "    # ===== Network architecture =====\n",
    "    V_net_in: int = 3\n",
    "    P_net_in: int = 3\n",
    "\n",
    "    V_net_sizes: list[int] = field(default_factory=lambda: [64, 64])\n",
    "    P_net_sizes: list[int] = field(default_factory=lambda: [64, 64])\n",
    "\n",
    "    V_net_out: int = 1\n",
    "    P_net_out: int = 1  # = act_dim\n",
    "\n",
    "    # ===== Optimizer =====\n",
    "    V_lr: float = 1e-3\n",
    "    P_lr: float = 3e-4\n",
    "\n",
    "    # ===== GAE / discount =====\n",
    "    gamma: float = 0.99\n",
    "    lam: float = 0.97\n",
    "\n",
    "    # ===== PPO hyperparameters =====\n",
    "    clip_ratio: float = 0.2\n",
    "    policy_train_iters: int = 50\n",
    "    target_kl: float = 0.01\n",
    "    reward_scaling: float = 0.01\n",
    "\n",
    "    # ===== Value function training =====\n",
    "    value_train_iters: int = 50\n",
    "    value_l2_reg: float = 1e-3\n",
    "    v_clip_epsilon: float = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aa0a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, Config, device=None):\n",
    "        if Config is None:\n",
    "            raise ValueError(\"No Config!!\")\n",
    "        self.Config = Config\n",
    "\n",
    "        # device\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "\n",
    "        self.u_low = torch.tensor(Config.u_llim, dtype=torch.float32, device=self.device)\n",
    "        self.u_high = torch.tensor(Config.u_ulim, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # networks\n",
    "        self.V_net = self.build_net(Config.V_net_in, Config.V_net_sizes, Config.V_net_out).to(self.device)\n",
    "        self.P_net = self.build_net(Config.P_net_in, Config.P_net_sizes, Config.P_net_out).to(self.device)\n",
    "\n",
    "        self.V_net.train()\n",
    "        self.P_net.train()\n",
    "\n",
    "        # log_std は状態に依存しないパラメータ\n",
    "        action_dim = Config.P_net_out\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim, device=self.device))\n",
    "\n",
    "        # optimizer（baselineはAdam）\n",
    "        self.V_optim = optim.Adam(self.V_net.parameters(), lr=Config.V_lr)\n",
    "        self.P_optim = optim.Adam(\n",
    "            list(self.P_net.parameters()) + [self.log_std], \n",
    "            lr=Config.P_lr\n",
    "        )\n",
    "\n",
    "        # hyperparams\n",
    "        self.gamma = float(Config.gamma)\n",
    "        self.tau = float(Config.lam)  # baselineの TAU (= GAE lambda)\n",
    "        self.target_kl = float(getattr(Config, \"target_kl\", 0.01))\n",
    "        self.reward_scaling = float(getattr(Config, \"reward_scaling\", 0.01))\n",
    "\n",
    "        self.policy_train_iters = int(getattr(Config, \"policy_train_iters\", 80))\n",
    "        self.value_train_iters = int(getattr(Config, \"value_train_iters\", 5))\n",
    "        self.value_l2_reg = float(getattr(Config, \"value_l2_reg\", 1e-3))\n",
    "        self.v_clip_epsilon = float(getattr(Config, \"v_clip_epsilon\", 0.2))\n",
    "\n",
    "    def build_net(self, input_size, hidden_sizes, output_size):\n",
    "        layers = []\n",
    "        in_size = input_size\n",
    "        for h_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_size, h_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_size = h_size\n",
    "        layers.append(nn.Linear(in_size, output_size))\n",
    "        net = nn.Sequential(*layers)\n",
    "        return net\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_action_and_log_prob(self, state, deterministic=False):\n",
    "        \"\"\"\n",
    "        deterministic: Trueなら平均値(mean)を返す。Falseならサンプリング。\n",
    "        並列学習に対応できる様に余分なsqueezeを削除（返す値が一つしかない場合のみ次元を削るように変更）\n",
    "        \"\"\"\n",
    "        s = torch.as_tensor(state, dtype=torch.float32, device=self.device)\n",
    "        if s.dim() == 1:\n",
    "            s = s.unsqueeze(0)  # (1, obs_dim)\n",
    "\n",
    "        dist = self._policy_dist(s)  # Normal(mean, std)\n",
    "\n",
    "        if deterministic:\n",
    "            a = dist.mean  # (1, act_dim) or (N, act_dim)\n",
    "            logp = None\n",
    "        else:\n",
    "            a = dist.sample()\n",
    "            logp = dist.log_prob(a).sum(axis=-1)  # (1,) or (N,)\n",
    "\n",
    "        # 返すactionはclip前\n",
    "        # envに入れるときにnp.clipする\n",
    "        if a.shape[0] == 1:\n",
    "            a = a.squeeze(0)\n",
    "            if logp is not None:\n",
    "                logp = logp.squeeze(0)\n",
    "        return a, logp\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self, state):\n",
    "        \"\"\"\n",
    "        推論時に使う用のwrapper関数\n",
    "        \"\"\"\n",
    "        a, _ = self.get_action_and_log_prob(state, deterministic=True)\n",
    "        return a.cpu().numpy()\n",
    "    \n",
    "    def _policy_mean(self, states):\n",
    "        \"\"\"\n",
    "        方策ネットから行動平均を計算するラッパー関数\n",
    "        states: (batch_size, obs_dim)\n",
    "        return: (batch_size, act_dim)\n",
    "        \"\"\"\n",
    "        mean = self.P_net(states)  # (batch_size, act_dim)\n",
    "        return mean\n",
    "    \n",
    "    def _policy_dist(self, states):\n",
    "        \"\"\"\n",
    "        方策ネットから平均を計算し、パラメータから分散を計算して、正規分布を返すラッパー関数\n",
    "        states: (batch_size, obs_dim)\n",
    "        return: Normal distribution\n",
    "        \"\"\"\n",
    "        mean = self._policy_mean(states)  # (batch_size, act_dim) or (batch_size, N, act_dim)\n",
    "        std = torch.exp(self.log_std)  # (act_dim,)\n",
    "        # std = std.unsqueeze(0).expand_as(mean)  # (batch_size, act_dim) or (batch_size, N, act_dim)\n",
    "        # stdを拡張しなくてもNormal内部で自動でbroadcastされるので問題ないはず\n",
    "        dist = Normal(mean, std)\n",
    "        return dist\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _compute_gae(self, rewards, values, next_values, dones):\n",
    "        \"\"\"\n",
    "        GAEを計算する関数\n",
    "        並列環境計算にも対応させる\n",
    "        rewards: (batch_size,) or (batch_size, N)\n",
    "        values: (batch_size,) or (batch_size, N)\n",
    "        next_values: (batch_size,) or (batch_size, N)\n",
    "        dones: (batch_size,) or (batch_size, N)\n",
    "        return: advantages: (batch_size,) or (batch_size, N), returns: (batch_size,) or (batch_size, N)\n",
    "        \"\"\"\n",
    "        batch_size = rewards.shape[0]\n",
    "        adv = torch.zeros_like(rewards, device=self.device)\n",
    "        if rewards.dim() == 1:\n",
    "            gae = 0.0\n",
    "        elif rewards.dim() == 2:\n",
    "            gae = torch.zeros(rewards.shape[1], device=self.device)\n",
    "\n",
    "        for t in reversed(range(batch_size)):\n",
    "            if t == batch_size - 1:\n",
    "                nv = next_values[t]\n",
    "            else:\n",
    "                nv = values[t + 1]\n",
    "            delta = rewards[t] + self.gamma * nv * (1 - dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.tau * (1 - dones[t]) * gae\n",
    "            adv[t] = gae\n",
    "\n",
    "        ret = adv + values\n",
    "        return adv, ret\n",
    "    \n",
    "    def _ppo_step(self, states, actions, old_log_probs, advantages):\n",
    "        \"\"\"\n",
    "        PPOの方策ネット更新を行う関数\n",
    "        states: (batch_size, obs_dim) or (batch_size, N, obs_dim)\n",
    "        actions: (batch_size, act_dim) or (batch_size, N, act_dim)\n",
    "        old_log_probs: (batch_size,) or (batch_size, N)\n",
    "        advantages: (batch_size,)  or (batch_size, N)\n",
    "        return: policy_loss\n",
    "        \"\"\"\n",
    "\n",
    "        for _ in range(self.policy_train_iters):\n",
    "            dist = self._policy_dist(states)  # Normal(mean, std)\n",
    "            log_probs = dist.log_prob(actions).sum(axis=-1)  # (batch_size,) or (batch_size, N)\n",
    "\n",
    "            ratios = torch.exp(log_probs - old_log_probs)  # (batch_size,) or (batch_size, N)\n",
    "\n",
    "            surr1 = ratios * advantages  # (batch_size,) or (batch_size, N)\n",
    "            surr2 = torch.clamp(ratios, 1.0 - self.Config.clip_ratio, 1.0 + self.Config.clip_ratio) * advantages  # (batch_size,) or (batch_size, N)\n",
    "\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            self.P_optim.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.P_optim.step()\n",
    "\n",
    "        # どれくらい変化したかを確認する\n",
    "        change = (old_log_probs - log_probs).mean()\n",
    "\n",
    "        return policy_loss, change\n",
    "    \n",
    "    def _update_value_function(self, states, returns, old_values):\n",
    "        \"\"\"\n",
    "        価値関数ネットワークの更新を行う関数\n",
    "        states: (batch_size, obs_dim) or (batch_size, N, obs_dim)\n",
    "        returns: (batch_size,) or (batch_size, N)\n",
    "        return: value_loss\n",
    "        \"\"\"\n",
    "        for _ in range(self.value_train_iters):\n",
    "            values = self.V_net(states).squeeze(-1)  # (batch_size,) or (batch_size, N)\n",
    "            value_loss = F.mse_loss(values, returns)\n",
    "\n",
    "            # クリッピング版の価値関数損失（Vの計算が暴走するのを防ぐため）\n",
    "            v_clip = old_values + torch.clamp(values - old_values, -self.v_clip_epsilon, self.v_clip_epsilon)\n",
    "            v_clip_loss = F.mse_loss(v_clip, returns)\n",
    "\n",
    "            # L2正則化\n",
    "            l2_reg = torch.tensor(0., device=self.device)\n",
    "            for param in self.V_net.parameters():\n",
    "                l2_reg += torch.norm(param)**2\n",
    "            loss = torch.max(value_loss, v_clip_loss) + self.value_l2_reg * l2_reg\n",
    "            # loss = value_loss + self.value_l2_reg * l2_reg\n",
    "\n",
    "            self.V_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            self.V_optim.step()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def update_net(self, states, actions, log_probs, rewards, next_states, dones):\n",
    "        \"\"\"\n",
    "        ネットワークを更新する関数\n",
    "        states: (batch_size, obs_dim)\n",
    "        actions: (batch_size, act_dim)\n",
    "        log_probs: (batch_size,)\n",
    "        rewards: (batch_size,)\n",
    "        next_states: (batch_size, obs_dim)\n",
    "        dones: (batch_size,)\n",
    "        return: dict of losses\n",
    "        \"\"\"\n",
    "        states = torch.as_tensor(states, dtype=torch.float32, device=self.device)\n",
    "        actions = torch.as_tensor(actions, dtype=torch.float32, device=self.device)\n",
    "        old_log_probs = torch.as_tensor(log_probs, dtype=torch.float32, device=self.device)\n",
    "        rewards = torch.as_tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        next_states = torch.as_tensor(next_states, dtype=torch.float32, device=self.device)\n",
    "        dones = torch.as_tensor(dones, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # GAEの計算\n",
    "        with torch.no_grad():\n",
    "            values = self.V_net(states).squeeze(-1)  # (batch_size,) or (batch_size, N)\n",
    "            next_values = self.V_net(next_states).squeeze(-1)  # (batch_size,) or (batch_size, N)\n",
    "\n",
    "            advantages, returns = self._compute_gae(rewards, values, next_values, dones)\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)  # 正規化\n",
    "\n",
    "        # 方策ネットワークの更新\n",
    "        policy_loss, kl_change = self._ppo_step(states, actions, old_log_probs, advantages)\n",
    "\n",
    "        # 変化量に応じて学習率を調整\n",
    "        if kl_change > 1.5 * self.target_kl:\n",
    "            for param_group in self.P_optim.param_groups:\n",
    "                param_group['lr'] = max(param_group['lr'] / 1.5, 1e-5)\n",
    "                logging.info(f\"Decreased policy learning rate to {param_group['lr']}\")\n",
    "        elif kl_change < self.target_kl / 1.5:\n",
    "            for param_group in self.P_optim.param_groups:\n",
    "                param_group['lr'] = min(param_group['lr'] * 1.5, 1e-2)\n",
    "                logging.info(f\"Increased policy learning rate to {param_group['lr']}\")\n",
    "\n",
    "        # 価値観数ネットワークの更新\n",
    "        value_loss = self._update_value_function(states, returns, values)\n",
    "\n",
    "        return {\"policy_loss\": policy_loss.item(), \"value_loss\": value_loss.item()}\n",
    "    \n",
    "    def to(self, device):\n",
    "        self.device = torch.device(device)\n",
    "        self.V_net.to(self.device)\n",
    "        self.P_net.to(self.device)\n",
    "        self.log_std.data = self.log_std.data.to(self.device)\n",
    "        self.u_low = self.u_low.to(self.device)\n",
    "        self.u_high = self.u_high.to(self.device)\n",
    "        return self\n",
    "\n",
    "    def mode2eval(self):\n",
    "        self.V_net.eval()\n",
    "        self.P_net.eval()\n",
    "\n",
    "    def mode2train(self):\n",
    "        self.V_net.train()\n",
    "        self.P_net.train()\n",
    "\n",
    "    def save_all(self, path: str, extra: dict | None = None):\n",
    "        cfg = asdict(self.Config) if is_dataclass(self.Config) else self.Config\n",
    "        save_dict = {\n",
    "            \"config\": cfg,\n",
    "            \"V_net_state_dict\": self.V_net.state_dict(),\n",
    "            \"P_net_state_dict\": self.P_net.state_dict(),\n",
    "            \"log_std\": self.log_std.data,\n",
    "        }\n",
    "        if extra is not None:\n",
    "            save_dict.update(extra)\n",
    "        torch.save(save_dict, path)\n",
    "        \n",
    "    def load_all(self, path: str, map_location=None ,weights_only=False):\n",
    "        load_dict = torch.load(path, map_location=map_location, weights_only=weights_only)\n",
    "        self.V_net.load_state_dict(load_dict[\"V_net_state_dict\"])\n",
    "        self.P_net.load_state_dict(load_dict[\"P_net_state_dict\"])\n",
    "        self.log_std.data = load_dict[\"log_std\"].to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71815088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "def train_ppo(\n",
    "    env,\n",
    "    agent,\n",
    "    total_step: int = 200_000,\n",
    "    batch_steps: int | None = None,\n",
    "    random_steps: int = 0,\n",
    "    bootstrap_on_timeout: bool | None = None,\n",
    "    log_interval_updates: int = 1,\n",
    "):\n",
    "    \"\"\"\n",
    "    PPO Agentのための学習ループ\n",
    "    \"\"\"\n",
    "    if batch_steps is None:\n",
    "        batch_steps = int(getattr(agent.Config, \"batch_steps\", 2048)) # PPOはバッチサイズ大きめが一般的\n",
    "    if bootstrap_on_timeout is None:\n",
    "        bootstrap_on_timeout = bool(getattr(agent.Config, \"bootstrap_on_timeout\", False))\n",
    "\n",
    "    print(f\"Start PPO Training: Device={agent.device}, Batch={batch_steps}\")\n",
    "\n",
    "    # 変数名修正に伴う変更\n",
    "    low_np = agent.u_low.detach().cpu().numpy()\n",
    "    high_np = agent.u_high.detach().cpu().numpy()\n",
    "\n",
    "    # 履歴保存用\n",
    "    loss_history = {\"policy_loss\": [], \"value_loss\": []}\n",
    "    train_reward_history = [] # ノイズあり（学習中の報酬）\n",
    "\n",
    "    # rollout buffer\n",
    "    rollout = {\"obs\": [], \"act\": [], \"logp\": [], \"rew\": [], \"obs_next\": [], \"done\": []}\n",
    "\n",
    "    def rollout_clear():\n",
    "        for k in rollout:\n",
    "            rollout[k].clear()\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    ep_return = 0.0\n",
    "    episode_num = 0\n",
    "    update_num = 0\n",
    "\n",
    "    for t in range(total_step):\n",
    "        # --- (1) 行動選択 ---\n",
    "        # ランダムステップ期間、もしくは学習初期\n",
    "        if t < random_steps:\n",
    "            action_raw = np.atleast_1d(env.action_space.sample()).astype(np.float32)\n",
    "            # ランダム行動の場合のlogpは適当(0.0)あるいは計算不要だが、\n",
    "            # PPOの更新で使うなら整合性を取るためにAgentからサンプリングした方が無難。\n",
    "            # ここではあくまで「完全ランダム」として扱うため、logp=0としてUpdateに使わない手もあるが、\n",
    "            # 実装を単純にするため、random_steps期間はバッファに入れないか、\n",
    "            # もしくはagentを使ってサンプリングする形が推奨されます。\n",
    "            # 今回は「agentを使う」形に倒します。\n",
    "            with torch.no_grad():\n",
    "                a_t, logp_t = agent.get_action_and_log_prob(obs, deterministic=False)\n",
    "                logp_val = logp_t.item()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # deterministic=False (確率的方策に従って探索)\n",
    "                a_t, logp_t = agent.get_action_and_log_prob(obs, deterministic=False)\n",
    "            \n",
    "            action_raw = np.atleast_1d(a_t.detach().cpu().numpy()).astype(np.float32)\n",
    "            logp_val = logp_t.item()\n",
    "\n",
    "        # --- (2) env step ---\n",
    "        # クリップして環境に入力\n",
    "        action_env = np.clip(action_raw, low_np, high_np)\n",
    "        obs_next, reward, terminated, truncated, info = env.step(action_env)\n",
    "\n",
    "        ep_return += float(reward)\n",
    "\n",
    "        # GAE計算用のdoneフラグ (TimeLimitによる打ち切りはFalse扱いにする場合が多い)\n",
    "        if bootstrap_on_timeout:\n",
    "            done_for_gae = float(terminated)\n",
    "        else:\n",
    "            done_for_gae = float(terminated or truncated)\n",
    "        \n",
    "        # 環境リセット用のdoneフラグ\n",
    "        done_for_reset = (terminated or truncated)\n",
    "\n",
    "        # 【AI用】学習用の変数は「スケーリングした reward」を作る\n",
    "        scaled_reward = float(reward) * agent.Config.reward_scaling  # Pendulum用に 1/100 にする\n",
    "        # scaled_reward = float(reward)\n",
    "\n",
    "        # --- (3) バッファに保存 ---\n",
    "        rollout[\"obs\"].append(np.asarray(obs, dtype=np.float32))\n",
    "        rollout[\"act\"].append(np.asarray(action_raw, dtype=np.float32))\n",
    "        rollout[\"logp\"].append(logp_val)\n",
    "        # rollout[\"rew\"].append(float(reward))\n",
    "        rollout[\"rew\"].append(float(scaled_reward))\n",
    "        rollout[\"obs_next\"].append(np.asarray(obs_next, dtype=np.float32))\n",
    "        rollout[\"done\"].append(float(done_for_gae))\n",
    "\n",
    "        # --- (4) Reset判定 ---\n",
    "        if done_for_reset:\n",
    "            train_reward_history.append(ep_return)\n",
    "            episode_num += 1\n",
    "            ep_return = 0.0\n",
    "            obs, info = env.reset()\n",
    "        else:\n",
    "            obs = obs_next\n",
    "\n",
    "        # --- (5) Update ---\n",
    "        # バッチサイズ分たまったら更新\n",
    "        if len(rollout[\"obs\"]) >= batch_steps:\n",
    "            update_num += 1\n",
    "\n",
    "            # list -> numpy\n",
    "            states      = np.stack(rollout[\"obs\"], axis=0)\n",
    "            actions     = np.stack(rollout[\"act\"], axis=0)\n",
    "            log_probs   = np.array(rollout[\"logp\"], dtype=np.float32)\n",
    "            rewards     = np.array(rollout[\"rew\"], dtype=np.float32)\n",
    "            states_next = np.stack(rollout[\"obs_next\"], axis=0)\n",
    "            dones       = np.array(rollout[\"done\"], dtype=np.float32)\n",
    "\n",
    "            # Update実行\n",
    "            loss_dict = agent.update_net(states, actions, log_probs, rewards, states_next, dones)\n",
    "            \n",
    "            # バッファクリア\n",
    "            rollout_clear()\n",
    "\n",
    "            # ログ記録\n",
    "            loss_history[\"policy_loss\"].append(loss_dict[\"policy_loss\"])\n",
    "            loss_history[\"value_loss\"].append(loss_dict[\"value_loss\"])\n",
    "\n",
    "            # 評価とログ出力\n",
    "            if (update_num % log_interval_updates) == 0:\n",
    "                # 決定論的モードで評価\n",
    "                eval_score = evaluate(env, agent, n_episodes=3)\n",
    "                \n",
    "                logging.info(\n",
    "                    f\"Update {update_num:4d} | Step {t:6d} | \"\n",
    "                    f\"Eval: {eval_score:8.2f} | \"\n",
    "                    f\"P_Loss: {loss_dict['policy_loss']:.4f} | \"\n",
    "                    f\"V_Loss: {loss_dict['value_loss']:.4f}\"\n",
    "                )\n",
    "\n",
    "    return loss_history, train_reward_history\n",
    "\n",
    "# 評価用関数（元のコードと同じものでOKですが、念のため再掲）\n",
    "def evaluate(env, agent, n_episodes=3):\n",
    "    scores = []\n",
    "    # 変数名修正対応\n",
    "    low_np = agent.u_low.detach().cpu().numpy()\n",
    "    high_np = agent.u_high.detach().cpu().numpy()\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        score = 0.0\n",
    "        while not done:\n",
    "            # step() は deterministic=True になっているはず\n",
    "            action = agent.step(obs)\n",
    "            action = np.clip(action, low_np, high_np)\n",
    "            \n",
    "            obs, rew, term, trunc, _ = env.step(action)\n",
    "            score += rew\n",
    "            done = term or trunc\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c1a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPOAgent(Config=Config(),device=device)\n",
    "total_step= 1000000\n",
    "\n",
    "lh, rh = train_ppo(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    total_step=total_step,\n",
    "    batch_steps=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b24ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc1c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def make_unique_path(path: str | Path) -> Path:\n",
    "    \"\"\"\n",
    "    path が既に存在する場合、末尾に _1, _2, ... を付けて未使用のパスを返す。\n",
    "    例: ddpg_final_20251221_235959.pth -> ddpg_final_20251221_235959_1.pth -> ...\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "\n",
    "    # 存在しないならそのまま使う\n",
    "    if not p.exists():\n",
    "        return p\n",
    "\n",
    "    parent = p.parent\n",
    "    stem = p.stem      # 拡張子抜きファイル名\n",
    "    suffix = p.suffix  # \".pth\"\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        cand = parent / f\"{stem}_{i}{suffix}\"\n",
    "        if not cand.exists():\n",
    "            return cand\n",
    "        i += 1\n",
    "\n",
    "\n",
    "# 推論用に eval モードにしておく（保存自体は train のままでも可）\n",
    "agent.mode2eval()\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "models_dir = Path(\"./models\")\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "base_path = models_dir / f\"ppo_final_{stamp}.pth\"\n",
    "save_path = make_unique_path(base_path)\n",
    "\n",
    "agent.save_all(\n",
    "    save_path.as_posix(),\n",
    "    extra={\n",
    "        \"total_step\": int(total_step),\n",
    "        \"reward_history\": rh,  # 必要ならそのままでOK\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
