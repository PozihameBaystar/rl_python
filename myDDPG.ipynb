{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf0a429",
   "metadata": {},
   "source": [
    "# 自作のDDPGノートブック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccfb1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "from dataclasses import dataclass, asdict, is_dataclass\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import sys\n",
    "import logging\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical, Normal\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from myActivator import tanhAndScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "                    stream=sys.stdout, datefmt=\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6695230",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\",render_mode=\"human\")\n",
    "for key in vars(env.spec):\n",
    "    logging.info('%s: %s', key, vars(env.spec)[key])\n",
    "for key in vars(env.unwrapped):\n",
    "    logging.info('%s: %s', key, vars(env.unwrapped)[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f4152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # ニューラルネットの設定\n",
    "    Q_net_sizes = [6,12,12,6]\n",
    "    P_net_sizes = [6,12,6]\n",
    "    Q_net_in = 4\n",
    "    P_net_in = 3\n",
    "    Q_net_out = 1\n",
    "    P_net_out = 1\n",
    "\n",
    "    # 環境の制約\n",
    "    u_ulim = 2.0\n",
    "    u_llim = -2.0\n",
    "    \n",
    "    # 学習に関するパラメータ\n",
    "    Q_lr = 1e-3\n",
    "    P_lr = 1e-3\n",
    "    gamma = 0.9  # 割引率\n",
    "    sig = 0.3    # 探索の標準偏差\n",
    "    tau = 5e-3    # ターゲットネットの更新幅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb55cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self,Config,device=None):\n",
    "        if Config:\n",
    "            self.Config = Config\n",
    "        else:\n",
    "            raise ValueError(\"No Config!!\")\n",
    "        \n",
    "        # ---- device 決定（指定がなければ CUDA があれば CUDA）----\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "\n",
    "        # ---- action bounds（device を揃えるため Tensor 化）----\n",
    "        self.u_low = torch.as_tensor(Config.u_llim, dtype=torch.float32, device=self.device)\n",
    "        self.u_high = torch.as_tensor(Config.u_ulim, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        self.Q_net = self.build_net(\n",
    "            Config.Q_net_in,\n",
    "            Config.Q_net_sizes,\n",
    "            Config.Q_net_out,\n",
    "            ).to(self.device)\n",
    "        self.Q_net.train()\n",
    "\n",
    "        self.P_net = self.build_net(\n",
    "            Config.P_net_in,\n",
    "            Config.P_net_sizes,\n",
    "            Config.P_net_out,\n",
    "            tanhAndScale(a_high=self.u_high,a_low=self.u_low),\n",
    "            ).to(self.device)\n",
    "        self.P_net.train()\n",
    "\n",
    "        # ---- Target nets（重要：deepcopy で別物を作る）----\n",
    "        self.Q_target_net = copy.deepcopy(self.Q_net).to(self.device)\n",
    "        self.P_target_net = copy.deepcopy(self.P_net).to(self.device)\n",
    "        self.Q_target_net.eval()\n",
    "        self.P_target_net.eval()\n",
    "\n",
    "        self.Q_optim = optim.Adam(self.Q_net.parameters(),lr=Config.Q_lr)\n",
    "        self.P_optim = optim.Adam(self.P_net.parameters(),lr=Config.P_lr)\n",
    "\n",
    "    def to(self, device):\n",
    "        \"\"\"エージェント内部のネットと必要Tensorを指定 device に移す。\"\"\"\n",
    "        self.device = torch.device(device)\n",
    "        self.Q_net.to(self.device)\n",
    "        self.P_net.to(self.device)\n",
    "        self.Q_target_net.to(self.device)\n",
    "        self.P_target_net.to(self.device)\n",
    "        self.u_low = self.u_low.to(self.device)\n",
    "        self.u_high = self.u_high.to(self.device)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def build_net(self, input_size, hidden_sizes, output_size=1, output_activator=None):\n",
    "        layers = []\n",
    "        for input_size, output_size in zip([input_size]+hidden_sizes, hidden_sizes+[output_size]):\n",
    "            layers.append(nn.Linear(input_size, output_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers = layers[:-1]  # 最後のReLUだけ取り除く\n",
    "        if output_activator:\n",
    "            layers.append(output_activator)\n",
    "        net = nn.Sequential(*layers)\n",
    "        return net\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, observation) -> np.ndarray:\n",
    "        \"\"\"ノイズなし（評価用）。環境に渡す行動を返す。\"\"\"\n",
    "        obs_t = torch.as_tensor(observation, dtype=torch.float32, device=self.device)\n",
    "        if obs_t.dim() == 1:\n",
    "            obs_t = obs_t.unsqueeze(0)\n",
    "\n",
    "        action = self.P_net(obs_t)\n",
    "        action = torch.clamp(action, self.u_low, self.u_high)\n",
    "        return action.squeeze(0).cpu().numpy()\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step_with_noise(self, observation):\n",
    "        # 1) observation を Tensor にし、ネットと同じ device に載せる\n",
    "        obs = torch.as_tensor(observation, dtype=torch.float32, device=self.device)\n",
    "        if obs.dim() == 1:\n",
    "            obs = obs.unsqueeze(0)  # (obs_dim,) -> (1, obs_dim)\n",
    "\n",
    "        # 2) 決定論的行動 a = μθ(s)\n",
    "        action = self.P_net(obs)  # shape: (1, act_dim) を想定\n",
    "\n",
    "        # 3) ε ~ N(0, σ^2 I) を生成して加算（探索）\n",
    "        eps = float(self.Config.sig) * torch.randn_like(action)\n",
    "        action = action + eps\n",
    "\n",
    "        # 4) 出力制約 [u_low, u_high] に収める（安全弁）\n",
    "        action = torch.clamp(action, self.u_low, self.u_high)\n",
    "\n",
    "        # 5) 環境に渡すならバッチ次元を落として返す（numpy が必要なら .cpu().numpy()）\n",
    "        return action.squeeze(0).cpu().numpy()\n",
    "    \n",
    "\n",
    "    def save_all(self, path: str, extra: dict | None = None):\n",
    "        \"\"\"\n",
    "        Actor/Critic + target nets をまとめて保存（最終モデル用）。\n",
    "        \"\"\"\n",
    "        cfg = asdict(self.Config) if is_dataclass(self.Config) else self.Config\n",
    "\n",
    "        ckpt = {\n",
    "            \"config\": cfg,\n",
    "            \"P_net\": self.P_net.state_dict(),\n",
    "            \"Q_net\": self.Q_net.state_dict(),\n",
    "            \"P_target_net\": self.P_target_net.state_dict(),\n",
    "            \"Q_target_net\": self.Q_target_net.state_dict(),\n",
    "        }\n",
    "        if extra is not None:\n",
    "            ckpt[\"extra\"] = extra\n",
    "\n",
    "        torch.save(ckpt, path)\n",
    "\n",
    "\n",
    "    def load_all(self, path: str, map_location=None):\n",
    "        \"\"\"\n",
    "        save_all() で保存したチェックポイントをロード。\n",
    "        device を変えて読みたい場合は map_location=\"cpu\" などを指定。\n",
    "        \"\"\"\n",
    "        ckpt = torch.load(path, map_location=map_location)\n",
    "\n",
    "        self.P_net.load_state_dict(ckpt[\"P_net\"])\n",
    "        self.Q_net.load_state_dict(ckpt[\"Q_net\"])\n",
    "        self.P_target_net.load_state_dict(ckpt[\"P_target_net\"])\n",
    "        self.Q_target_net.load_state_dict(ckpt[\"Q_target_net\"])\n",
    "\n",
    "        return ckpt.get(\"extra\", None)\n",
    "    \n",
    "\n",
    "    def mode2eval(self):\n",
    "        self.P_net.eval()\n",
    "        self.Q_net.eval()\n",
    "\n",
    "\n",
    "    def mode2train(self):\n",
    "        self.P_net.train()\n",
    "        self.Q_net.train()\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def soft_update(self, target_net, online_net, tau):\n",
    "        \"\"\"\n",
    "        Polyak averaging:\n",
    "          θ' ← (1-τ) θ' + τ θ\n",
    "        \"\"\"\n",
    "        for target_param, online_param in zip(target_net.parameters(), online_net.parameters()):\n",
    "            target_param.mul_(1.0 - tau).add_(tau * online_param)\n",
    "\n",
    "    \n",
    "    def update_net(self,states,actions,rewards,states_next,dones=None):\n",
    "        \"\"\"\n",
    "        1回の更新（Critic→Actor→Target soft update）\n",
    "        戻り値： (q_loss, p_loss) のスカラー\n",
    "        \"\"\"\n",
    "        # ---- minibatch を device 上 Tensor に統一 ----\n",
    "        states = torch.as_tensor(states, dtype=torch.float32, device=self.device)\n",
    "        actions = torch.as_tensor(actions, dtype=torch.float32, device=self.device)\n",
    "        rewards = torch.as_tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        states_next = torch.as_tensor(states_next, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        if rewards.dim() == 1:\n",
    "            rewards = rewards.unsqueeze(1)\n",
    "\n",
    "        if dones is None:\n",
    "            dones = torch.zeros((states.shape[0], 1), dtype=torch.float32, device=self.device)\n",
    "        else:\n",
    "            dones = torch.as_tensor(dones, dtype=torch.float32, device=self.device)\n",
    "            if dones.dim() == 1:\n",
    "                dones = dones.unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            actions_next_for_target = self.P_target_net(states_next)\n",
    "            y_targets = rewards + self.Config.gamma*(1-dones)*self.Q_target_net(torch.cat([states_next, actions_next_for_target], dim=1))\n",
    "        \n",
    "        Q_values = self.Q_net(torch.cat([states,actions],dim=1))\n",
    "        Q_loss = F.mse_loss(y_targets,Q_values)\n",
    "        self.Q_optim.zero_grad()\n",
    "        Q_loss.backward()\n",
    "        self.Q_optim.step()\n",
    "\n",
    "        # ---- Actor update ----\n",
    "        # Actor 更新では Q_net を通すが、Q_net 自体は更新しないので凍結（計算の節約＋安全）\n",
    "        for p in self.Q_net.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        \n",
    "        actions_for_Ploss = self.P_net(states)\n",
    "        P_loss = -self.Q_net(torch.cat([states,actions_for_Ploss],dim=1)).mean()\n",
    "        self.P_optim.zero_grad()\n",
    "        P_loss.backward()\n",
    "        self.P_optim.step()\n",
    "\n",
    "        for p in self.Q_net.parameters():\n",
    "            p.requires_grad_(True)\n",
    "\n",
    "        self.soft_update(\n",
    "            target_net=self.Q_target_net,\n",
    "            online_net=self.Q_net,\n",
    "            tau=self.Config.tau\n",
    "            )\n",
    "        self.soft_update(\n",
    "            target_net=self.P_target_net,\n",
    "            online_net=self.P_net,\n",
    "            tau=self.Config.tau\n",
    "            )\n",
    "        \n",
    "        return float(Q_loss.item()), float(P_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71695fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from myReplayBuffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e097cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    env,\n",
    "    agent,\n",
    "    buffer,\n",
    "    total_step=40000,\n",
    "    warmup_steps=1000,\n",
    "    batch_num=512,\n",
    "):\n",
    "    print(\"cuda available:\", torch.cuda.is_available())\n",
    "    print(\"agent device:\", agent.device)\n",
    "    print(\"P_net device:\", next(agent.P_net.parameters()).device)\n",
    "    print(\"Q_net device:\", next(agent.Q_net.parameters()).device)\n",
    "    \n",
    "    # ログ保存用のリスト\n",
    "    Q_loss_history = []\n",
    "    P_loss_history = []\n",
    "    episode_num = 1\n",
    "    reward_history = []\n",
    "    reward_log = 0\n",
    "\n",
    "    # 1) 環境を初期化して最初の観測を得る\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    # 2) 環境ステップを total_step 回まわす\n",
    "    for t in range(total_step):\n",
    "\n",
    "        # logging.info(\"train step %d start\", t)\n",
    "\n",
    "        # ---- (A) 行動選択：warmup まではランダム、その後は方策+ノイズが定石 ----\n",
    "        if len(buffer) < warmup_steps:\n",
    "            # 環境の action_space に従ってランダム行動（探索の立ち上がりを安定化）\n",
    "            action = env.action_space.sample()\n",
    "            # logging.info(\"warmup now\")\n",
    "        else:\n",
    "            # DDPG の探索：方策にノイズを加えた行動\n",
    "            action = agent.step_with_noise(obs)\n",
    "            # logging.info(\"training\")\n",
    "\n",
    "        # 3) 環境を1ステップ進める\n",
    "        obs_next, reward, terminated, truncated, info = env.step(action)\n",
    "        reward_log += reward\n",
    "\n",
    "        # 4) “エピソード終了”判定（reset のため）\n",
    "        done = float(terminated)\n",
    "\n",
    "        # ---- (B) バッファに格納：学習ターゲット用の done は方針に注意 ----\n",
    "        # 方針1（簡単）：done をそのまま入れる（truncatedでもブートストラップ停止）\n",
    "        buffer.add(obs, action, reward, obs_next, done)\n",
    "\n",
    "        # 方針2（理屈に忠実）：terminated を入れる（truncatedはブートストラップ継続）\n",
    "        # buffer.add(obs, action, reward, obs_next, terminated)\n",
    "\n",
    "        # 5) 次の観測へ更新（done なら reset）\n",
    "        # doneはterminatedにしたので、ここではterminatedとtruncatedのorを使う\n",
    "        if terminated or truncated:\n",
    "            logging.info('train episode %d: reward = %.2f',\n",
    "                         episode_num, reward_log)\n",
    "            episode_num += 1\n",
    "            obs, info = env.reset()\n",
    "            reward_history.append(reward_log)\n",
    "            reward_log = 0\n",
    "        else:\n",
    "            obs = obs_next\n",
    "\n",
    "        # 6) バッファが十分でなければ学習をスキップ\n",
    "        if len(buffer) < warmup_steps:\n",
    "            continue\n",
    "\n",
    "        # 7) ミニバッチを取り出して更新\n",
    "        minibatch = buffer.sample(batch_num)\n",
    "        states      = minibatch[\"obs\"]\n",
    "        actions     = minibatch[\"act\"]\n",
    "        rewards     = minibatch[\"rew\"]\n",
    "        states_next = minibatch[\"obs_next\"]\n",
    "        dones       = minibatch[\"done\"]\n",
    "\n",
    "        Q_loss, P_loss = agent.update_net(states, actions, rewards, states_next, dones)\n",
    "\n",
    "        # 8) ログが欲しいならここで（毎step item() は遅くなるので間引くのが定石）\n",
    "        if t % 100 == 0:\n",
    "            Q_loss_history.append(Q_loss)\n",
    "            P_loss_history.append(P_loss)\n",
    "\n",
    "    return Q_loss_history, P_loss_history, reward_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3246743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDPGAgent(Config=Config(),device=device)\n",
    "DDPGReplayBuffer = ReplayBuffer(obs_dim=3,act_dim=1,size=2000,device=device)\n",
    "total_step = 60000\n",
    "\n",
    "Qh, Ph, rh = train(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    buffer=DDPGReplayBuffer,\n",
    "    total_step=total_step,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ba7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05010af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論用に eval モードにしておく（保存自体は train のままでも可）\n",
    "agent.mode2eval()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "agent.save_all(\n",
    "    \"./models/ddpg_final_\" + stamp + \".pth\",\n",
    "    extra={\n",
    "        \"total_step\": total_step,\n",
    "        \"reward_history\": rh,  # 要らなければ外してOK\n",
    "    }\n",
    ")\n",
    "print(\"saved to ddpg_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6a794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from dataclasses import asdict, is_dataclass\n",
    "\n",
    "# def save_ddpg_checkpoint(agent, path: str, config=None, extra: dict | None = None):\n",
    "#     \"\"\"\n",
    "#     agent を改造せずに、DDPG の全ネット（actor/critic + target）を保存する。\n",
    "#     - config: 保存したければ Config を渡す（dataclassでもOK）\n",
    "#     - extra : step数や報酬履歴など任意情報\n",
    "#     \"\"\"\n",
    "#     ckpt = {\n",
    "#         \"P_net\": agent.P_net.state_dict(),\n",
    "#         \"Q_net\": agent.Q_net.state_dict(),\n",
    "#         \"P_target_net\": agent.P_target_net.state_dict(),\n",
    "#         \"Q_target_net\": agent.Q_target_net.state_dict(),\n",
    "#     }\n",
    "\n",
    "#     # optimizer も一緒に保存したい場合（最終保存なら無くてもOK）\n",
    "#     if hasattr(agent, \"P_optim\"):\n",
    "#         ckpt[\"P_optim\"] = agent.P_optim.state_dict()\n",
    "#     if hasattr(agent, \"Q_optim\"):\n",
    "#         ckpt[\"Q_optim\"] = agent.Q_optim.state_dict()\n",
    "\n",
    "#     # Config を保存したい場合\n",
    "#     if config is not None:\n",
    "#         ckpt[\"config\"] = asdict(config) if is_dataclass(config) else config\n",
    "\n",
    "#     if extra is not None:\n",
    "#         ckpt[\"extra\"] = extra\n",
    "\n",
    "#     torch.save(ckpt, path)\n",
    "#     print(f\"saved checkpoint: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133507b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_ddpg_checkpoint(\n",
    "#     agent,\n",
    "#     \"./models/ddpg_final.pth\",\n",
    "#     #config=agent.Config,  # あるなら\n",
    "#     extra={\"reward_history\": rh, \"total_step\": 40000},\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
