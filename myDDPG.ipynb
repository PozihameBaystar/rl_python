{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf0a429",
   "metadata": {},
   "source": [
    "# 自作のDDPGノートブック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccfb1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from dataclasses import dataclass, asdict, is_dataclass, field\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from myActivator import tanhAndScale\n",
    "from myReplayBuffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a32d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "                    stream=sys.stdout, datefmt=\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6695230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:01:55 [INFO] id: Pendulum-v1\n",
      "23:01:55 [INFO] entry_point: gymnasium.envs.classic_control.pendulum:PendulumEnv\n",
      "23:01:55 [INFO] reward_threshold: None\n",
      "23:01:55 [INFO] nondeterministic: False\n",
      "23:01:55 [INFO] max_episode_steps: 200\n",
      "23:01:55 [INFO] order_enforce: True\n",
      "23:01:55 [INFO] disable_env_checker: False\n",
      "23:01:55 [INFO] kwargs: {'render_mode': 'human'}\n",
      "23:01:55 [INFO] additional_wrappers: ()\n",
      "23:01:55 [INFO] vector_entry_point: None\n",
      "23:01:55 [INFO] namespace: None\n",
      "23:01:55 [INFO] name: Pendulum\n",
      "23:01:55 [INFO] version: 1\n",
      "23:01:55 [INFO] max_speed: 8\n",
      "23:01:55 [INFO] max_torque: 2.0\n",
      "23:01:55 [INFO] dt: 0.05\n",
      "23:01:55 [INFO] g: 10.0\n",
      "23:01:55 [INFO] m: 1.0\n",
      "23:01:55 [INFO] l: 1.0\n",
      "23:01:55 [INFO] render_mode: human\n",
      "23:01:55 [INFO] screen_dim: 500\n",
      "23:01:55 [INFO] screen: None\n",
      "23:01:55 [INFO] clock: None\n",
      "23:01:55 [INFO] isopen: True\n",
      "23:01:55 [INFO] action_space: Box(-2.0, 2.0, (1,), float32)\n",
      "23:01:55 [INFO] observation_space: Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n",
      "23:01:55 [INFO] spec: EnvSpec(id='Pendulum-v1', entry_point='gymnasium.envs.classic_control.pendulum:PendulumEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, disable_env_checker=True, kwargs={'render_mode': 'human'}, namespace=None, name='Pendulum', version=1, additional_wrappers=(), vector_entry_point=None)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\",render_mode=\"human\")\n",
    "for key in vars(env.spec):\n",
    "    logging.info('%s: %s', key, vars(env.spec)[key])\n",
    "for key in vars(env.unwrapped):\n",
    "    logging.info('%s: %s', key, vars(env.unwrapped)[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58f4152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # =========================\n",
    "    # ニューラルネットの設定\n",
    "    # =========================\n",
    "    Q_net_sizes: list[int] = field(default_factory=lambda: [6, 12, 6])\n",
    "    P_net_sizes: list[int] = field(default_factory=lambda: [6, 12, 6])\n",
    "    Q_net_in: int = 4\n",
    "    P_net_in: int = 3\n",
    "    Q_net_out: int = 1\n",
    "    P_net_out: int = 1\n",
    "\n",
    "    # =========================\n",
    "    # 環境の制約\n",
    "    # =========================\n",
    "    u_ulim: float = 2.0\n",
    "    u_llim: float = -2.0\n",
    "\n",
    "    # =========================\n",
    "    # 学習に関するパラメータ\n",
    "    # =========================\n",
    "    Q_lr: float = 1e-2\n",
    "    P_lr: float = 1e-2\n",
    "    gamma: float = 0.95  # 割引率\n",
    "    sig: float = 1.0     # 探索ノイズの標準偏差\n",
    "    tau: float = 5e-3    # ターゲットネットの更新幅（Polyak係数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb55cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self,Config,device=None):\n",
    "        if Config:\n",
    "            self.Config = Config\n",
    "        else:\n",
    "            raise ValueError(\"No Config!!\")\n",
    "        \n",
    "        # ---- device 決定（指定がなければ CUDA があれば CUDA）----\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "\n",
    "        # ---- action bounds（device を揃えるため Tensor 化）----\n",
    "        self.u_low = torch.as_tensor(Config.u_llim, dtype=torch.float32, device=self.device)\n",
    "        self.u_high = torch.as_tensor(Config.u_ulim, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        self.Q_net = self.build_net(\n",
    "            Config.Q_net_in,\n",
    "            Config.Q_net_sizes,\n",
    "            Config.Q_net_out,\n",
    "            ).to(self.device)\n",
    "        self.Q_net.train()\n",
    "\n",
    "        self.P_net = self.build_net(\n",
    "            Config.P_net_in,\n",
    "            Config.P_net_sizes,\n",
    "            Config.P_net_out,\n",
    "            tanhAndScale(a_high=self.u_high,a_low=self.u_low),\n",
    "            ).to(self.device)\n",
    "        self.P_net.train()\n",
    "\n",
    "        # ---- Target nets（重要：deepcopy で別物を作る）----\n",
    "        self.Q_target_net = copy.deepcopy(self.Q_net).to(self.device)\n",
    "        self.P_target_net = copy.deepcopy(self.P_net).to(self.device)\n",
    "        self.Q_target_net.eval()\n",
    "        self.P_target_net.eval()\n",
    "\n",
    "        self.Q_optim = optim.Adam(self.Q_net.parameters(),lr=Config.Q_lr)\n",
    "        self.P_optim = optim.Adam(self.P_net.parameters(),lr=Config.P_lr)\n",
    "\n",
    "    def to(self, device):\n",
    "        \"\"\"エージェント内部のネットと必要Tensorを指定 device に移す。\"\"\"\n",
    "        self.device = torch.device(device)\n",
    "        self.Q_net.to(self.device)\n",
    "        self.P_net.to(self.device)\n",
    "        self.Q_target_net.to(self.device)\n",
    "        self.P_target_net.to(self.device)\n",
    "        self.u_low = self.u_low.to(self.device)\n",
    "        self.u_high = self.u_high.to(self.device)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def build_net(self, input_size, hidden_sizes, output_size=1, output_activator=None):\n",
    "        layers = []\n",
    "        for input_size, output_size in zip([input_size]+hidden_sizes, hidden_sizes+[output_size]):\n",
    "            layers.append(nn.Linear(input_size, output_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers = layers[:-1]  # 最後のReLUだけ取り除く\n",
    "        if output_activator:\n",
    "            layers.append(output_activator)\n",
    "        net = nn.Sequential(*layers)\n",
    "        return net\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, observation) -> np.ndarray:\n",
    "        \"\"\"ノイズなし（評価用）。環境に渡す行動を返す。\"\"\"\n",
    "        obs_t = torch.as_tensor(observation, dtype=torch.float32, device=self.device)\n",
    "        if obs_t.dim() == 1:\n",
    "            obs_t = obs_t.unsqueeze(0)\n",
    "\n",
    "        action = self.P_net(obs_t)\n",
    "        action = torch.clamp(action, self.u_low, self.u_high)\n",
    "        return action.squeeze(0).cpu().numpy()\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step_with_noise(self, observation):\n",
    "        # 1) observation を Tensor にし、ネットと同じ device に載せる\n",
    "        obs = torch.as_tensor(observation, dtype=torch.float32, device=self.device)\n",
    "        if obs.dim() == 1:\n",
    "            obs = obs.unsqueeze(0)  # (obs_dim,) -> (1, obs_dim)\n",
    "\n",
    "        # 2) 決定論的行動 a = μθ(s)\n",
    "        action = self.P_net(obs)  # shape: (1, act_dim) を想定\n",
    "\n",
    "        # 3) ε ~ N(0, σ^2 I) を生成して加算（探索）\n",
    "        eps = float(self.Config.sig) * torch.randn_like(action)\n",
    "        action = action + eps\n",
    "\n",
    "        # 4) 出力制約 [u_low, u_high] に収める（安全弁）\n",
    "        action = torch.clamp(action, self.u_low, self.u_high)\n",
    "\n",
    "        # 5) 環境に渡すならバッチ次元を落として返す（numpy が必要なら .cpu().numpy()）\n",
    "        return action.squeeze(0).cpu().numpy()\n",
    "    \n",
    "\n",
    "    def save_all(self, path: str, extra: dict | None = None):\n",
    "        \"\"\"\n",
    "        Actor/Critic + target nets をまとめて保存（最終モデル用）。\n",
    "        \"\"\"\n",
    "        cfg = asdict(self.Config) if is_dataclass(self.Config) else self.Config\n",
    "\n",
    "        ckpt = {\n",
    "            \"config\": cfg,\n",
    "            \"P_net\": self.P_net.state_dict(),\n",
    "            \"Q_net\": self.Q_net.state_dict(),\n",
    "            \"P_target_net\": self.P_target_net.state_dict(),\n",
    "            \"Q_target_net\": self.Q_target_net.state_dict(),\n",
    "        }\n",
    "        if extra is not None:\n",
    "            ckpt[\"extra\"] = extra\n",
    "\n",
    "        torch.save(ckpt, path)\n",
    "\n",
    "\n",
    "    def load_all(self, path: str, map_location=None):\n",
    "        \"\"\"\n",
    "        save_all() で保存したチェックポイントをロード。\n",
    "\n",
    "        PyTorch 2.6 以降:\n",
    "        torch.load() のデフォルトが weights_only=True になったため、\n",
    "        config/extra を含むチェックポイントはそのままだと UnpicklingError になり得る。\n",
    "        その回避として「信頼できるチェックポイントに限り」 weights_only=False を明示する。\n",
    "\n",
    "        ※ map_location は \"cpu\" や device を指定可。\n",
    "        \"\"\"\n",
    "        # PyTorch 2.6+ では weights_only 引数がある\n",
    "        try:\n",
    "            ckpt = torch.load(path, map_location=map_location, weights_only=False)\n",
    "        except TypeError:\n",
    "            # 古い PyTorch（weights_only 引数が無い）向け\n",
    "            ckpt = torch.load(path, map_location=map_location)\n",
    "\n",
    "        self.P_net.load_state_dict(ckpt[\"P_net\"])\n",
    "        self.Q_net.load_state_dict(ckpt[\"Q_net\"])\n",
    "        self.P_target_net.load_state_dict(ckpt[\"P_target_net\"])\n",
    "        self.Q_target_net.load_state_dict(ckpt[\"Q_target_net\"])\n",
    "\n",
    "        return ckpt.get(\"extra\", None)\n",
    "    \n",
    "\n",
    "    def mode2eval(self):\n",
    "        self.P_net.eval()\n",
    "        self.Q_net.eval()\n",
    "\n",
    "\n",
    "    def mode2train(self):\n",
    "        self.P_net.train()\n",
    "        self.Q_net.train()\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def soft_update(self, target_net, online_net, tau):\n",
    "        \"\"\"\n",
    "        Polyak averaging:\n",
    "          θ' ← (1-τ) θ' + τ θ\n",
    "        \"\"\"\n",
    "        for target_param, online_param in zip(target_net.parameters(), online_net.parameters()):\n",
    "            target_param.mul_(1.0 - tau).add_(tau * online_param)\n",
    "\n",
    "    \n",
    "    def update_net(self,states,actions,rewards,states_next,dones=None):\n",
    "        \"\"\"\n",
    "        1回の更新（Critic→Actor→Target soft update）\n",
    "        戻り値： (q_loss, p_loss) のスカラー\n",
    "        \"\"\"\n",
    "        # ---- minibatch を device 上 Tensor に統一 ----\n",
    "        states = torch.as_tensor(states, dtype=torch.float32, device=self.device)\n",
    "        actions = torch.as_tensor(actions, dtype=torch.float32, device=self.device)\n",
    "        rewards = torch.as_tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        states_next = torch.as_tensor(states_next, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        if rewards.dim() == 1:\n",
    "            rewards = rewards.unsqueeze(1)\n",
    "\n",
    "        if dones is None:\n",
    "            dones = torch.zeros((states.shape[0], 1), dtype=torch.float32, device=self.device)\n",
    "        else:\n",
    "            dones = torch.as_tensor(dones, dtype=torch.float32, device=self.device)\n",
    "            if dones.dim() == 1:\n",
    "                dones = dones.unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            actions_next_for_target = self.P_target_net(states_next)\n",
    "            y_targets = rewards + self.Config.gamma*(1-dones)*self.Q_target_net(torch.cat([states_next, actions_next_for_target], dim=1))\n",
    "        \n",
    "        Q_values = self.Q_net(torch.cat([states,actions],dim=1))\n",
    "        Q_loss = F.mse_loss(y_targets,Q_values)\n",
    "        self.Q_optim.zero_grad()\n",
    "        Q_loss.backward()\n",
    "        self.Q_optim.step()\n",
    "\n",
    "        # ---- Actor update ----\n",
    "        # Actor 更新では Q_net を通すが、Q_net 自体は更新しないので凍結（計算の節約＋安全）\n",
    "        for p in self.Q_net.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        \n",
    "        actions_for_Ploss = self.P_net(states)\n",
    "        P_loss = -self.Q_net(torch.cat([states,actions_for_Ploss],dim=1)).mean()\n",
    "        self.P_optim.zero_grad()\n",
    "        P_loss.backward()\n",
    "        self.P_optim.step()\n",
    "\n",
    "        for p in self.Q_net.parameters():\n",
    "            p.requires_grad_(True)\n",
    "\n",
    "        self.soft_update(\n",
    "            target_net=self.Q_target_net,\n",
    "            online_net=self.Q_net,\n",
    "            tau=self.Config.tau\n",
    "            )\n",
    "        self.soft_update(\n",
    "            target_net=self.P_target_net,\n",
    "            online_net=self.P_net,\n",
    "            tau=self.Config.tau\n",
    "            )\n",
    "        \n",
    "        return float(Q_loss.item()), float(P_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8e097cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    env,\n",
    "    agent,\n",
    "    buffer,\n",
    "    total_step=40000,\n",
    "    warmup_steps=1000,\n",
    "    batch_num=512,\n",
    "):\n",
    "    print(\"cuda available:\", torch.cuda.is_available())\n",
    "    print(\"agent device:\", agent.device)\n",
    "    print(\"P_net device:\", next(agent.P_net.parameters()).device)\n",
    "    print(\"Q_net device:\", next(agent.Q_net.parameters()).device)\n",
    "    \n",
    "    # ログ保存用のリスト\n",
    "    Q_loss_history = []\n",
    "    P_loss_history = []\n",
    "    episode_num = 1\n",
    "    reward_history = []\n",
    "    reward_log = 0\n",
    "\n",
    "    # 1) 環境を初期化して最初の観測を得る\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    # 2) 環境ステップを total_step 回まわす\n",
    "    for t in range(total_step):\n",
    "\n",
    "        # logging.info(\"train step %d start\", t)\n",
    "\n",
    "        # ---- (A) 行動選択：warmup まではランダム、その後は方策+ノイズが定石 ----\n",
    "        if len(buffer) < warmup_steps:\n",
    "            # 環境の action_space に従ってランダム行動（探索の立ち上がりを安定化）\n",
    "            action = env.action_space.sample()\n",
    "            # logging.info(\"warmup now\")\n",
    "        else:\n",
    "            # DDPG の探索：方策にノイズを加えた行動\n",
    "            action = agent.step_with_noise(obs)\n",
    "            # logging.info(\"training\")\n",
    "\n",
    "        # 3) 環境を1ステップ進める\n",
    "        obs_next, reward, terminated, truncated, info = env.step(action)\n",
    "        reward_log += reward\n",
    "\n",
    "        # 4) “エピソード終了”判定（reset のため）\n",
    "        done = float(terminated)\n",
    "\n",
    "        # ---- (B) バッファに格納：学習ターゲット用の done は方針に注意 ----\n",
    "        # 方針1（簡単）：done をそのまま入れる（truncatedでもブートストラップ停止）\n",
    "        buffer.add(obs, action, reward, obs_next, done)\n",
    "\n",
    "        # 方針2（理屈に忠実）：terminated を入れる（truncatedはブートストラップ継続）\n",
    "        # buffer.add(obs, action, reward, obs_next, terminated)\n",
    "\n",
    "        # 5) 次の観測へ更新（done なら reset）\n",
    "        # doneはterminatedにしたので、ここではterminatedとtruncatedのorを使う\n",
    "        if terminated or truncated:\n",
    "            logging.info('train episode %d: reward = %.2f',\n",
    "                         episode_num, reward_log)\n",
    "            episode_num += 1\n",
    "            obs, info = env.reset()\n",
    "            reward_history.append(reward_log)\n",
    "            reward_log = 0\n",
    "        else:\n",
    "            obs = obs_next\n",
    "\n",
    "        # 6) バッファが十分でなければ学習をスキップ\n",
    "        if len(buffer) < warmup_steps:\n",
    "            continue\n",
    "\n",
    "        # 7) ミニバッチを取り出して更新\n",
    "        minibatch = buffer.sample(batch_num)\n",
    "        states      = minibatch[\"obs\"]\n",
    "        actions     = minibatch[\"act\"]\n",
    "        rewards     = minibatch[\"rew\"]\n",
    "        states_next = minibatch[\"obs_next\"]\n",
    "        dones       = minibatch[\"done\"]\n",
    "\n",
    "        Q_loss, P_loss = agent.update_net(states, actions, rewards, states_next, dones)\n",
    "\n",
    "        # 8) ログが欲しいならここで（毎step item() は遅くなるので間引くのが定石）\n",
    "        if t % 100 == 0:\n",
    "            Q_loss_history.append(Q_loss)\n",
    "            P_loss_history.append(P_loss)\n",
    "\n",
    "    return Q_loss_history, P_loss_history, reward_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3246743b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "agent device: cuda\n",
      "P_net device: cuda:0\n",
      "Q_net device: cuda:0\n",
      "23:02:05 [INFO] train episode 1: reward = -1616.18\n",
      "23:02:12 [INFO] train episode 2: reward = -1047.24\n",
      "23:02:19 [INFO] train episode 3: reward = -1275.22\n",
      "23:02:25 [INFO] train episode 4: reward = -1464.75\n",
      "23:02:32 [INFO] train episode 5: reward = -983.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_106916/1172717755.py:209: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at /pytorch/aten/src/ATen/native/Scalar.cpp:22.)\n",
      "  return float(Q_loss.item()), float(P_loss.item())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:02:39 [INFO] train episode 6: reward = -1511.19\n",
      "23:02:46 [INFO] train episode 7: reward = -1623.64\n",
      "23:02:52 [INFO] train episode 8: reward = -1575.44\n",
      "23:02:59 [INFO] train episode 9: reward = -1619.12\n",
      "23:03:05 [INFO] train episode 10: reward = -1515.45\n",
      "23:03:12 [INFO] train episode 11: reward = -1349.83\n",
      "23:03:19 [INFO] train episode 12: reward = -1575.49\n",
      "23:03:25 [INFO] train episode 13: reward = -1473.63\n",
      "23:03:32 [INFO] train episode 14: reward = -1588.83\n",
      "23:03:39 [INFO] train episode 15: reward = -1434.45\n",
      "23:03:45 [INFO] train episode 16: reward = -3.96\n",
      "23:03:52 [INFO] train episode 17: reward = -4.16\n",
      "23:03:59 [INFO] train episode 18: reward = -971.54\n",
      "23:04:05 [INFO] train episode 19: reward = -972.80\n",
      "23:04:12 [INFO] train episode 20: reward = -992.14\n",
      "23:04:19 [INFO] train episode 21: reward = -929.44\n",
      "23:04:25 [INFO] train episode 22: reward = -1030.52\n",
      "23:04:32 [INFO] train episode 23: reward = -1.53\n",
      "23:04:39 [INFO] train episode 24: reward = -2.53\n",
      "23:04:45 [INFO] train episode 25: reward = -128.42\n",
      "23:04:52 [INFO] train episode 26: reward = -507.03\n",
      "23:04:59 [INFO] train episode 27: reward = -496.38\n",
      "23:05:05 [INFO] train episode 28: reward = -387.33\n",
      "23:05:12 [INFO] train episode 29: reward = -889.86\n",
      "23:05:19 [INFO] train episode 30: reward = -950.44\n",
      "23:05:25 [INFO] train episode 31: reward = -786.94\n",
      "23:05:32 [INFO] train episode 32: reward = -134.56\n",
      "23:05:39 [INFO] train episode 33: reward = -400.72\n",
      "23:05:45 [INFO] train episode 34: reward = -133.73\n",
      "23:05:52 [INFO] train episode 35: reward = -530.55\n",
      "23:05:59 [INFO] train episode 36: reward = -240.55\n",
      "23:06:05 [INFO] train episode 37: reward = -789.61\n",
      "23:06:12 [INFO] train episode 38: reward = -947.07\n",
      "23:06:19 [INFO] train episode 39: reward = -929.85\n",
      "23:06:25 [INFO] train episode 40: reward = -1213.06\n",
      "23:06:32 [INFO] train episode 41: reward = -511.32\n",
      "23:06:39 [INFO] train episode 42: reward = -642.39\n",
      "23:06:45 [INFO] train episode 43: reward = -647.11\n",
      "23:06:52 [INFO] train episode 44: reward = -3.25\n",
      "23:06:59 [INFO] train episode 45: reward = -517.22\n",
      "23:07:05 [INFO] train episode 46: reward = -133.62\n",
      "23:07:12 [INFO] train episode 47: reward = -257.93\n",
      "23:07:18 [INFO] train episode 48: reward = -119.00\n",
      "23:07:25 [INFO] train episode 49: reward = -267.35\n",
      "23:07:32 [INFO] train episode 50: reward = -131.81\n",
      "23:07:38 [INFO] train episode 51: reward = -1.65\n",
      "23:07:45 [INFO] train episode 52: reward = -128.50\n",
      "23:07:52 [INFO] train episode 53: reward = -130.36\n",
      "23:07:58 [INFO] train episode 54: reward = -132.01\n",
      "23:08:05 [INFO] train episode 55: reward = -2.62\n",
      "23:08:12 [INFO] train episode 56: reward = -130.55\n",
      "23:08:18 [INFO] train episode 57: reward = -128.97\n",
      "23:08:25 [INFO] train episode 58: reward = -131.38\n",
      "23:08:32 [INFO] train episode 59: reward = -548.36\n",
      "23:08:38 [INFO] train episode 60: reward = -553.45\n",
      "23:08:45 [INFO] train episode 61: reward = -130.54\n",
      "23:08:52 [INFO] train episode 62: reward = -370.33\n",
      "23:08:58 [INFO] train episode 63: reward = -365.19\n",
      "23:09:05 [INFO] train episode 64: reward = -374.58\n",
      "23:09:12 [INFO] train episode 65: reward = -252.25\n",
      "23:09:18 [INFO] train episode 66: reward = -130.51\n",
      "23:09:25 [INFO] train episode 67: reward = -1.20\n",
      "23:09:32 [INFO] train episode 68: reward = -446.88\n",
      "23:09:38 [INFO] train episode 69: reward = -131.84\n",
      "23:09:45 [INFO] train episode 70: reward = -1.76\n",
      "23:09:52 [INFO] train episode 71: reward = -623.56\n",
      "23:09:58 [INFO] train episode 72: reward = -634.26\n",
      "23:10:05 [INFO] train episode 73: reward = -375.95\n",
      "23:10:12 [INFO] train episode 74: reward = -135.52\n",
      "23:10:18 [INFO] train episode 75: reward = -393.38\n",
      "23:10:25 [INFO] train episode 76: reward = -617.61\n",
      "23:10:32 [INFO] train episode 77: reward = -342.73\n",
      "23:10:38 [INFO] train episode 78: reward = -377.26\n",
      "23:10:45 [INFO] train episode 79: reward = -784.06\n",
      "23:10:52 [INFO] train episode 80: reward = -388.09\n",
      "23:10:58 [INFO] train episode 81: reward = -129.40\n",
      "23:11:05 [INFO] train episode 82: reward = -1.90\n",
      "23:11:12 [INFO] train episode 83: reward = -2.16\n",
      "23:11:18 [INFO] train episode 84: reward = -533.62\n",
      "23:11:25 [INFO] train episode 85: reward = -534.30\n",
      "23:11:31 [INFO] train episode 86: reward = -132.42\n",
      "23:11:38 [INFO] train episode 87: reward = -396.35\n",
      "23:11:45 [INFO] train episode 88: reward = -784.89\n",
      "23:11:51 [INFO] train episode 89: reward = -793.28\n",
      "23:11:58 [INFO] train episode 90: reward = -261.11\n",
      "23:12:05 [INFO] train episode 91: reward = -132.41\n",
      "23:12:11 [INFO] train episode 92: reward = -953.12\n",
      "23:12:18 [INFO] train episode 93: reward = -379.04\n",
      "23:12:25 [INFO] train episode 94: reward = -262.04\n",
      "23:12:31 [INFO] train episode 95: reward = -383.38\n",
      "23:12:38 [INFO] train episode 96: reward = -259.09\n",
      "23:12:45 [INFO] train episode 97: reward = -262.84\n",
      "23:12:51 [INFO] train episode 98: reward = -375.00\n",
      "23:12:58 [INFO] train episode 99: reward = -132.42\n",
      "23:13:05 [INFO] train episode 100: reward = -378.58\n",
      "23:13:11 [INFO] train episode 101: reward = -373.64\n",
      "23:13:18 [INFO] train episode 102: reward = -251.45\n",
      "23:13:25 [INFO] train episode 103: reward = -531.85\n",
      "23:13:31 [INFO] train episode 104: reward = -519.17\n",
      "23:13:38 [INFO] train episode 105: reward = -137.64\n",
      "23:13:45 [INFO] train episode 106: reward = -265.17\n",
      "23:13:51 [INFO] train episode 107: reward = -261.88\n",
      "23:13:58 [INFO] train episode 108: reward = -379.65\n",
      "23:14:05 [INFO] train episode 109: reward = -264.14\n",
      "23:14:11 [INFO] train episode 110: reward = -533.75\n",
      "23:14:18 [INFO] train episode 111: reward = -13.35\n",
      "23:14:25 [INFO] train episode 112: reward = -768.05\n",
      "23:14:31 [INFO] train episode 113: reward = -265.72\n",
      "23:14:38 [INFO] train episode 114: reward = -394.36\n",
      "23:14:45 [INFO] train episode 115: reward = -386.43\n",
      "23:14:51 [INFO] train episode 116: reward = -634.45\n",
      "23:14:58 [INFO] train episode 117: reward = -124.93\n",
      "23:15:04 [INFO] train episode 118: reward = -132.47\n",
      "23:15:11 [INFO] train episode 119: reward = -353.28\n",
      "23:15:18 [INFO] train episode 120: reward = -263.40\n",
      "23:15:25 [INFO] train episode 121: reward = -394.83\n",
      "23:15:31 [INFO] train episode 122: reward = -280.23\n",
      "23:15:38 [INFO] train episode 123: reward = -497.69\n",
      "23:15:44 [INFO] train episode 124: reward = -240.83\n",
      "23:15:51 [INFO] train episode 125: reward = -122.54\n",
      "23:15:58 [INFO] train episode 126: reward = -498.66\n",
      "23:16:04 [INFO] train episode 127: reward = -242.92\n",
      "23:16:11 [INFO] train episode 128: reward = -244.60\n",
      "23:16:18 [INFO] train episode 129: reward = -127.03\n",
      "23:16:24 [INFO] train episode 130: reward = -127.03\n",
      "23:16:31 [INFO] train episode 131: reward = -130.13\n",
      "23:16:38 [INFO] train episode 132: reward = -6.11\n",
      "23:16:44 [INFO] train episode 133: reward = -252.84\n",
      "23:16:51 [INFO] train episode 134: reward = -132.64\n",
      "23:16:58 [INFO] train episode 135: reward = -395.27\n",
      "23:17:04 [INFO] train episode 136: reward = -531.33\n",
      "23:17:11 [INFO] train episode 137: reward = -2.19\n",
      "23:17:18 [INFO] train episode 138: reward = -128.15\n",
      "23:17:24 [INFO] train episode 139: reward = -128.11\n",
      "23:17:31 [INFO] train episode 140: reward = -131.98\n",
      "23:17:38 [INFO] train episode 141: reward = -129.08\n",
      "23:17:44 [INFO] train episode 142: reward = -243.10\n",
      "23:17:51 [INFO] train episode 143: reward = -1.96\n",
      "23:17:58 [INFO] train episode 144: reward = -129.86\n",
      "23:18:04 [INFO] train episode 145: reward = -130.92\n",
      "23:18:11 [INFO] train episode 146: reward = -385.69\n",
      "23:18:18 [INFO] train episode 147: reward = -4.50\n",
      "23:18:24 [INFO] train episode 148: reward = -128.73\n",
      "23:18:31 [INFO] train episode 149: reward = -356.80\n",
      "23:18:38 [INFO] train episode 150: reward = -249.96\n",
      "23:18:44 [INFO] train episode 151: reward = -130.96\n",
      "23:18:51 [INFO] train episode 152: reward = -129.85\n",
      "23:18:58 [INFO] train episode 153: reward = -131.80\n",
      "23:19:04 [INFO] train episode 154: reward = -245.69\n",
      "23:19:11 [INFO] train episode 155: reward = -132.64\n",
      "23:19:18 [INFO] train episode 156: reward = -248.21\n",
      "23:19:24 [INFO] train episode 157: reward = -260.33\n",
      "23:19:31 [INFO] train episode 158: reward = -239.84\n",
      "23:19:37 [INFO] train episode 159: reward = -656.17\n",
      "23:19:44 [INFO] train episode 160: reward = -436.95\n",
      "23:19:51 [INFO] train episode 161: reward = -375.56\n",
      "23:19:57 [INFO] train episode 162: reward = -138.41\n",
      "23:20:04 [INFO] train episode 163: reward = -513.44\n",
      "23:20:11 [INFO] train episode 164: reward = -625.11\n",
      "23:20:17 [INFO] train episode 165: reward = -503.88\n",
      "23:20:24 [INFO] train episode 166: reward = -143.68\n",
      "23:20:31 [INFO] train episode 167: reward = -9.08\n",
      "23:20:37 [INFO] train episode 168: reward = -644.09\n",
      "23:20:44 [INFO] train episode 169: reward = -264.22\n",
      "23:20:51 [INFO] train episode 170: reward = -130.69\n",
      "23:20:57 [INFO] train episode 171: reward = -135.77\n",
      "23:21:04 [INFO] train episode 172: reward = -1209.21\n",
      "23:21:11 [INFO] train episode 173: reward = -369.48\n",
      "23:21:17 [INFO] train episode 174: reward = -122.21\n",
      "23:21:24 [INFO] train episode 175: reward = -366.34\n",
      "23:21:31 [INFO] train episode 176: reward = -130.12\n",
      "23:21:37 [INFO] train episode 177: reward = -7.95\n",
      "23:21:44 [INFO] train episode 178: reward = -359.61\n",
      "23:21:51 [INFO] train episode 179: reward = -126.15\n",
      "23:21:57 [INFO] train episode 180: reward = -508.54\n",
      "23:22:04 [INFO] train episode 181: reward = -245.36\n",
      "23:22:11 [INFO] train episode 182: reward = -240.54\n",
      "23:22:17 [INFO] train episode 183: reward = -1054.60\n",
      "23:22:24 [INFO] train episode 184: reward = -120.00\n",
      "23:22:31 [INFO] train episode 185: reward = -121.72\n",
      "23:22:37 [INFO] train episode 186: reward = -349.65\n",
      "23:22:44 [INFO] train episode 187: reward = -128.51\n",
      "23:22:50 [INFO] train episode 188: reward = -257.51\n",
      "23:22:57 [INFO] train episode 189: reward = -387.16\n",
      "23:23:04 [INFO] train episode 190: reward = -653.48\n",
      "23:23:10 [INFO] train episode 191: reward = -758.76\n",
      "23:23:17 [INFO] train episode 192: reward = -257.87\n",
      "23:23:24 [INFO] train episode 193: reward = -1011.66\n",
      "23:23:30 [INFO] train episode 194: reward = -388.40\n",
      "23:23:37 [INFO] train episode 195: reward = -512.73\n",
      "23:23:44 [INFO] train episode 196: reward = -495.01\n",
      "23:23:50 [INFO] train episode 197: reward = -130.31\n",
      "23:23:57 [INFO] train episode 198: reward = -127.82\n",
      "23:24:04 [INFO] train episode 199: reward = -374.27\n",
      "23:24:10 [INFO] train episode 200: reward = -128.53\n"
     ]
    }
   ],
   "source": [
    "agent = DDPGAgent(Config=Config(),device=device)\n",
    "DDPGReplayBuffer = ReplayBuffer(obs_dim=3,act_dim=1,size=2000,device=device)\n",
    "total_step = 40000\n",
    "\n",
    "Qh, Ph, rh = train(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    buffer=DDPGReplayBuffer,\n",
    "    total_step=total_step,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a01ba7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05010af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to ddpg_final.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def make_unique_path(path: str | Path) -> Path:\n",
    "    \"\"\"\n",
    "    path が既に存在する場合、末尾に _1, _2, ... を付けて未使用のパスを返す。\n",
    "    例: ddpg_final_20251221_235959.pth -> ddpg_final_20251221_235959_1.pth -> ...\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "\n",
    "    # 存在しないならそのまま使う\n",
    "    if not p.exists():\n",
    "        return p\n",
    "\n",
    "    parent = p.parent\n",
    "    stem = p.stem      # 拡張子抜きファイル名\n",
    "    suffix = p.suffix  # \".pth\"\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        cand = parent / f\"{stem}_{i}{suffix}\"\n",
    "        if not cand.exists():\n",
    "            return cand\n",
    "        i += 1\n",
    "\n",
    "\n",
    "# 推論用に eval モードにしておく（保存自体は train のままでも可）\n",
    "agent.mode2eval()\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "models_dir = Path(\"./models\")\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "base_path = models_dir / f\"ddpg_final_{stamp}.pth\"\n",
    "save_path = make_unique_path(base_path)\n",
    "\n",
    "agent.save_all(\n",
    "    save_path.as_posix(),\n",
    "    extra={\n",
    "        \"total_step\": int(total_step),\n",
    "        \"reward_history\": rh,  # 必要ならそのままでOK\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
