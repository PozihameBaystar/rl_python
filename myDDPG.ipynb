{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf0a429",
   "metadata": {},
   "source": [
    "# 自作のDDPGノートブック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ccfb1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from dataclasses import dataclass, asdict, is_dataclass, field\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from myActivator import tanhAndScale\n",
    "from myReplayBuffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a32d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "                    stream=sys.stdout, datefmt=\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6695230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:22:10 [INFO] id: Pendulum-v1\n",
      "01:22:10 [INFO] entry_point: gymnasium.envs.classic_control.pendulum:PendulumEnv\n",
      "01:22:10 [INFO] reward_threshold: None\n",
      "01:22:10 [INFO] nondeterministic: False\n",
      "01:22:10 [INFO] max_episode_steps: 200\n",
      "01:22:10 [INFO] order_enforce: True\n",
      "01:22:10 [INFO] disable_env_checker: False\n",
      "01:22:10 [INFO] kwargs: {}\n",
      "01:22:11 [INFO] additional_wrappers: ()\n",
      "01:22:11 [INFO] vector_entry_point: None\n",
      "01:22:11 [INFO] namespace: None\n",
      "01:22:11 [INFO] name: Pendulum\n",
      "01:22:11 [INFO] version: 1\n",
      "01:22:11 [INFO] max_speed: 8\n",
      "01:22:11 [INFO] max_torque: 2.0\n",
      "01:22:11 [INFO] dt: 0.05\n",
      "01:22:11 [INFO] g: 10.0\n",
      "01:22:11 [INFO] m: 1.0\n",
      "01:22:11 [INFO] l: 1.0\n",
      "01:22:11 [INFO] render_mode: None\n",
      "01:22:11 [INFO] screen_dim: 500\n",
      "01:22:11 [INFO] screen: None\n",
      "01:22:11 [INFO] clock: None\n",
      "01:22:11 [INFO] isopen: True\n",
      "01:22:11 [INFO] action_space: Box(-2.0, 2.0, (1,), float32)\n",
      "01:22:11 [INFO] observation_space: Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n",
      "01:22:11 [INFO] spec: EnvSpec(id='Pendulum-v1', entry_point='gymnasium.envs.classic_control.pendulum:PendulumEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, disable_env_checker=True, kwargs={}, namespace=None, name='Pendulum', version=1, additional_wrappers=(), vector_entry_point=None)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "for key in vars(env.spec):\n",
    "    logging.info('%s: %s', key, vars(env.spec)[key])\n",
    "for key in vars(env.unwrapped):\n",
    "    logging.info('%s: %s', key, vars(env.unwrapped)[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58f4152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c46db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # =========================\n",
    "    # ニューラルネットの設定\n",
    "    # =========================\n",
    "    Q_net_sizes: list[int] = field(default_factory=lambda: [6, 12, 6])\n",
    "    P_net_sizes: list[int] = field(default_factory=lambda: [6, 12, 6])\n",
    "    Q_net_in: int = 4\n",
    "    P_net_in: int = 3\n",
    "    Q_net_out: int = 1\n",
    "    P_net_out: int = 1\n",
    "\n",
    "    # =========================\n",
    "    # 環境の制約\n",
    "    # =========================\n",
    "    u_ulim: float = 2.0\n",
    "    u_llim: float = -2.0\n",
    "\n",
    "    # =========================\n",
    "    # 学習に関するパラメータ\n",
    "    # =========================\n",
    "    Q_lr: float = 1e-2\n",
    "    P_lr: float = 1e-2\n",
    "    gamma: float = 0.95  # 割引率\n",
    "    sig: float = 1.0     # 探索ノイズの標準偏差\n",
    "    tau: float = 5e-3    # ターゲットネットの更新幅（Polyak係数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebb55cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self,Config,device=None):\n",
    "        if Config:\n",
    "            self.Config = Config\n",
    "        else:\n",
    "            raise ValueError(\"No Config!!\")\n",
    "        \n",
    "        # ---- device 決定（指定がなければ CUDA があれば CUDA）----\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "\n",
    "        # ---- action bounds（device を揃えるため Tensor 化）----\n",
    "        self.u_low = torch.as_tensor(Config.u_llim, dtype=torch.float32, device=self.device)\n",
    "        self.u_high = torch.as_tensor(Config.u_ulim, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        self.Q_net = self.build_net(\n",
    "            Config.Q_net_in,\n",
    "            Config.Q_net_sizes,\n",
    "            Config.Q_net_out,\n",
    "            ).to(self.device)\n",
    "        self.Q_net.train()\n",
    "\n",
    "        self.P_net = self.build_net(\n",
    "            Config.P_net_in,\n",
    "            Config.P_net_sizes,\n",
    "            Config.P_net_out,\n",
    "            tanhAndScale(a_high=self.u_high,a_low=self.u_low),\n",
    "            ).to(self.device)\n",
    "        self.P_net.train()\n",
    "\n",
    "        # ---- Target nets（重要：deepcopy で別物を作る）----\n",
    "        self.Q_target_net = copy.deepcopy(self.Q_net).to(self.device)\n",
    "        self.P_target_net = copy.deepcopy(self.P_net).to(self.device)\n",
    "        self.Q_target_net.eval()\n",
    "        self.P_target_net.eval()\n",
    "\n",
    "        self.Q_optim = optim.Adam(self.Q_net.parameters(),lr=Config.Q_lr)\n",
    "        self.P_optim = optim.Adam(self.P_net.parameters(),lr=Config.P_lr)\n",
    "\n",
    "    def to(self, device):\n",
    "        \"\"\"エージェント内部のネットと必要Tensorを指定 device に移す。\"\"\"\n",
    "        self.device = torch.device(device)\n",
    "        self.Q_net.to(self.device)\n",
    "        self.P_net.to(self.device)\n",
    "        self.Q_target_net.to(self.device)\n",
    "        self.P_target_net.to(self.device)\n",
    "        self.u_low = self.u_low.to(self.device)\n",
    "        self.u_high = self.u_high.to(self.device)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def build_net(self, input_size, hidden_sizes, output_size=1, output_activator=None):\n",
    "        layers = []\n",
    "        for input_size, output_size in zip([input_size]+hidden_sizes, hidden_sizes+[output_size]):\n",
    "            layers.append(nn.Linear(input_size, output_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers = layers[:-1]  # 最後のReLUだけ取り除く\n",
    "        if output_activator:\n",
    "            layers.append(output_activator)\n",
    "        net = nn.Sequential(*layers)\n",
    "        return net\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, observation) -> np.ndarray:\n",
    "        \"\"\"ノイズなし（評価用）。環境に渡す行動を返す。\"\"\"\n",
    "        obs_t = torch.as_tensor(observation, dtype=torch.float32, device=self.device)\n",
    "        if obs_t.dim() == 1:\n",
    "            obs_t = obs_t.unsqueeze(0)\n",
    "\n",
    "        action = self.P_net(obs_t)\n",
    "        action = torch.clamp(action, self.u_low, self.u_high)\n",
    "        return action.squeeze(0).cpu().numpy()\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step_with_noise(self, observation):\n",
    "        # 1) observation を Tensor にし、ネットと同じ device に載せる\n",
    "        obs = torch.as_tensor(observation, dtype=torch.float32, device=self.device)\n",
    "        if obs.dim() == 1:\n",
    "            obs = obs.unsqueeze(0)  # (obs_dim,) -> (1, obs_dim)\n",
    "\n",
    "        # 2) 決定論的行動 a = μθ(s)\n",
    "        action = self.P_net(obs)  # shape: (1, act_dim) を想定\n",
    "\n",
    "        # 3) ε ~ N(0, σ^2 I) を生成して加算（探索）\n",
    "        eps = float(self.Config.sig) * torch.randn_like(action)\n",
    "        action = action + eps\n",
    "\n",
    "        # 4) 出力制約 [u_low, u_high] に収める（安全弁）\n",
    "        action = torch.clamp(action, self.u_low, self.u_high)\n",
    "\n",
    "        # 5) 環境に渡すならバッチ次元を落として返す（numpy が必要なら .cpu().numpy()）\n",
    "        return action.squeeze(0).cpu().numpy()\n",
    "    \n",
    "\n",
    "    def save_all(self, path: str, extra: dict | None = None):\n",
    "        \"\"\"\n",
    "        Actor/Critic + target nets をまとめて保存（最終モデル用）。\n",
    "        \"\"\"\n",
    "        cfg = asdict(self.Config) if is_dataclass(self.Config) else self.Config\n",
    "\n",
    "        ckpt = {\n",
    "            \"config\": cfg,\n",
    "            \"P_net\": self.P_net.state_dict(),\n",
    "            \"Q_net\": self.Q_net.state_dict(),\n",
    "            \"P_target_net\": self.P_target_net.state_dict(),\n",
    "            \"Q_target_net\": self.Q_target_net.state_dict(),\n",
    "        }\n",
    "        if extra is not None:\n",
    "            ckpt[\"extra\"] = extra\n",
    "\n",
    "        torch.save(ckpt, path)\n",
    "\n",
    "\n",
    "    def load_all(self, path: str, map_location=None):\n",
    "        \"\"\"\n",
    "        save_all() で保存したチェックポイントをロード。\n",
    "\n",
    "        PyTorch 2.6 以降:\n",
    "        torch.load() のデフォルトが weights_only=True になったため、\n",
    "        config/extra を含むチェックポイントはそのままだと UnpicklingError になり得る。\n",
    "        その回避として「信頼できるチェックポイントに限り」 weights_only=False を明示する。\n",
    "\n",
    "        ※ map_location は \"cpu\" や device を指定可。\n",
    "        \"\"\"\n",
    "        # PyTorch 2.6+ では weights_only 引数がある\n",
    "        try:\n",
    "            ckpt = torch.load(path, map_location=map_location, weights_only=False)\n",
    "        except TypeError:\n",
    "            # 古い PyTorch（weights_only 引数が無い）向け\n",
    "            ckpt = torch.load(path, map_location=map_location)\n",
    "\n",
    "        self.P_net.load_state_dict(ckpt[\"P_net\"])\n",
    "        self.Q_net.load_state_dict(ckpt[\"Q_net\"])\n",
    "        self.P_target_net.load_state_dict(ckpt[\"P_target_net\"])\n",
    "        self.Q_target_net.load_state_dict(ckpt[\"Q_target_net\"])\n",
    "\n",
    "        return ckpt.get(\"extra\", None)\n",
    "    \n",
    "\n",
    "    def mode2eval(self):\n",
    "        self.P_net.eval()\n",
    "        self.Q_net.eval()\n",
    "\n",
    "\n",
    "    def mode2train(self):\n",
    "        self.P_net.train()\n",
    "        self.Q_net.train()\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def soft_update(self, target_net, online_net, tau):\n",
    "        \"\"\"\n",
    "        Polyak averaging:\n",
    "          θ' ← (1-τ) θ' + τ θ\n",
    "        \"\"\"\n",
    "        for target_param, online_param in zip(target_net.parameters(), online_net.parameters()):\n",
    "            target_param.mul_(1.0 - tau).add_(tau * online_param)\n",
    "\n",
    "    \n",
    "    def update_net(self,states,actions,rewards,states_next,dones=None):\n",
    "        \"\"\"\n",
    "        1回の更新（Critic→Actor→Target soft update）\n",
    "        戻り値： (q_loss, p_loss) のスカラー\n",
    "        \"\"\"\n",
    "        # ---- minibatch を device 上 Tensor に統一 ----\n",
    "        states = torch.as_tensor(states, dtype=torch.float32, device=self.device)\n",
    "        actions = torch.as_tensor(actions, dtype=torch.float32, device=self.device)\n",
    "        rewards = torch.as_tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        states_next = torch.as_tensor(states_next, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        if rewards.dim() == 1:\n",
    "            rewards = rewards.unsqueeze(1)\n",
    "\n",
    "        if dones is None:\n",
    "            dones = torch.zeros((states.shape[0], 1), dtype=torch.float32, device=self.device)\n",
    "        else:\n",
    "            dones = torch.as_tensor(dones, dtype=torch.float32, device=self.device)\n",
    "            if dones.dim() == 1:\n",
    "                dones = dones.unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            actions_next_for_target = self.P_target_net(states_next)\n",
    "            y_targets = rewards + self.Config.gamma*(1-dones)*self.Q_target_net(torch.cat([states_next, actions_next_for_target], dim=1))\n",
    "        \n",
    "        Q_values = self.Q_net(torch.cat([states,actions],dim=1))\n",
    "        Q_loss = F.mse_loss(y_targets,Q_values)\n",
    "        self.Q_optim.zero_grad()\n",
    "        Q_loss.backward()\n",
    "        self.Q_optim.step()\n",
    "\n",
    "        # ---- Actor update ----\n",
    "        # Actor 更新では Q_net を通すが、Q_net 自体は更新しないので凍結（計算の節約＋安全）\n",
    "        for p in self.Q_net.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        \n",
    "        actions_for_Ploss = self.P_net(states)\n",
    "        P_loss = -self.Q_net(torch.cat([states,actions_for_Ploss],dim=1)).mean()\n",
    "        self.P_optim.zero_grad()\n",
    "        P_loss.backward()\n",
    "        self.P_optim.step()\n",
    "\n",
    "        for p in self.Q_net.parameters():\n",
    "            p.requires_grad_(True)\n",
    "\n",
    "        self.soft_update(\n",
    "            target_net=self.Q_target_net,\n",
    "            online_net=self.Q_net,\n",
    "            tau=self.Config.tau\n",
    "            )\n",
    "        self.soft_update(\n",
    "            target_net=self.P_target_net,\n",
    "            online_net=self.P_net,\n",
    "            tau=self.Config.tau\n",
    "            )\n",
    "        \n",
    "        return float(Q_loss.item()), float(P_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8e097cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    env,\n",
    "    agent,\n",
    "    buffer,\n",
    "    total_step=40000,\n",
    "    warmup_steps=1000,\n",
    "    batch_num=512,\n",
    "):\n",
    "    print(\"cuda available:\", torch.cuda.is_available())\n",
    "    print(\"agent device:\", agent.device)\n",
    "    print(\"P_net device:\", next(agent.P_net.parameters()).device)\n",
    "    print(\"Q_net device:\", next(agent.Q_net.parameters()).device)\n",
    "    \n",
    "    # ログ保存用のリスト\n",
    "    Q_loss_history = []\n",
    "    P_loss_history = []\n",
    "    episode_num = 1\n",
    "    reward_history = []\n",
    "    reward_log = 0\n",
    "\n",
    "    # 1) 環境を初期化して最初の観測を得る\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    # 2) 環境ステップを total_step 回まわす\n",
    "    for t in range(total_step):\n",
    "\n",
    "        # logging.info(\"train step %d start\", t)\n",
    "\n",
    "        # ---- (A) 行動選択：warmup まではランダム、その後は方策+ノイズが定石 ----\n",
    "        if len(buffer) < warmup_steps:\n",
    "            # 環境の action_space に従ってランダム行動（探索の立ち上がりを安定化）\n",
    "            action = env.action_space.sample()\n",
    "            # logging.info(\"warmup now\")\n",
    "        else:\n",
    "            # DDPG の探索：方策にノイズを加えた行動\n",
    "            action = agent.step_with_noise(obs)\n",
    "            # logging.info(\"training\")\n",
    "\n",
    "        # 3) 環境を1ステップ進める\n",
    "        obs_next, reward, terminated, truncated, info = env.step(action)\n",
    "        reward_log += reward\n",
    "\n",
    "        # 4) “エピソード終了”判定（reset のため）\n",
    "        done = float(terminated)\n",
    "\n",
    "        # ---- (B) バッファに格納：学習ターゲット用の done は方針に注意 ----\n",
    "        # 方針1（簡単）：done をそのまま入れる（truncatedでもブートストラップ停止）\n",
    "        buffer.add(obs, action, reward, obs_next, done)\n",
    "\n",
    "        # 方針2（理屈に忠実）：terminated を入れる（truncatedはブートストラップ継続）\n",
    "        # buffer.add(obs, action, reward, obs_next, terminated)\n",
    "\n",
    "        # 5) 次の観測へ更新（done なら reset）\n",
    "        # doneはterminatedにしたので、ここではterminatedとtruncatedのorを使う\n",
    "        if terminated or truncated:\n",
    "            logging.info('train episode %d: reward = %.2f',\n",
    "                         episode_num, reward_log)\n",
    "            episode_num += 1\n",
    "            obs, info = env.reset()\n",
    "            reward_history.append(reward_log)\n",
    "            reward_log = 0\n",
    "        else:\n",
    "            obs = obs_next\n",
    "\n",
    "        # 6) バッファが十分でなければ学習をスキップ\n",
    "        if len(buffer) < warmup_steps:\n",
    "            continue\n",
    "\n",
    "        # 7) ミニバッチを取り出して更新\n",
    "        minibatch = buffer.sample(batch_num)\n",
    "        states      = minibatch[\"obs\"]\n",
    "        actions     = minibatch[\"act\"]\n",
    "        rewards     = minibatch[\"rew\"]\n",
    "        states_next = minibatch[\"obs_next\"]\n",
    "        dones       = minibatch[\"done\"]\n",
    "\n",
    "        Q_loss, P_loss = agent.update_net(states, actions, rewards, states_next, dones)\n",
    "\n",
    "        # 8) ログが欲しいならここで（毎step item() は遅くなるので間引くのが定石）\n",
    "        if t % 100 == 0:\n",
    "            Q_loss_history.append(Q_loss)\n",
    "            P_loss_history.append(P_loss)\n",
    "\n",
    "    return Q_loss_history, P_loss_history, reward_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3246743b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "agent device: cuda\n",
      "P_net device: cuda:0\n",
      "Q_net device: cuda:0\n",
      "01:22:13 [INFO] train episode 1: reward = -990.51\n",
      "01:22:13 [INFO] train episode 2: reward = -1163.27\n",
      "01:22:13 [INFO] train episode 3: reward = -1026.20\n",
      "01:22:13 [INFO] train episode 4: reward = -1047.89\n",
      "01:22:13 [INFO] train episode 5: reward = -1198.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_704160/2797934865.py:220: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at /pytorch/aten/src/ATen/native/Scalar.cpp:22.)\n",
      "  return float(Q_loss.item()), float(P_loss.item())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:22:16 [INFO] train episode 6: reward = -1473.12\n",
      "01:22:18 [INFO] train episode 7: reward = -1525.94\n",
      "01:22:20 [INFO] train episode 8: reward = -1583.87\n",
      "01:22:23 [INFO] train episode 9: reward = -1513.50\n",
      "01:22:25 [INFO] train episode 10: reward = -1593.19\n",
      "01:22:27 [INFO] train episode 11: reward = -689.48\n",
      "01:22:29 [INFO] train episode 12: reward = -1173.72\n",
      "01:22:32 [INFO] train episode 13: reward = -1057.91\n",
      "01:22:34 [INFO] train episode 14: reward = -1588.27\n",
      "01:22:36 [INFO] train episode 15: reward = -1141.75\n",
      "01:22:38 [INFO] train episode 16: reward = -906.98\n",
      "01:22:41 [INFO] train episode 17: reward = -512.01\n",
      "01:22:43 [INFO] train episode 18: reward = -503.35\n",
      "01:22:45 [INFO] train episode 19: reward = -2.02\n",
      "01:22:48 [INFO] train episode 20: reward = -2.41\n",
      "01:22:50 [INFO] train episode 21: reward = -1019.99\n",
      "01:22:52 [INFO] train episode 22: reward = -266.19\n",
      "01:22:55 [INFO] train episode 23: reward = -242.88\n",
      "01:22:57 [INFO] train episode 24: reward = -257.36\n",
      "01:22:59 [INFO] train episode 25: reward = -245.05\n",
      "01:23:01 [INFO] train episode 26: reward = -270.71\n",
      "01:23:04 [INFO] train episode 27: reward = -261.56\n",
      "01:23:07 [INFO] train episode 28: reward = -497.38\n",
      "01:23:09 [INFO] train episode 29: reward = -653.38\n",
      "01:23:11 [INFO] train episode 30: reward = -130.98\n",
      "01:23:13 [INFO] train episode 31: reward = -336.35\n",
      "01:23:15 [INFO] train episode 32: reward = -424.33\n",
      "01:23:17 [INFO] train episode 33: reward = -391.03\n",
      "01:23:19 [INFO] train episode 34: reward = -603.67\n",
      "01:23:21 [INFO] train episode 35: reward = -378.24\n",
      "01:23:23 [INFO] train episode 36: reward = -503.08\n",
      "01:23:25 [INFO] train episode 37: reward = -511.07\n",
      "01:23:27 [INFO] train episode 38: reward = -628.05\n",
      "01:23:29 [INFO] train episode 39: reward = -815.01\n",
      "01:23:31 [INFO] train episode 40: reward = -457.97\n",
      "01:23:33 [INFO] train episode 41: reward = -766.14\n",
      "01:23:35 [INFO] train episode 42: reward = -526.93\n",
      "01:23:37 [INFO] train episode 43: reward = -525.96\n",
      "01:23:39 [INFO] train episode 44: reward = -847.93\n",
      "01:23:41 [INFO] train episode 45: reward = -683.41\n",
      "01:23:44 [INFO] train episode 46: reward = -521.80\n",
      "01:23:46 [INFO] train episode 47: reward = -672.68\n",
      "01:23:48 [INFO] train episode 48: reward = -646.31\n",
      "01:23:50 [INFO] train episode 49: reward = -692.03\n",
      "01:23:52 [INFO] train episode 50: reward = -654.45\n",
      "01:23:54 [INFO] train episode 51: reward = -514.87\n",
      "01:23:56 [INFO] train episode 52: reward = -637.31\n",
      "01:23:59 [INFO] train episode 53: reward = -568.14\n",
      "01:24:01 [INFO] train episode 54: reward = -1350.25\n",
      "01:24:03 [INFO] train episode 55: reward = -763.71\n",
      "01:24:05 [INFO] train episode 56: reward = -628.13\n",
      "01:24:07 [INFO] train episode 57: reward = -626.03\n",
      "01:24:09 [INFO] train episode 58: reward = -494.85\n",
      "01:24:11 [INFO] train episode 59: reward = -758.27\n",
      "01:24:13 [INFO] train episode 60: reward = -595.34\n",
      "01:24:16 [INFO] train episode 61: reward = -637.09\n",
      "01:24:18 [INFO] train episode 62: reward = -638.25\n",
      "01:24:20 [INFO] train episode 63: reward = -892.91\n",
      "01:24:22 [INFO] train episode 64: reward = -724.80\n",
      "01:24:24 [INFO] train episode 65: reward = -683.20\n",
      "01:24:26 [INFO] train episode 66: reward = -726.03\n",
      "01:24:28 [INFO] train episode 67: reward = -663.48\n",
      "01:24:30 [INFO] train episode 68: reward = -383.11\n",
      "01:24:32 [INFO] train episode 69: reward = -395.25\n",
      "01:24:34 [INFO] train episode 70: reward = -391.00\n",
      "01:24:36 [INFO] train episode 71: reward = -750.26\n",
      "01:24:39 [INFO] train episode 72: reward = -550.00\n",
      "01:24:41 [INFO] train episode 73: reward = -740.17\n",
      "01:24:43 [INFO] train episode 74: reward = -560.86\n",
      "01:24:45 [INFO] train episode 75: reward = -517.54\n",
      "01:24:47 [INFO] train episode 76: reward = -638.36\n",
      "01:24:49 [INFO] train episode 77: reward = -758.02\n",
      "01:24:51 [INFO] train episode 78: reward = -730.92\n",
      "01:24:54 [INFO] train episode 79: reward = -760.24\n",
      "01:24:56 [INFO] train episode 80: reward = -647.52\n",
      "01:24:58 [INFO] train episode 81: reward = -655.15\n",
      "01:25:00 [INFO] train episode 82: reward = -707.00\n",
      "01:25:02 [INFO] train episode 83: reward = -645.79\n",
      "01:25:04 [INFO] train episode 84: reward = -666.47\n",
      "01:25:06 [INFO] train episode 85: reward = -773.69\n",
      "01:25:08 [INFO] train episode 86: reward = -890.64\n",
      "01:25:10 [INFO] train episode 87: reward = -655.10\n",
      "01:25:12 [INFO] train episode 88: reward = -750.37\n",
      "01:25:14 [INFO] train episode 89: reward = -760.06\n",
      "01:25:16 [INFO] train episode 90: reward = -686.32\n",
      "01:25:18 [INFO] train episode 91: reward = -759.42\n",
      "01:25:19 [INFO] train episode 92: reward = -804.22\n",
      "01:25:21 [INFO] train episode 93: reward = -767.89\n",
      "01:25:23 [INFO] train episode 94: reward = -730.89\n",
      "01:25:26 [INFO] train episode 95: reward = -731.67\n",
      "01:25:28 [INFO] train episode 96: reward = -770.06\n",
      "01:25:30 [INFO] train episode 97: reward = -270.59\n",
      "01:25:32 [INFO] train episode 98: reward = -724.17\n",
      "01:25:34 [INFO] train episode 99: reward = -805.18\n",
      "01:25:36 [INFO] train episode 100: reward = -754.20\n",
      "01:25:37 [INFO] train episode 101: reward = -502.27\n",
      "01:25:39 [INFO] train episode 102: reward = -734.12\n",
      "01:25:41 [INFO] train episode 103: reward = -645.33\n",
      "01:25:43 [INFO] train episode 104: reward = -522.97\n",
      "01:25:45 [INFO] train episode 105: reward = -888.66\n",
      "01:25:47 [INFO] train episode 106: reward = -640.55\n",
      "01:25:48 [INFO] train episode 107: reward = -530.57\n",
      "01:25:50 [INFO] train episode 108: reward = -743.99\n",
      "01:25:52 [INFO] train episode 109: reward = -527.80\n",
      "01:25:54 [INFO] train episode 110: reward = -765.39\n",
      "01:25:56 [INFO] train episode 111: reward = -666.29\n",
      "01:25:58 [INFO] train episode 112: reward = -579.47\n",
      "01:26:00 [INFO] train episode 113: reward = -778.36\n",
      "01:26:01 [INFO] train episode 114: reward = -410.56\n",
      "01:26:03 [INFO] train episode 115: reward = -507.19\n",
      "01:26:05 [INFO] train episode 116: reward = -653.99\n",
      "01:26:07 [INFO] train episode 117: reward = -755.35\n",
      "01:26:09 [INFO] train episode 118: reward = -124.77\n",
      "01:26:11 [INFO] train episode 119: reward = -257.17\n",
      "01:26:13 [INFO] train episode 120: reward = -522.78\n",
      "01:26:14 [INFO] train episode 121: reward = -721.89\n",
      "01:26:16 [INFO] train episode 122: reward = -644.56\n",
      "01:26:18 [INFO] train episode 123: reward = -1.38\n",
      "01:26:20 [INFO] train episode 124: reward = -388.58\n",
      "01:26:22 [INFO] train episode 125: reward = -258.37\n",
      "01:26:24 [INFO] train episode 126: reward = -511.73\n",
      "01:26:26 [INFO] train episode 127: reward = -756.88\n",
      "01:26:28 [INFO] train episode 128: reward = -668.54\n",
      "01:26:30 [INFO] train episode 129: reward = -515.47\n",
      "01:26:32 [INFO] train episode 130: reward = -710.10\n",
      "01:26:34 [INFO] train episode 131: reward = -642.52\n",
      "01:26:36 [INFO] train episode 132: reward = -830.75\n",
      "01:26:38 [INFO] train episode 133: reward = -759.90\n",
      "01:26:40 [INFO] train episode 134: reward = -516.19\n",
      "01:26:42 [INFO] train episode 135: reward = -772.24\n",
      "01:26:43 [INFO] train episode 136: reward = -951.13\n",
      "01:26:45 [INFO] train episode 137: reward = -399.49\n",
      "01:26:47 [INFO] train episode 138: reward = -833.13\n",
      "01:26:49 [INFO] train episode 139: reward = -642.77\n",
      "01:26:51 [INFO] train episode 140: reward = -850.90\n",
      "01:26:52 [INFO] train episode 141: reward = -889.07\n",
      "01:26:54 [INFO] train episode 142: reward = -808.41\n",
      "01:26:55 [INFO] train episode 143: reward = -484.64\n",
      "01:26:57 [INFO] train episode 144: reward = -903.82\n",
      "01:26:58 [INFO] train episode 145: reward = -788.51\n",
      "01:27:00 [INFO] train episode 146: reward = -521.95\n",
      "01:27:01 [INFO] train episode 147: reward = -520.26\n",
      "01:27:03 [INFO] train episode 148: reward = -409.33\n",
      "01:27:04 [INFO] train episode 149: reward = -375.97\n",
      "01:27:06 [INFO] train episode 150: reward = -129.32\n",
      "01:27:07 [INFO] train episode 151: reward = -386.01\n",
      "01:27:09 [INFO] train episode 152: reward = -256.46\n",
      "01:27:10 [INFO] train episode 153: reward = -499.60\n",
      "01:27:11 [INFO] train episode 154: reward = -252.24\n",
      "01:27:13 [INFO] train episode 155: reward = -923.80\n",
      "01:27:14 [INFO] train episode 156: reward = -1585.74\n",
      "01:27:16 [INFO] train episode 157: reward = -639.06\n",
      "01:27:17 [INFO] train episode 158: reward = -615.33\n",
      "01:27:18 [INFO] train episode 159: reward = -128.50\n",
      "01:27:20 [INFO] train episode 160: reward = -393.03\n",
      "01:27:22 [INFO] train episode 161: reward = -411.17\n",
      "01:27:23 [INFO] train episode 162: reward = -258.16\n",
      "01:27:24 [INFO] train episode 163: reward = -401.13\n",
      "01:27:26 [INFO] train episode 164: reward = -242.56\n",
      "01:27:27 [INFO] train episode 165: reward = -246.49\n",
      "01:27:29 [INFO] train episode 166: reward = -260.87\n",
      "01:27:30 [INFO] train episode 167: reward = -384.15\n",
      "01:27:32 [INFO] train episode 168: reward = -378.64\n",
      "01:27:33 [INFO] train episode 169: reward = -128.15\n",
      "01:27:35 [INFO] train episode 170: reward = -389.70\n",
      "01:27:36 [INFO] train episode 171: reward = -252.63\n",
      "01:27:38 [INFO] train episode 172: reward = -264.41\n",
      "01:27:40 [INFO] train episode 173: reward = -514.66\n",
      "01:27:41 [INFO] train episode 174: reward = -135.33\n",
      "01:27:43 [INFO] train episode 175: reward = -775.46\n",
      "01:27:44 [INFO] train episode 176: reward = -373.40\n",
      "01:27:46 [INFO] train episode 177: reward = -250.29\n",
      "01:27:47 [INFO] train episode 178: reward = -387.93\n",
      "01:27:49 [INFO] train episode 179: reward = -3.81\n",
      "01:27:51 [INFO] train episode 180: reward = -4.59\n",
      "01:27:52 [INFO] train episode 181: reward = -263.28\n",
      "01:27:54 [INFO] train episode 182: reward = -131.40\n",
      "01:27:55 [INFO] train episode 183: reward = -644.75\n",
      "01:27:57 [INFO] train episode 184: reward = -255.83\n",
      "01:27:59 [INFO] train episode 185: reward = -254.58\n",
      "01:28:00 [INFO] train episode 186: reward = -263.44\n",
      "01:28:02 [INFO] train episode 187: reward = -257.63\n",
      "01:28:04 [INFO] train episode 188: reward = -365.06\n",
      "01:28:06 [INFO] train episode 189: reward = -771.80\n",
      "01:28:08 [INFO] train episode 190: reward = -383.08\n",
      "01:28:09 [INFO] train episode 191: reward = -282.63\n",
      "01:28:11 [INFO] train episode 192: reward = -127.69\n",
      "01:28:12 [INFO] train episode 193: reward = -498.29\n",
      "01:28:14 [INFO] train episode 194: reward = -130.57\n",
      "01:28:15 [INFO] train episode 195: reward = -131.06\n",
      "01:28:17 [INFO] train episode 196: reward = -691.18\n",
      "01:28:18 [INFO] train episode 197: reward = -643.55\n",
      "01:28:20 [INFO] train episode 198: reward = -526.96\n",
      "01:28:21 [INFO] train episode 199: reward = -647.15\n",
      "01:28:23 [INFO] train episode 200: reward = -387.76\n",
      "01:28:25 [INFO] train episode 201: reward = -529.17\n",
      "01:28:27 [INFO] train episode 202: reward = -654.50\n",
      "01:28:29 [INFO] train episode 203: reward = -778.40\n",
      "01:28:31 [INFO] train episode 204: reward = -262.87\n",
      "01:28:33 [INFO] train episode 205: reward = -1025.04\n",
      "01:28:35 [INFO] train episode 206: reward = -260.90\n",
      "01:28:37 [INFO] train episode 207: reward = -943.75\n",
      "01:28:39 [INFO] train episode 208: reward = -361.08\n",
      "01:28:41 [INFO] train episode 209: reward = -929.34\n",
      "01:28:43 [INFO] train episode 210: reward = -487.58\n",
      "01:28:45 [INFO] train episode 211: reward = -434.98\n",
      "01:28:47 [INFO] train episode 212: reward = -373.34\n",
      "01:28:49 [INFO] train episode 213: reward = -517.82\n",
      "01:28:50 [INFO] train episode 214: reward = -256.29\n",
      "01:28:52 [INFO] train episode 215: reward = -256.88\n",
      "01:28:54 [INFO] train episode 216: reward = -251.64\n",
      "01:28:57 [INFO] train episode 217: reward = -804.15\n",
      "01:28:59 [INFO] train episode 218: reward = -380.69\n",
      "01:29:01 [INFO] train episode 219: reward = -264.97\n",
      "01:29:03 [INFO] train episode 220: reward = -737.81\n",
      "01:29:05 [INFO] train episode 221: reward = -749.42\n",
      "01:29:07 [INFO] train episode 222: reward = -396.56\n",
      "01:29:08 [INFO] train episode 223: reward = -29.03\n",
      "01:29:10 [INFO] train episode 224: reward = -634.82\n",
      "01:29:11 [INFO] train episode 225: reward = -368.92\n",
      "01:29:13 [INFO] train episode 226: reward = -266.02\n",
      "01:29:15 [INFO] train episode 227: reward = -142.41\n",
      "01:29:17 [INFO] train episode 228: reward = -381.19\n",
      "01:29:19 [INFO] train episode 229: reward = -254.37\n",
      "01:29:20 [INFO] train episode 230: reward = -257.50\n",
      "01:29:22 [INFO] train episode 231: reward = -380.62\n",
      "01:29:23 [INFO] train episode 232: reward = -593.48\n",
      "01:29:25 [INFO] train episode 233: reward = -770.11\n",
      "01:29:27 [INFO] train episode 234: reward = -660.41\n",
      "01:29:28 [INFO] train episode 235: reward = -1592.23\n",
      "01:29:30 [INFO] train episode 236: reward = -808.40\n",
      "01:29:32 [INFO] train episode 237: reward = -484.45\n",
      "01:29:33 [INFO] train episode 238: reward = -266.58\n",
      "01:29:35 [INFO] train episode 239: reward = -653.48\n",
      "01:29:37 [INFO] train episode 240: reward = -524.65\n",
      "01:29:38 [INFO] train episode 241: reward = -641.03\n",
      "01:29:40 [INFO] train episode 242: reward = -534.58\n",
      "01:29:41 [INFO] train episode 243: reward = -581.16\n",
      "01:29:43 [INFO] train episode 244: reward = -634.78\n",
      "01:29:44 [INFO] train episode 245: reward = -616.55\n",
      "01:29:46 [INFO] train episode 246: reward = -390.85\n",
      "01:29:47 [INFO] train episode 247: reward = -389.42\n",
      "01:29:49 [INFO] train episode 248: reward = -495.57\n",
      "01:29:50 [INFO] train episode 249: reward = -264.45\n",
      "01:29:52 [INFO] train episode 250: reward = -386.59\n",
      "01:29:53 [INFO] train episode 251: reward = -384.09\n",
      "01:29:55 [INFO] train episode 252: reward = -273.62\n",
      "01:29:56 [INFO] train episode 253: reward = -265.71\n",
      "01:29:58 [INFO] train episode 254: reward = -647.77\n",
      "01:30:00 [INFO] train episode 255: reward = -632.87\n",
      "01:30:02 [INFO] train episode 256: reward = -271.88\n",
      "01:30:03 [INFO] train episode 257: reward = -518.66\n",
      "01:30:05 [INFO] train episode 258: reward = -263.66\n",
      "01:30:07 [INFO] train episode 259: reward = -396.57\n",
      "01:30:08 [INFO] train episode 260: reward = -268.81\n",
      "01:30:10 [INFO] train episode 261: reward = -130.97\n",
      "01:30:12 [INFO] train episode 262: reward = -482.00\n",
      "01:30:13 [INFO] train episode 263: reward = -255.26\n",
      "01:30:15 [INFO] train episode 264: reward = -129.01\n",
      "01:30:17 [INFO] train episode 265: reward = -255.67\n",
      "01:30:18 [INFO] train episode 266: reward = -249.63\n",
      "01:30:20 [INFO] train episode 267: reward = -241.96\n",
      "01:30:22 [INFO] train episode 268: reward = -119.37\n",
      "01:30:23 [INFO] train episode 269: reward = -383.81\n",
      "01:30:25 [INFO] train episode 270: reward = -129.40\n",
      "01:30:27 [INFO] train episode 271: reward = -228.89\n",
      "01:30:28 [INFO] train episode 272: reward = -806.41\n",
      "01:30:30 [INFO] train episode 273: reward = -131.30\n",
      "01:30:32 [INFO] train episode 274: reward = -131.07\n",
      "01:30:33 [INFO] train episode 275: reward = -130.92\n",
      "01:30:35 [INFO] train episode 276: reward = -484.61\n",
      "01:30:37 [INFO] train episode 277: reward = -133.80\n",
      "01:30:38 [INFO] train episode 278: reward = -131.27\n",
      "01:30:40 [INFO] train episode 279: reward = -259.83\n",
      "01:30:42 [INFO] train episode 280: reward = -411.23\n",
      "01:30:43 [INFO] train episode 281: reward = -380.57\n",
      "01:30:45 [INFO] train episode 282: reward = -132.87\n",
      "01:30:46 [INFO] train episode 283: reward = -254.54\n",
      "01:30:48 [INFO] train episode 284: reward = -372.67\n",
      "01:30:50 [INFO] train episode 285: reward = -248.58\n",
      "01:30:51 [INFO] train episode 286: reward = -124.53\n",
      "01:30:53 [INFO] train episode 287: reward = -235.25\n",
      "01:30:55 [INFO] train episode 288: reward = -600.50\n",
      "01:30:56 [INFO] train episode 289: reward = -130.08\n",
      "01:30:58 [INFO] train episode 290: reward = -373.15\n",
      "01:31:00 [INFO] train episode 291: reward = -126.54\n",
      "01:31:02 [INFO] train episode 292: reward = -261.13\n",
      "01:31:03 [INFO] train episode 293: reward = -448.51\n",
      "01:31:05 [INFO] train episode 294: reward = -121.76\n",
      "01:31:07 [INFO] train episode 295: reward = -377.52\n",
      "01:31:09 [INFO] train episode 296: reward = -764.12\n",
      "01:31:10 [INFO] train episode 297: reward = -389.36\n",
      "01:31:12 [INFO] train episode 298: reward = -3.12\n",
      "01:31:14 [INFO] train episode 299: reward = -540.42\n",
      "01:31:16 [INFO] train episode 300: reward = -371.59\n",
      "01:31:17 [INFO] train episode 301: reward = -341.12\n",
      "01:31:19 [INFO] train episode 302: reward = -127.34\n",
      "01:31:21 [INFO] train episode 303: reward = -124.29\n",
      "01:31:22 [INFO] train episode 304: reward = -382.96\n",
      "01:31:24 [INFO] train episode 305: reward = -255.70\n",
      "01:31:26 [INFO] train episode 306: reward = -133.97\n",
      "01:31:27 [INFO] train episode 307: reward = -130.03\n",
      "01:31:29 [INFO] train episode 308: reward = -130.26\n",
      "01:31:31 [INFO] train episode 309: reward = -5.85\n",
      "01:31:32 [INFO] train episode 310: reward = -140.44\n",
      "01:31:34 [INFO] train episode 311: reward = -325.76\n",
      "01:31:36 [INFO] train episode 312: reward = -663.96\n",
      "01:31:37 [INFO] train episode 313: reward = -771.65\n",
      "01:31:39 [INFO] train episode 314: reward = -793.52\n",
      "01:31:41 [INFO] train episode 315: reward = -250.15\n",
      "01:31:43 [INFO] train episode 316: reward = -523.49\n",
      "01:31:45 [INFO] train episode 317: reward = -396.04\n",
      "01:31:46 [INFO] train episode 318: reward = -529.08\n",
      "01:31:48 [INFO] train episode 319: reward = -516.21\n",
      "01:31:50 [INFO] train episode 320: reward = -380.67\n",
      "01:31:52 [INFO] train episode 321: reward = -384.20\n",
      "01:31:54 [INFO] train episode 322: reward = -394.96\n",
      "01:31:56 [INFO] train episode 323: reward = -368.24\n",
      "01:31:57 [INFO] train episode 324: reward = -377.54\n",
      "01:31:59 [INFO] train episode 325: reward = -145.26\n",
      "01:32:01 [INFO] train episode 326: reward = -8.24\n",
      "01:32:03 [INFO] train episode 327: reward = -134.74\n",
      "01:32:05 [INFO] train episode 328: reward = -374.36\n",
      "01:32:06 [INFO] train episode 329: reward = -256.99\n",
      "01:32:08 [INFO] train episode 330: reward = -477.09\n",
      "01:32:10 [INFO] train episode 331: reward = -641.00\n",
      "01:32:12 [INFO] train episode 332: reward = -393.45\n",
      "01:32:14 [INFO] train episode 333: reward = -515.39\n",
      "01:32:16 [INFO] train episode 334: reward = -389.38\n",
      "01:32:17 [INFO] train episode 335: reward = -496.99\n",
      "01:32:19 [INFO] train episode 336: reward = -261.37\n",
      "01:32:21 [INFO] train episode 337: reward = -249.10\n",
      "01:32:23 [INFO] train episode 338: reward = -642.70\n",
      "01:32:25 [INFO] train episode 339: reward = -370.54\n",
      "01:32:27 [INFO] train episode 340: reward = -389.12\n",
      "01:32:29 [INFO] train episode 341: reward = -519.07\n",
      "01:32:30 [INFO] train episode 342: reward = -636.00\n",
      "01:32:32 [INFO] train episode 343: reward = -518.86\n",
      "01:32:34 [INFO] train episode 344: reward = -263.81\n",
      "01:32:36 [INFO] train episode 345: reward = -259.92\n",
      "01:32:38 [INFO] train episode 346: reward = -259.34\n",
      "01:32:39 [INFO] train episode 347: reward = -133.15\n",
      "01:32:41 [INFO] train episode 348: reward = -119.42\n",
      "01:32:43 [INFO] train episode 349: reward = -119.82\n",
      "01:32:45 [INFO] train episode 350: reward = -365.58\n",
      "01:32:47 [INFO] train episode 351: reward = -129.60\n",
      "01:32:50 [INFO] train episode 352: reward = -373.63\n",
      "01:32:53 [INFO] train episode 353: reward = -254.46\n",
      "01:32:55 [INFO] train episode 354: reward = -242.92\n",
      "01:32:57 [INFO] train episode 355: reward = -127.42\n",
      "01:32:58 [INFO] train episode 356: reward = -122.49\n",
      "01:33:00 [INFO] train episode 357: reward = -487.10\n",
      "01:33:01 [INFO] train episode 358: reward = -803.51\n",
      "01:33:03 [INFO] train episode 359: reward = -602.11\n",
      "01:33:04 [INFO] train episode 360: reward = -386.62\n",
      "01:33:06 [INFO] train episode 361: reward = -643.00\n",
      "01:33:07 [INFO] train episode 362: reward = -390.06\n",
      "01:33:09 [INFO] train episode 363: reward = -396.26\n",
      "01:33:10 [INFO] train episode 364: reward = -262.13\n",
      "01:33:11 [INFO] train episode 365: reward = -387.66\n",
      "01:33:13 [INFO] train episode 366: reward = -615.93\n",
      "01:33:14 [INFO] train episode 367: reward = -512.23\n",
      "01:33:16 [INFO] train episode 368: reward = -477.15\n",
      "01:33:17 [INFO] train episode 369: reward = -131.18\n",
      "01:33:19 [INFO] train episode 370: reward = -365.61\n",
      "01:33:20 [INFO] train episode 371: reward = -377.64\n",
      "01:33:21 [INFO] train episode 372: reward = -257.99\n",
      "01:33:23 [INFO] train episode 373: reward = -250.53\n",
      "01:33:24 [INFO] train episode 374: reward = -127.36\n",
      "01:33:26 [INFO] train episode 375: reward = -467.93\n",
      "01:33:27 [INFO] train episode 376: reward = -262.51\n",
      "01:33:29 [INFO] train episode 377: reward = -246.70\n",
      "01:33:30 [INFO] train episode 378: reward = -644.25\n",
      "01:33:32 [INFO] train episode 379: reward = -521.91\n",
      "01:33:33 [INFO] train episode 380: reward = -415.19\n",
      "01:33:34 [INFO] train episode 381: reward = -391.20\n",
      "01:33:36 [INFO] train episode 382: reward = -650.82\n",
      "01:33:37 [INFO] train episode 383: reward = -586.06\n",
      "01:33:39 [INFO] train episode 384: reward = -2.99\n",
      "01:33:40 [INFO] train episode 385: reward = -387.92\n",
      "01:33:41 [INFO] train episode 386: reward = -2.32\n",
      "01:33:43 [INFO] train episode 387: reward = -385.76\n",
      "01:33:44 [INFO] train episode 388: reward = -259.57\n",
      "01:33:46 [INFO] train episode 389: reward = -902.43\n",
      "01:33:47 [INFO] train episode 390: reward = -452.17\n",
      "01:33:48 [INFO] train episode 391: reward = -526.90\n",
      "01:33:50 [INFO] train episode 392: reward = -646.36\n",
      "01:33:51 [INFO] train episode 393: reward = -397.09\n",
      "01:33:53 [INFO] train episode 394: reward = -134.95\n",
      "01:33:55 [INFO] train episode 395: reward = -643.58\n",
      "01:33:56 [INFO] train episode 396: reward = -784.81\n",
      "01:33:58 [INFO] train episode 397: reward = -389.44\n",
      "01:34:00 [INFO] train episode 398: reward = -512.68\n",
      "01:34:01 [INFO] train episode 399: reward = -256.08\n",
      "01:34:02 [INFO] train episode 400: reward = -378.74\n",
      "01:34:04 [INFO] train episode 401: reward = -506.53\n",
      "01:34:05 [INFO] train episode 402: reward = -648.23\n",
      "01:34:07 [INFO] train episode 403: reward = -668.09\n",
      "01:34:08 [INFO] train episode 404: reward = -269.17\n",
      "01:34:10 [INFO] train episode 405: reward = -808.51\n",
      "01:34:11 [INFO] train episode 406: reward = -780.31\n",
      "01:34:13 [INFO] train episode 407: reward = -396.97\n",
      "01:34:14 [INFO] train episode 408: reward = -639.87\n",
      "01:34:16 [INFO] train episode 409: reward = -518.66\n",
      "01:34:17 [INFO] train episode 410: reward = -753.31\n",
      "01:34:19 [INFO] train episode 411: reward = -1226.82\n",
      "01:34:20 [INFO] train episode 412: reward = -632.59\n",
      "01:34:22 [INFO] train episode 413: reward = -813.59\n",
      "01:34:23 [INFO] train episode 414: reward = -772.13\n",
      "01:34:24 [INFO] train episode 415: reward = -748.60\n",
      "01:34:26 [INFO] train episode 416: reward = -654.04\n",
      "01:34:27 [INFO] train episode 417: reward = -590.09\n",
      "01:34:29 [INFO] train episode 418: reward = -580.27\n",
      "01:34:30 [INFO] train episode 419: reward = -548.84\n",
      "01:34:31 [INFO] train episode 420: reward = -510.45\n",
      "01:34:33 [INFO] train episode 421: reward = -745.90\n",
      "01:34:34 [INFO] train episode 422: reward = -756.39\n",
      "01:34:36 [INFO] train episode 423: reward = -642.86\n",
      "01:34:37 [INFO] train episode 424: reward = -633.54\n",
      "01:34:39 [INFO] train episode 425: reward = -624.61\n",
      "01:34:40 [INFO] train episode 426: reward = -624.28\n",
      "01:34:41 [INFO] train episode 427: reward = -521.18\n",
      "01:34:43 [INFO] train episode 428: reward = -767.39\n",
      "01:34:44 [INFO] train episode 429: reward = -394.21\n",
      "01:34:46 [INFO] train episode 430: reward = -544.39\n",
      "01:34:47 [INFO] train episode 431: reward = -613.73\n",
      "01:34:49 [INFO] train episode 432: reward = -656.85\n",
      "01:34:50 [INFO] train episode 433: reward = -640.78\n",
      "01:34:52 [INFO] train episode 434: reward = -411.57\n",
      "01:34:53 [INFO] train episode 435: reward = -635.80\n",
      "01:34:55 [INFO] train episode 436: reward = -649.03\n",
      "01:34:56 [INFO] train episode 437: reward = -514.54\n",
      "01:34:58 [INFO] train episode 438: reward = -745.53\n",
      "01:34:59 [INFO] train episode 439: reward = -751.98\n",
      "01:35:00 [INFO] train episode 440: reward = -795.06\n",
      "01:35:02 [INFO] train episode 441: reward = -641.93\n",
      "01:35:03 [INFO] train episode 442: reward = -644.42\n",
      "01:35:05 [INFO] train episode 443: reward = -634.97\n",
      "01:35:06 [INFO] train episode 444: reward = -516.70\n",
      "01:35:08 [INFO] train episode 445: reward = -543.92\n",
      "01:35:09 [INFO] train episode 446: reward = -517.72\n",
      "01:35:10 [INFO] train episode 447: reward = -778.57\n",
      "01:35:12 [INFO] train episode 448: reward = -396.12\n",
      "01:35:13 [INFO] train episode 449: reward = -521.24\n",
      "01:35:15 [INFO] train episode 450: reward = -392.73\n",
      "01:35:16 [INFO] train episode 451: reward = -931.55\n",
      "01:35:18 [INFO] train episode 452: reward = -584.03\n",
      "01:35:19 [INFO] train episode 453: reward = -402.29\n",
      "01:35:21 [INFO] train episode 454: reward = -523.48\n",
      "01:35:22 [INFO] train episode 455: reward = -637.59\n",
      "01:35:23 [INFO] train episode 456: reward = -655.11\n",
      "01:35:25 [INFO] train episode 457: reward = -645.90\n",
      "01:35:26 [INFO] train episode 458: reward = -521.69\n",
      "01:35:28 [INFO] train episode 459: reward = -860.07\n",
      "01:35:29 [INFO] train episode 460: reward = -781.85\n",
      "01:35:31 [INFO] train episode 461: reward = -765.31\n",
      "01:35:32 [INFO] train episode 462: reward = -525.97\n",
      "01:35:33 [INFO] train episode 463: reward = -396.04\n",
      "01:35:35 [INFO] train episode 464: reward = -763.18\n",
      "01:35:36 [INFO] train episode 465: reward = -723.39\n",
      "01:35:38 [INFO] train episode 466: reward = -645.80\n",
      "01:35:39 [INFO] train episode 467: reward = -651.35\n",
      "01:35:41 [INFO] train episode 468: reward = -523.43\n",
      "01:35:42 [INFO] train episode 469: reward = -706.80\n",
      "01:35:43 [INFO] train episode 470: reward = -767.46\n",
      "01:35:45 [INFO] train episode 471: reward = -643.01\n",
      "01:35:46 [INFO] train episode 472: reward = -646.87\n",
      "01:35:48 [INFO] train episode 473: reward = -762.90\n",
      "01:35:49 [INFO] train episode 474: reward = -522.58\n",
      "01:35:51 [INFO] train episode 475: reward = -524.52\n",
      "01:35:52 [INFO] train episode 476: reward = -636.20\n",
      "01:35:54 [INFO] train episode 477: reward = -755.96\n",
      "01:35:55 [INFO] train episode 478: reward = -522.19\n",
      "01:35:57 [INFO] train episode 479: reward = -645.76\n",
      "01:35:58 [INFO] train episode 480: reward = -746.43\n",
      "01:36:00 [INFO] train episode 481: reward = -648.16\n",
      "01:36:02 [INFO] train episode 482: reward = -644.28\n",
      "01:36:03 [INFO] train episode 483: reward = -394.80\n",
      "01:36:05 [INFO] train episode 484: reward = -509.77\n",
      "01:36:06 [INFO] train episode 485: reward = -878.72\n",
      "01:36:08 [INFO] train episode 486: reward = -798.70\n",
      "01:36:10 [INFO] train episode 487: reward = -675.81\n",
      "01:36:11 [INFO] train episode 488: reward = -770.04\n",
      "01:36:13 [INFO] train episode 489: reward = -788.69\n",
      "01:36:14 [INFO] train episode 490: reward = -754.43\n",
      "01:36:16 [INFO] train episode 491: reward = -770.27\n",
      "01:36:17 [INFO] train episode 492: reward = -653.35\n",
      "01:36:19 [INFO] train episode 493: reward = -3.15\n",
      "01:36:20 [INFO] train episode 494: reward = -764.29\n",
      "01:36:22 [INFO] train episode 495: reward = -522.01\n",
      "01:36:23 [INFO] train episode 496: reward = -762.29\n",
      "01:36:25 [INFO] train episode 497: reward = -765.69\n",
      "01:36:26 [INFO] train episode 498: reward = -800.26\n",
      "01:36:28 [INFO] train episode 499: reward = -768.87\n",
      "01:36:29 [INFO] train episode 500: reward = -767.18\n",
      "01:36:31 [INFO] train episode 501: reward = -872.74\n",
      "01:36:32 [INFO] train episode 502: reward = -767.95\n",
      "01:36:34 [INFO] train episode 503: reward = -875.06\n",
      "01:36:36 [INFO] train episode 504: reward = -768.80\n",
      "01:36:38 [INFO] train episode 505: reward = -765.93\n",
      "01:36:39 [INFO] train episode 506: reward = -778.23\n",
      "01:36:41 [INFO] train episode 507: reward = -765.22\n",
      "01:36:42 [INFO] train episode 508: reward = -760.15\n",
      "01:36:44 [INFO] train episode 509: reward = -784.40\n",
      "01:36:45 [INFO] train episode 510: reward = -635.67\n",
      "01:36:47 [INFO] train episode 511: reward = -764.09\n",
      "01:36:48 [INFO] train episode 512: reward = -637.11\n",
      "01:36:49 [INFO] train episode 513: reward = -901.53\n",
      "01:36:51 [INFO] train episode 514: reward = -645.29\n",
      "01:36:52 [INFO] train episode 515: reward = -490.14\n",
      "01:36:53 [INFO] train episode 516: reward = -648.44\n",
      "01:36:55 [INFO] train episode 517: reward = -517.78\n",
      "01:36:56 [INFO] train episode 518: reward = -516.81\n",
      "01:36:58 [INFO] train episode 519: reward = -622.84\n",
      "01:36:59 [INFO] train episode 520: reward = -632.02\n",
      "01:37:01 [INFO] train episode 521: reward = -484.36\n",
      "01:37:02 [INFO] train episode 522: reward = -558.42\n",
      "01:37:03 [INFO] train episode 523: reward = -766.58\n",
      "01:37:05 [INFO] train episode 524: reward = -648.90\n",
      "01:37:06 [INFO] train episode 525: reward = -795.49\n",
      "01:37:07 [INFO] train episode 526: reward = -647.17\n",
      "01:37:09 [INFO] train episode 527: reward = -825.08\n",
      "01:37:10 [INFO] train episode 528: reward = -873.67\n",
      "01:37:11 [INFO] train episode 529: reward = -529.09\n",
      "01:37:13 [INFO] train episode 530: reward = -514.11\n",
      "01:37:14 [INFO] train episode 531: reward = -718.94\n",
      "01:37:16 [INFO] train episode 532: reward = -377.05\n",
      "01:37:17 [INFO] train episode 533: reward = -762.38\n",
      "01:37:18 [INFO] train episode 534: reward = -893.35\n",
      "01:37:20 [INFO] train episode 535: reward = -1061.90\n",
      "01:37:21 [INFO] train episode 536: reward = -898.07\n",
      "01:37:23 [INFO] train episode 537: reward = -983.67\n",
      "01:37:24 [INFO] train episode 538: reward = -802.93\n",
      "01:37:25 [INFO] train episode 539: reward = -771.55\n",
      "01:37:27 [INFO] train episode 540: reward = -1228.19\n",
      "01:37:28 [INFO] train episode 541: reward = -771.19\n",
      "01:37:30 [INFO] train episode 542: reward = -836.13\n",
      "01:37:31 [INFO] train episode 543: reward = -385.47\n",
      "01:37:33 [INFO] train episode 544: reward = -388.89\n",
      "01:37:34 [INFO] train episode 545: reward = -833.32\n",
      "01:37:36 [INFO] train episode 546: reward = -908.16\n",
      "01:37:37 [INFO] train episode 547: reward = -340.97\n",
      "01:37:39 [INFO] train episode 548: reward = -810.34\n",
      "01:37:40 [INFO] train episode 549: reward = -259.79\n",
      "01:37:41 [INFO] train episode 550: reward = -257.56\n",
      "01:37:43 [INFO] train episode 551: reward = -250.91\n",
      "01:37:45 [INFO] train episode 552: reward = -909.35\n",
      "01:37:46 [INFO] train episode 553: reward = -247.18\n",
      "01:37:48 [INFO] train episode 554: reward = -605.93\n",
      "01:37:49 [INFO] train episode 555: reward = -1.91\n",
      "01:37:51 [INFO] train episode 556: reward = -647.40\n",
      "01:37:53 [INFO] train episode 557: reward = -378.28\n",
      "01:37:54 [INFO] train episode 558: reward = -663.58\n",
      "01:37:56 [INFO] train episode 559: reward = -898.34\n",
      "01:37:57 [INFO] train episode 560: reward = -654.58\n",
      "01:37:59 [INFO] train episode 561: reward = -4.00\n",
      "01:38:00 [INFO] train episode 562: reward = -131.62\n",
      "01:38:02 [INFO] train episode 563: reward = -639.83\n",
      "01:38:03 [INFO] train episode 564: reward = -124.04\n",
      "01:38:05 [INFO] train episode 565: reward = -134.68\n",
      "01:38:06 [INFO] train episode 566: reward = -645.05\n",
      "01:38:08 [INFO] train episode 567: reward = -768.00\n",
      "01:38:09 [INFO] train episode 568: reward = -811.11\n",
      "01:38:11 [INFO] train episode 569: reward = -249.52\n",
      "01:38:12 [INFO] train episode 570: reward = -260.01\n",
      "01:38:14 [INFO] train episode 571: reward = -856.79\n",
      "01:38:15 [INFO] train episode 572: reward = -876.72\n",
      "01:38:17 [INFO] train episode 573: reward = -514.56\n",
      "01:38:18 [INFO] train episode 574: reward = -133.77\n",
      "01:38:20 [INFO] train episode 575: reward = -245.54\n",
      "01:38:21 [INFO] train episode 576: reward = -266.58\n",
      "01:38:23 [INFO] train episode 577: reward = -3.44\n",
      "01:38:24 [INFO] train episode 578: reward = -392.27\n",
      "01:38:26 [INFO] train episode 579: reward = -524.86\n",
      "01:38:27 [INFO] train episode 580: reward = -262.47\n",
      "01:38:29 [INFO] train episode 581: reward = -1579.02\n",
      "01:38:30 [INFO] train episode 582: reward = -761.82\n",
      "01:38:32 [INFO] train episode 583: reward = -767.08\n",
      "01:38:33 [INFO] train episode 584: reward = -3.81\n",
      "01:38:35 [INFO] train episode 585: reward = -869.49\n",
      "01:38:37 [INFO] train episode 586: reward = -779.23\n",
      "01:38:39 [INFO] train episode 587: reward = -886.08\n",
      "01:38:40 [INFO] train episode 588: reward = -812.79\n",
      "01:38:42 [INFO] train episode 589: reward = -798.27\n",
      "01:38:43 [INFO] train episode 590: reward = -889.63\n",
      "01:38:45 [INFO] train episode 591: reward = -768.16\n",
      "01:38:46 [INFO] train episode 592: reward = -890.86\n",
      "01:38:48 [INFO] train episode 593: reward = -768.41\n",
      "01:38:49 [INFO] train episode 594: reward = -143.81\n",
      "01:38:51 [INFO] train episode 595: reward = -544.97\n",
      "01:38:52 [INFO] train episode 596: reward = -251.58\n",
      "01:38:54 [INFO] train episode 597: reward = -253.76\n",
      "01:38:56 [INFO] train episode 598: reward = -258.77\n",
      "01:38:58 [INFO] train episode 599: reward = -771.61\n",
      "01:39:00 [INFO] train episode 600: reward = -391.17\n",
      "01:39:01 [INFO] train episode 601: reward = -523.58\n",
      "01:39:03 [INFO] train episode 602: reward = -271.56\n",
      "01:39:05 [INFO] train episode 603: reward = -509.13\n",
      "01:39:06 [INFO] train episode 604: reward = -259.02\n",
      "01:39:08 [INFO] train episode 605: reward = -752.42\n",
      "01:39:10 [INFO] train episode 606: reward = -767.29\n",
      "01:39:11 [INFO] train episode 607: reward = -253.97\n",
      "01:39:13 [INFO] train episode 608: reward = -372.91\n",
      "01:39:14 [INFO] train episode 609: reward = -383.29\n",
      "01:39:16 [INFO] train episode 610: reward = -487.87\n",
      "01:39:17 [INFO] train episode 611: reward = -251.54\n",
      "01:39:19 [INFO] train episode 612: reward = -374.12\n",
      "01:39:20 [INFO] train episode 613: reward = -148.67\n",
      "01:39:22 [INFO] train episode 614: reward = -1185.93\n",
      "01:39:23 [INFO] train episode 615: reward = -139.25\n",
      "01:39:25 [INFO] train episode 616: reward = -132.76\n",
      "01:39:27 [INFO] train episode 617: reward = -124.42\n",
      "01:39:28 [INFO] train episode 618: reward = -136.64\n",
      "01:39:30 [INFO] train episode 619: reward = -381.30\n",
      "01:39:32 [INFO] train episode 620: reward = -383.92\n",
      "01:39:34 [INFO] train episode 621: reward = -252.30\n",
      "01:39:35 [INFO] train episode 622: reward = -10.73\n",
      "01:39:37 [INFO] train episode 623: reward = -363.31\n",
      "01:39:38 [INFO] train episode 624: reward = -267.80\n",
      "01:39:40 [INFO] train episode 625: reward = -149.12\n",
      "01:39:41 [INFO] train episode 626: reward = -566.26\n",
      "01:39:43 [INFO] train episode 627: reward = -511.32\n",
      "01:39:44 [INFO] train episode 628: reward = -264.19\n",
      "01:39:46 [INFO] train episode 629: reward = -254.74\n",
      "01:39:47 [INFO] train episode 630: reward = -384.12\n",
      "01:39:49 [INFO] train episode 631: reward = -638.69\n",
      "01:39:51 [INFO] train episode 632: reward = -379.51\n",
      "01:39:52 [INFO] train episode 633: reward = -510.44\n",
      "01:39:54 [INFO] train episode 634: reward = -21.94\n",
      "01:39:55 [INFO] train episode 635: reward = -390.72\n",
      "01:39:57 [INFO] train episode 636: reward = -515.20\n",
      "01:39:58 [INFO] train episode 637: reward = -271.25\n",
      "01:40:00 [INFO] train episode 638: reward = -382.41\n",
      "01:40:01 [INFO] train episode 639: reward = -303.90\n",
      "01:40:02 [INFO] train episode 640: reward = -7.35\n",
      "01:40:04 [INFO] train episode 641: reward = -263.84\n",
      "01:40:05 [INFO] train episode 642: reward = -509.86\n",
      "01:40:06 [INFO] train episode 643: reward = -8.80\n",
      "01:40:08 [INFO] train episode 644: reward = -382.08\n",
      "01:40:10 [INFO] train episode 645: reward = -702.24\n",
      "01:40:12 [INFO] train episode 646: reward = -838.11\n",
      "01:40:13 [INFO] train episode 647: reward = -132.55\n",
      "01:40:15 [INFO] train episode 648: reward = -137.14\n",
      "01:40:18 [INFO] train episode 649: reward = -127.37\n",
      "01:40:20 [INFO] train episode 650: reward = -361.55\n",
      "01:40:22 [INFO] train episode 651: reward = -337.29\n",
      "01:40:24 [INFO] train episode 652: reward = -133.45\n",
      "01:40:26 [INFO] train episode 653: reward = -488.86\n",
      "01:40:28 [INFO] train episode 654: reward = -612.21\n",
      "01:40:30 [INFO] train episode 655: reward = -263.50\n",
      "01:40:32 [INFO] train episode 656: reward = -519.48\n",
      "01:40:33 [INFO] train episode 657: reward = -622.91\n",
      "01:40:35 [INFO] train episode 658: reward = -398.47\n",
      "01:40:37 [INFO] train episode 659: reward = -244.85\n",
      "01:40:38 [INFO] train episode 660: reward = -636.72\n",
      "01:40:40 [INFO] train episode 661: reward = -141.23\n",
      "01:40:41 [INFO] train episode 662: reward = -390.70\n",
      "01:40:42 [INFO] train episode 663: reward = -259.85\n",
      "01:40:43 [INFO] train episode 664: reward = -248.94\n",
      "01:40:45 [INFO] train episode 665: reward = -489.80\n",
      "01:40:46 [INFO] train episode 666: reward = -243.06\n",
      "01:40:47 [INFO] train episode 667: reward = -131.34\n",
      "01:40:49 [INFO] train episode 668: reward = -128.52\n",
      "01:40:50 [INFO] train episode 669: reward = -360.00\n",
      "01:40:51 [INFO] train episode 670: reward = -121.51\n",
      "01:40:53 [INFO] train episode 671: reward = -255.28\n",
      "01:40:54 [INFO] train episode 672: reward = -639.48\n",
      "01:40:55 [INFO] train episode 673: reward = -576.54\n",
      "01:40:57 [INFO] train episode 674: reward = -619.42\n",
      "01:40:58 [INFO] train episode 675: reward = -499.90\n",
      "01:40:59 [INFO] train episode 676: reward = -119.03\n",
      "01:41:01 [INFO] train episode 677: reward = -123.88\n",
      "01:41:02 [INFO] train episode 678: reward = -1.71\n",
      "01:41:03 [INFO] train episode 679: reward = -519.37\n",
      "01:41:05 [INFO] train episode 680: reward = -130.22\n",
      "01:41:06 [INFO] train episode 681: reward = -243.74\n",
      "01:41:08 [INFO] train episode 682: reward = -386.72\n",
      "01:41:09 [INFO] train episode 683: reward = -368.44\n",
      "01:41:11 [INFO] train episode 684: reward = -122.25\n",
      "01:41:12 [INFO] train episode 685: reward = -395.01\n",
      "01:41:14 [INFO] train episode 686: reward = -512.31\n",
      "01:41:15 [INFO] train episode 687: reward = -374.04\n",
      "01:41:17 [INFO] train episode 688: reward = -504.11\n",
      "01:41:18 [INFO] train episode 689: reward = -615.08\n",
      "01:41:20 [INFO] train episode 690: reward = -657.71\n",
      "01:41:22 [INFO] train episode 691: reward = -624.19\n",
      "01:41:23 [INFO] train episode 692: reward = -630.69\n",
      "01:41:25 [INFO] train episode 693: reward = -749.78\n",
      "01:41:27 [INFO] train episode 694: reward = -524.02\n",
      "01:41:28 [INFO] train episode 695: reward = -607.50\n",
      "01:41:30 [INFO] train episode 696: reward = -382.87\n",
      "01:41:31 [INFO] train episode 697: reward = -634.11\n",
      "01:41:32 [INFO] train episode 698: reward = -855.57\n",
      "01:41:34 [INFO] train episode 699: reward = -711.78\n",
      "01:41:35 [INFO] train episode 700: reward = -876.82\n",
      "01:41:36 [INFO] train episode 701: reward = -771.35\n",
      "01:41:38 [INFO] train episode 702: reward = -775.82\n",
      "01:41:39 [INFO] train episode 703: reward = -782.30\n",
      "01:41:40 [INFO] train episode 704: reward = -529.55\n",
      "01:41:42 [INFO] train episode 705: reward = -772.68\n",
      "01:41:43 [INFO] train episode 706: reward = -885.34\n",
      "01:41:45 [INFO] train episode 707: reward = -652.21\n",
      "01:41:47 [INFO] train episode 708: reward = -768.59\n",
      "01:41:48 [INFO] train episode 709: reward = -637.10\n",
      "01:41:49 [INFO] train episode 710: reward = -666.18\n",
      "01:41:51 [INFO] train episode 711: reward = -764.40\n",
      "01:41:52 [INFO] train episode 712: reward = -773.22\n",
      "01:41:54 [INFO] train episode 713: reward = -808.33\n",
      "01:41:55 [INFO] train episode 714: reward = -771.79\n",
      "01:41:56 [INFO] train episode 715: reward = -758.63\n",
      "01:41:58 [INFO] train episode 716: reward = -906.23\n",
      "01:41:59 [INFO] train episode 717: reward = -779.49\n",
      "01:42:00 [INFO] train episode 718: reward = -777.63\n",
      "01:42:02 [INFO] train episode 719: reward = -774.80\n",
      "01:42:03 [INFO] train episode 720: reward = -755.08\n",
      "01:42:05 [INFO] train episode 721: reward = -632.93\n",
      "01:42:06 [INFO] train episode 722: reward = -925.56\n",
      "01:42:07 [INFO] train episode 723: reward = -697.91\n",
      "01:42:09 [INFO] train episode 724: reward = -634.52\n",
      "01:42:10 [INFO] train episode 725: reward = -512.38\n",
      "01:42:11 [INFO] train episode 726: reward = -548.50\n",
      "01:42:13 [INFO] train episode 727: reward = -883.38\n",
      "01:42:14 [INFO] train episode 728: reward = -130.07\n",
      "01:42:15 [INFO] train episode 729: reward = -136.12\n",
      "01:42:17 [INFO] train episode 730: reward = -519.63\n",
      "01:42:18 [INFO] train episode 731: reward = -719.06\n",
      "01:42:19 [INFO] train episode 732: reward = -868.47\n",
      "01:42:21 [INFO] train episode 733: reward = -766.43\n",
      "01:42:22 [INFO] train episode 734: reward = -391.52\n",
      "01:42:23 [INFO] train episode 735: reward = -255.72\n",
      "01:42:25 [INFO] train episode 736: reward = -260.68\n",
      "01:42:26 [INFO] train episode 737: reward = -257.10\n",
      "01:42:27 [INFO] train episode 738: reward = -401.62\n",
      "01:42:29 [INFO] train episode 739: reward = -386.53\n",
      "01:42:30 [INFO] train episode 740: reward = -390.25\n",
      "01:42:31 [INFO] train episode 741: reward = -388.67\n",
      "01:42:33 [INFO] train episode 742: reward = -638.68\n",
      "01:42:34 [INFO] train episode 743: reward = -406.25\n",
      "01:42:36 [INFO] train episode 744: reward = -513.57\n",
      "01:42:37 [INFO] train episode 745: reward = -661.50\n",
      "01:42:38 [INFO] train episode 746: reward = -389.38\n",
      "01:42:39 [INFO] train episode 747: reward = -639.76\n",
      "01:42:41 [INFO] train episode 748: reward = -691.84\n",
      "01:42:42 [INFO] train episode 749: reward = -379.26\n",
      "01:42:43 [INFO] train episode 750: reward = -268.77\n",
      "01:42:45 [INFO] train episode 751: reward = -390.61\n",
      "01:42:46 [INFO] train episode 752: reward = -257.82\n",
      "01:42:47 [INFO] train episode 753: reward = -129.98\n",
      "01:42:48 [INFO] train episode 754: reward = -130.21\n",
      "01:42:50 [INFO] train episode 755: reward = -260.27\n",
      "01:42:51 [INFO] train episode 756: reward = -279.09\n",
      "01:42:52 [INFO] train episode 757: reward = -251.52\n",
      "01:42:54 [INFO] train episode 758: reward = -131.25\n",
      "01:42:55 [INFO] train episode 759: reward = -316.11\n",
      "01:42:56 [INFO] train episode 760: reward = -471.77\n",
      "01:42:58 [INFO] train episode 761: reward = -382.83\n",
      "01:42:59 [INFO] train episode 762: reward = -280.95\n",
      "01:43:00 [INFO] train episode 763: reward = -388.84\n",
      "01:43:02 [INFO] train episode 764: reward = -491.53\n",
      "01:43:03 [INFO] train episode 765: reward = -129.93\n",
      "01:43:04 [INFO] train episode 766: reward = -3.17\n",
      "01:43:05 [INFO] train episode 767: reward = -122.60\n",
      "01:43:07 [INFO] train episode 768: reward = -240.55\n",
      "01:43:08 [INFO] train episode 769: reward = -264.35\n",
      "01:43:09 [INFO] train episode 770: reward = -364.16\n",
      "01:43:11 [INFO] train episode 771: reward = -259.90\n",
      "01:43:12 [INFO] train episode 772: reward = -815.94\n",
      "01:43:13 [INFO] train episode 773: reward = -753.37\n",
      "01:43:15 [INFO] train episode 774: reward = -780.50\n",
      "01:43:16 [INFO] train episode 775: reward = -471.88\n",
      "01:43:17 [INFO] train episode 776: reward = -513.63\n",
      "01:43:19 [INFO] train episode 777: reward = -627.70\n",
      "01:43:20 [INFO] train episode 778: reward = -551.75\n",
      "01:43:21 [INFO] train episode 779: reward = -503.06\n",
      "01:43:22 [INFO] train episode 780: reward = -7.39\n",
      "01:43:24 [INFO] train episode 781: reward = -516.16\n",
      "01:43:25 [INFO] train episode 782: reward = -475.07\n",
      "01:43:27 [INFO] train episode 783: reward = -513.25\n",
      "01:43:28 [INFO] train episode 784: reward = -262.72\n",
      "01:43:29 [INFO] train episode 785: reward = -477.48\n",
      "01:43:30 [INFO] train episode 786: reward = -293.47\n",
      "01:43:32 [INFO] train episode 787: reward = -389.86\n",
      "01:43:34 [INFO] train episode 788: reward = -573.10\n",
      "01:43:36 [INFO] train episode 789: reward = -506.31\n",
      "01:43:37 [INFO] train episode 790: reward = -613.13\n",
      "01:43:39 [INFO] train episode 791: reward = -389.21\n",
      "01:43:41 [INFO] train episode 792: reward = -396.10\n",
      "01:43:42 [INFO] train episode 793: reward = -265.02\n",
      "01:43:44 [INFO] train episode 794: reward = -515.78\n",
      "01:43:45 [INFO] train episode 795: reward = -126.40\n",
      "01:43:46 [INFO] train episode 796: reward = -265.78\n",
      "01:43:48 [INFO] train episode 797: reward = -517.14\n",
      "01:43:49 [INFO] train episode 798: reward = -524.60\n",
      "01:43:51 [INFO] train episode 799: reward = -399.76\n",
      "01:43:52 [INFO] train episode 800: reward = -796.48\n",
      "01:43:53 [INFO] train episode 801: reward = -639.79\n",
      "01:43:55 [INFO] train episode 802: reward = -132.64\n",
      "01:43:57 [INFO] train episode 803: reward = -131.34\n",
      "01:43:58 [INFO] train episode 804: reward = -642.95\n",
      "01:44:00 [INFO] train episode 805: reward = -261.33\n",
      "01:44:01 [INFO] train episode 806: reward = -260.89\n",
      "01:44:02 [INFO] train episode 807: reward = -389.64\n",
      "01:44:04 [INFO] train episode 808: reward = -131.15\n",
      "01:44:06 [INFO] train episode 809: reward = -135.86\n",
      "01:44:07 [INFO] train episode 810: reward = -245.69\n",
      "01:44:08 [INFO] train episode 811: reward = -527.65\n",
      "01:44:10 [INFO] train episode 812: reward = -252.58\n",
      "01:44:11 [INFO] train episode 813: reward = -634.14\n",
      "01:44:13 [INFO] train episode 814: reward = -132.14\n",
      "01:44:14 [INFO] train episode 815: reward = -687.51\n",
      "01:44:15 [INFO] train episode 816: reward = -134.79\n",
      "01:44:17 [INFO] train episode 817: reward = -135.56\n",
      "01:44:18 [INFO] train episode 818: reward = -249.43\n",
      "01:44:20 [INFO] train episode 819: reward = -134.35\n",
      "01:44:21 [INFO] train episode 820: reward = -380.95\n",
      "01:44:23 [INFO] train episode 821: reward = -437.33\n",
      "01:44:24 [INFO] train episode 822: reward = -1133.24\n",
      "01:44:26 [INFO] train episode 823: reward = -395.59\n",
      "01:44:27 [INFO] train episode 824: reward = -591.36\n",
      "01:44:29 [INFO] train episode 825: reward = -260.43\n",
      "01:44:30 [INFO] train episode 826: reward = -638.98\n",
      "01:44:32 [INFO] train episode 827: reward = -392.24\n",
      "01:44:33 [INFO] train episode 828: reward = -267.02\n",
      "01:44:35 [INFO] train episode 829: reward = -515.31\n",
      "01:44:37 [INFO] train episode 830: reward = -510.02\n",
      "01:44:38 [INFO] train episode 831: reward = -658.67\n",
      "01:44:39 [INFO] train episode 832: reward = -636.96\n",
      "01:44:41 [INFO] train episode 833: reward = -517.33\n",
      "01:44:43 [INFO] train episode 834: reward = -511.02\n",
      "01:44:44 [INFO] train episode 835: reward = -783.28\n",
      "01:44:45 [INFO] train episode 836: reward = -518.50\n",
      "01:44:47 [INFO] train episode 837: reward = -747.20\n",
      "01:44:48 [INFO] train episode 838: reward = -503.01\n",
      "01:44:50 [INFO] train episode 839: reward = -379.61\n",
      "01:44:51 [INFO] train episode 840: reward = -722.78\n",
      "01:44:53 [INFO] train episode 841: reward = -443.75\n",
      "01:44:54 [INFO] train episode 842: reward = -633.97\n",
      "01:44:56 [INFO] train episode 843: reward = -626.76\n",
      "01:44:57 [INFO] train episode 844: reward = -524.19\n",
      "01:44:59 [INFO] train episode 845: reward = -647.07\n",
      "01:45:01 [INFO] train episode 846: reward = -561.85\n",
      "01:45:02 [INFO] train episode 847: reward = -639.82\n",
      "01:45:04 [INFO] train episode 848: reward = -763.63\n",
      "01:45:06 [INFO] train episode 849: reward = -643.19\n",
      "01:45:08 [INFO] train episode 850: reward = -406.80\n",
      "01:45:09 [INFO] train episode 851: reward = -642.56\n",
      "01:45:10 [INFO] train episode 852: reward = -607.50\n",
      "01:45:12 [INFO] train episode 853: reward = -868.66\n",
      "01:45:14 [INFO] train episode 854: reward = -386.72\n",
      "01:45:16 [INFO] train episode 855: reward = -270.02\n",
      "01:45:17 [INFO] train episode 856: reward = -265.58\n",
      "01:45:19 [INFO] train episode 857: reward = -379.95\n",
      "01:45:21 [INFO] train episode 858: reward = -135.03\n",
      "01:45:22 [INFO] train episode 859: reward = -782.88\n",
      "01:45:24 [INFO] train episode 860: reward = -267.64\n",
      "01:45:25 [INFO] train episode 861: reward = -266.68\n",
      "01:45:27 [INFO] train episode 862: reward = -631.17\n",
      "01:45:28 [INFO] train episode 863: reward = -646.95\n",
      "01:45:30 [INFO] train episode 864: reward = -511.18\n",
      "01:45:32 [INFO] train episode 865: reward = -647.19\n",
      "01:45:33 [INFO] train episode 866: reward = -535.96\n",
      "01:45:35 [INFO] train episode 867: reward = -753.69\n",
      "01:45:37 [INFO] train episode 868: reward = -648.55\n",
      "01:45:38 [INFO] train episode 869: reward = -516.34\n",
      "01:45:40 [INFO] train episode 870: reward = -531.61\n",
      "01:45:41 [INFO] train episode 871: reward = -133.80\n",
      "01:45:42 [INFO] train episode 872: reward = -135.77\n",
      "01:45:44 [INFO] train episode 873: reward = -123.88\n",
      "01:45:45 [INFO] train episode 874: reward = -515.72\n",
      "01:45:47 [INFO] train episode 875: reward = -510.66\n",
      "01:45:49 [INFO] train episode 876: reward = -274.35\n",
      "01:45:51 [INFO] train episode 877: reward = -876.09\n",
      "01:45:52 [INFO] train episode 878: reward = -790.44\n",
      "01:45:54 [INFO] train episode 879: reward = -643.37\n",
      "01:45:56 [INFO] train episode 880: reward = -264.20\n",
      "01:45:58 [INFO] train episode 881: reward = -513.57\n",
      "01:45:59 [INFO] train episode 882: reward = -791.93\n",
      "01:46:01 [INFO] train episode 883: reward = -515.03\n",
      "01:46:03 [INFO] train episode 884: reward = -640.05\n",
      "01:46:04 [INFO] train episode 885: reward = -258.93\n",
      "01:46:06 [INFO] train episode 886: reward = -390.64\n",
      "01:46:07 [INFO] train episode 887: reward = -988.27\n",
      "01:46:09 [INFO] train episode 888: reward = -384.32\n",
      "01:46:12 [INFO] train episode 889: reward = -1404.30\n",
      "01:46:14 [INFO] train episode 890: reward = -1565.28\n",
      "01:46:17 [INFO] train episode 891: reward = -1058.59\n",
      "01:46:19 [INFO] train episode 892: reward = -1195.41\n",
      "01:46:21 [INFO] train episode 893: reward = -135.01\n",
      "01:46:23 [INFO] train episode 894: reward = -129.14\n",
      "01:46:25 [INFO] train episode 895: reward = -1076.55\n",
      "01:46:27 [INFO] train episode 896: reward = -215.29\n",
      "01:46:29 [INFO] train episode 897: reward = -641.46\n",
      "01:46:32 [INFO] train episode 898: reward = -514.36\n",
      "01:46:34 [INFO] train episode 899: reward = -500.44\n",
      "01:46:36 [INFO] train episode 900: reward = -252.63\n",
      "01:46:38 [INFO] train episode 901: reward = -611.76\n",
      "01:46:41 [INFO] train episode 902: reward = -127.71\n",
      "01:46:43 [INFO] train episode 903: reward = -125.97\n",
      "01:46:45 [INFO] train episode 904: reward = -242.32\n",
      "01:46:48 [INFO] train episode 905: reward = -506.15\n",
      "01:46:50 [INFO] train episode 906: reward = -388.17\n",
      "01:46:52 [INFO] train episode 907: reward = -493.41\n",
      "01:46:55 [INFO] train episode 908: reward = -386.08\n",
      "01:46:57 [INFO] train episode 909: reward = -380.72\n",
      "01:46:59 [INFO] train episode 910: reward = -394.49\n",
      "01:47:02 [INFO] train episode 911: reward = -142.07\n",
      "01:47:04 [INFO] train episode 912: reward = -511.58\n",
      "01:47:07 [INFO] train episode 913: reward = -648.41\n",
      "01:47:09 [INFO] train episode 914: reward = -506.11\n",
      "01:47:11 [INFO] train episode 915: reward = -353.48\n",
      "01:47:14 [INFO] train episode 916: reward = -119.95\n",
      "01:47:16 [INFO] train episode 917: reward = -361.16\n",
      "01:47:19 [INFO] train episode 918: reward = -2.38\n",
      "01:47:21 [INFO] train episode 919: reward = -238.69\n",
      "01:47:24 [INFO] train episode 920: reward = -124.24\n",
      "01:47:27 [INFO] train episode 921: reward = -238.84\n",
      "01:47:29 [INFO] train episode 922: reward = -115.54\n",
      "01:47:31 [INFO] train episode 923: reward = -241.50\n",
      "01:47:34 [INFO] train episode 924: reward = -128.77\n",
      "01:47:37 [INFO] train episode 925: reward = -237.78\n",
      "01:47:39 [INFO] train episode 926: reward = -124.65\n",
      "01:47:42 [INFO] train episode 927: reward = -374.34\n",
      "01:47:45 [INFO] train episode 928: reward = -126.01\n",
      "01:47:47 [INFO] train episode 929: reward = -128.78\n",
      "01:47:50 [INFO] train episode 930: reward = -371.11\n",
      "01:47:52 [INFO] train episode 931: reward = -862.29\n",
      "01:47:55 [INFO] train episode 932: reward = -6.01\n",
      "01:47:57 [INFO] train episode 933: reward = -357.97\n",
      "01:47:59 [INFO] train episode 934: reward = -249.19\n",
      "01:48:02 [INFO] train episode 935: reward = -263.19\n",
      "01:48:04 [INFO] train episode 936: reward = -500.46\n",
      "01:48:06 [INFO] train episode 937: reward = -386.81\n",
      "01:48:08 [INFO] train episode 938: reward = -365.71\n",
      "01:48:11 [INFO] train episode 939: reward = -251.75\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m DDPGReplayBuffer \u001b[38;5;241m=\u001b[39m ReplayBuffer(obs_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,act_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m,device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      3\u001b[0m total_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500000\u001b[39m\n\u001b[0;32m----> 5\u001b[0m Qh, Ph, rh \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDDPGReplayBuffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 77\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, agent, buffer, total_step, warmup_steps, batch_num)\u001b[0m\n\u001b[1;32m     74\u001b[0m states_next \u001b[38;5;241m=\u001b[39m minibatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs_next\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     75\u001b[0m dones       \u001b[38;5;241m=\u001b[39m minibatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 77\u001b[0m Q_loss, P_loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates_next\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# 8) ログが欲しいならここで（毎step item() は遅くなるので間引くのが定石）\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[6], line 209\u001b[0m, in \u001b[0;36mDDPGAgent.update_net\u001b[0;34m(self, states, actions, rewards, states_next, dones)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_net\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m    207\u001b[0m     p\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 209\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoft_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_net\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ_target_net\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43monline_net\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ_net\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtau\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoft_update(\n\u001b[1;32m    215\u001b[0m     target_net\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP_target_net,\n\u001b[1;32m    216\u001b[0m     online_net\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP_net,\n\u001b[1;32m    217\u001b[0m     tau\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mConfig\u001b[38;5;241m.\u001b[39mtau\n\u001b[1;32m    218\u001b[0m     )\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(Q_loss\u001b[38;5;241m.\u001b[39mitem()), \u001b[38;5;28mfloat\u001b[39m(P_loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 161\u001b[0m, in \u001b[0;36mDDPGAgent.soft_update\u001b[0;34m(self, target_net, online_net, tau)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mPolyak averaging:\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m  θ' ← (1-τ) θ' + τ θ\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target_param, online_param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target_net\u001b[38;5;241m.\u001b[39mparameters(), online_net\u001b[38;5;241m.\u001b[39mparameters()):\n\u001b[0;32m--> 161\u001b[0m     \u001b[43mtarget_param\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtau\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43monline_param\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = DDPGAgent(Config=Config(),device=device)\n",
    "DDPGReplayBuffer = ReplayBuffer(obs_dim=3,act_dim=1,size=2000,device=device)\n",
    "total_step = 2500000\n",
    "\n",
    "Qh, Ph, rh = train(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    buffer=DDPGReplayBuffer,\n",
    "    total_step=total_step,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a01ba7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05010af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to models/ddpg_final_20260129_014833.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def make_unique_path(path: str | Path) -> Path:\n",
    "    \"\"\"\n",
    "    path が既に存在する場合、末尾に _1, _2, ... を付けて未使用のパスを返す。\n",
    "    例: ddpg_final_20251221_235959.pth -> ddpg_final_20251221_235959_1.pth -> ...\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "\n",
    "    # 存在しないならそのまま使う\n",
    "    if not p.exists():\n",
    "        return p\n",
    "\n",
    "    parent = p.parent\n",
    "    stem = p.stem      # 拡張子抜きファイル名\n",
    "    suffix = p.suffix  # \".pth\"\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        cand = parent / f\"{stem}_{i}{suffix}\"\n",
    "        if not cand.exists():\n",
    "            return cand\n",
    "        i += 1\n",
    "\n",
    "\n",
    "# 推論用に eval モードにしておく（保存自体は train のままでも可）\n",
    "agent.mode2eval()\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "models_dir = Path(\"./models\")\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "base_path = models_dir / f\"ddpg_final_{stamp}.pth\"\n",
    "save_path = make_unique_path(base_path)\n",
    "\n",
    "agent.save_all(\n",
    "    save_path.as_posix(),\n",
    "    extra={\n",
    "        \"total_step\": int(total_step),\n",
    "        # \"reward_history\": rh,  # 必要ならそのままでOK\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
