{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da24c76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[upd   20/3000] loss= 4645.607 pi=-0.000 v= 9291.220 ent= 2.396 avg_return(50ep)=-1199.3 elapsed= 22.6s device=cuda bins=11\n",
      "[upd   40/3000] loss= 3958.690 pi= 0.001 v= 7917.383 ent= 2.394 avg_return(50ep)=-1232.9 elapsed= 40.9s device=cuda bins=11\n",
      "[upd   60/3000] loss= 3772.094 pi= 0.001 v= 7544.191 ent= 2.394 avg_return(50ep)=-1155.2 elapsed= 66.9s device=cuda bins=11\n",
      "[upd   80/3000] loss= 4447.222 pi=-0.003 v= 8894.455 ent= 2.392 avg_return(50ep)=-1249.5 elapsed= 84.1s device=cuda bins=11\n",
      "[upd  100/3000] loss= 4836.534 pi= 0.012 v= 9673.049 ent= 2.381 avg_return(50ep)=-1166.6 elapsed= 100.1s device=cuda bins=11\n",
      "[upd  120/3000] loss= 4386.787 pi= 0.014 v= 8773.551 ent= 2.367 avg_return(50ep)=-1208.5 elapsed= 119.4s device=cuda bins=11\n",
      "[upd  140/3000] loss= 4111.366 pi= 0.013 v= 8222.711 ent= 2.353 avg_return(50ep)=-1186.6 elapsed= 138.2s device=cuda bins=11\n",
      "[upd  160/3000] loss= 4015.539 pi= 0.034 v= 8031.014 ent= 2.336 avg_return(50ep)=-1151.5 elapsed= 151.9s device=cuda bins=11\n",
      "[upd  180/3000] loss= 4987.773 pi= 0.028 v= 9975.496 ent= 2.262 avg_return(50ep)=-1189.0 elapsed= 163.1s device=cuda bins=11\n",
      "[upd  200/3000] loss= 3607.283 pi= 0.019 v= 7214.532 ent= 2.295 avg_return(50ep)=-1218.9 elapsed= 174.9s device=cuda bins=11\n",
      "[upd  220/3000] loss= 3270.804 pi= 0.013 v= 6541.586 ent= 2.235 avg_return(50ep)=-1194.1 elapsed= 189.4s device=cuda bins=11\n",
      "[upd  240/3000] loss= 4511.015 pi= 0.071 v= 9021.893 ent= 2.092 avg_return(50ep)=-1248.7 elapsed= 201.3s device=cuda bins=11\n",
      "[upd  260/3000] loss= 4244.110 pi= 0.020 v= 8488.185 ent= 2.140 avg_return(50ep)=-1214.1 elapsed= 214.0s device=cuda bins=11\n",
      "[upd  280/3000] loss= 3471.816 pi= 0.065 v= 6943.508 ent= 2.209 avg_return(50ep)=-1228.6 elapsed= 229.2s device=cuda bins=11\n",
      "[upd  300/3000] loss= 4067.730 pi= 0.027 v= 8135.409 ent= 2.245 avg_return(50ep)=-1208.2 elapsed= 246.1s device=cuda bins=11\n",
      "[upd  320/3000] loss= 3714.597 pi= 0.021 v= 7429.157 ent= 2.240 avg_return(50ep)=-1224.7 elapsed= 264.5s device=cuda bins=11\n",
      "[upd  340/3000] loss= 3686.270 pi= 0.035 v= 7372.475 ent= 2.211 avg_return(50ep)=-1180.9 elapsed= 280.7s device=cuda bins=11\n",
      "[upd  360/3000] loss= 4325.927 pi= 0.029 v= 8651.801 ent= 2.238 avg_return(50ep)=-1293.9 elapsed= 297.9s device=cuda bins=11\n",
      "[upd  380/3000] loss= 4165.903 pi= 0.008 v= 8331.795 ent= 2.140 avg_return(50ep)=-1255.8 elapsed= 311.5s device=cuda bins=11\n",
      "[upd  400/3000] loss= 4393.565 pi= 0.009 v= 8787.117 ent= 2.164 avg_return(50ep)=-1280.1 elapsed= 327.7s device=cuda bins=11\n",
      "[upd  420/3000] loss= 3902.839 pi= 0.006 v= 7805.670 ent= 2.151 avg_return(50ep)=-1257.5 elapsed= 343.7s device=cuda bins=11\n",
      "[upd  440/3000] loss= 3423.499 pi=-0.018 v= 6847.038 ent= 2.028 avg_return(50ep)=-1221.6 elapsed= 360.6s device=cuda bins=11\n",
      "[upd  460/3000] loss= 4929.329 pi= 0.041 v= 9858.578 ent= 2.055 avg_return(50ep)=-1287.5 elapsed= 376.8s device=cuda bins=11\n",
      "[upd  480/3000] loss= 3919.142 pi= 0.027 v= 7838.234 ent= 2.150 avg_return(50ep)=-1255.1 elapsed= 395.9s device=cuda bins=11\n",
      "[upd  500/3000] loss= 3802.516 pi=-0.021 v= 7605.078 ent= 2.034 avg_return(50ep)=-1279.2 elapsed= 415.7s device=cuda bins=11\n",
      "[upd  520/3000] loss= 3378.373 pi=-0.003 v= 6756.755 ent= 2.083 avg_return(50ep)=-1179.6 elapsed= 432.3s device=cuda bins=11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 320\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    309\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m Config(\n\u001b[1;32m    310\u001b[0m         n_bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11\u001b[39m,          \u001b[38;5;66;03m# 11→21に増やすと精度は上がるが学習は難しくなりがち\u001b[39;00m\n\u001b[1;32m    311\u001b[0m         n_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    318\u001b[0m         log_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m    319\u001b[0m     )\n\u001b[0;32m--> 320\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 233\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mrollout_len):\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 233\u001b[0m         a_idx, logp, v \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# a_idx: [n_envs]\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# map discrete index -> continuous action, shape (n_envs, 1)\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     a_np \u001b[38;5;241m=\u001b[39m action_values[a_idx\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()]\u001b[38;5;241m.\u001b[39mreshape(cfg\u001b[38;5;241m.\u001b[39mn_envs, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 101\u001b[0m, in \u001b[0;36mDiscreteActorCritic.act\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    obs: [B, obs_dim]\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m      v: [B]\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     logits, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(logits\u001b[38;5;241m=\u001b[39mlogits)\n\u001b[1;32m    103\u001b[0m     a_idx \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "Cell \u001b[0;32mIn[2], line 87\u001b[0m, in \u001b[0;36mDiscreteActorCritic.forward\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    obs: [B, obs_dim]\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    returns:\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m      logits: [B, n_actions]\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m      v: [B]\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrunk\u001b[49m(obs)\n\u001b[1;32m     88\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpi_head(h)\n\u001b[1;32m     89\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_head(h)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1930\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1929\u001b[0m     _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 1930\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _parameters:\n\u001b[1;32m   1931\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _parameters[name]\n\u001b[1;32m   1932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_buffers\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "from collections import deque\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    env_id: str = \"Pendulum-v1\"\n",
    "    seed: int = 0\n",
    "\n",
    "    # Discretize action in [-2, 2] into n_bins\n",
    "    n_bins: int = 11          # まず 11 がおすすめ（粗いが学習しやすい）\n",
    "    action_low: float = -2.0\n",
    "    action_high: float = 2.0\n",
    "\n",
    "    # Vectorized A2C (安定しやすい)\n",
    "    n_envs: int = 8\n",
    "    rollout_len: int = 256\n",
    "\n",
    "    # Training length\n",
    "    total_updates: int = 3000\n",
    "\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "\n",
    "    lr: float = 3e-4\n",
    "    value_coef: float = 0.5\n",
    "    entropy_coef: float = 1e-3   # 離散だと探索が死にやすいので 1e-3 を推奨\n",
    "    max_grad_norm: float = 1.0\n",
    "\n",
    "    device: str = \"cuda\"  # \"cuda\" or \"cpu\"\n",
    "    log_every: int = 20\n",
    "\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Actor-Critic Network (Discrete)\n",
    "# -----------------------------\n",
    "class DiscreteActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim: int, n_actions: int):\n",
    "        super().__init__()\n",
    "        hidden = 256\n",
    "\n",
    "        self.trunk = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.pi_head = nn.Linear(hidden, n_actions)  # logits\n",
    "        self.v_head = nn.Linear(hidden, 1)           # state value\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # よくある RL の初期化（必須ではないが安定しやすい）\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=1.0)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "        nn.init.orthogonal_(self.pi_head.weight, gain=0.01)\n",
    "        nn.init.orthogonal_(self.v_head.weight, gain=1.0)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        obs: [B, obs_dim]\n",
    "        returns:\n",
    "          logits: [B, n_actions]\n",
    "          v: [B]\n",
    "        \"\"\"\n",
    "        h = self.trunk(obs)\n",
    "        logits = self.pi_head(h)\n",
    "        v = self.v_head(h).squeeze(-1)\n",
    "        return logits, v\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        obs: [B, obs_dim]\n",
    "        returns:\n",
    "          a_idx: [B] (int64)\n",
    "          logp: [B]\n",
    "          v: [B]\n",
    "        \"\"\"\n",
    "        logits, v = self.forward(obs)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        a_idx = dist.sample()\n",
    "        logp = dist.log_prob(a_idx)\n",
    "        return a_idx, logp, v\n",
    "\n",
    "    def evaluate(self, obs: torch.Tensor, a_idx: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        obs: [B, obs_dim]\n",
    "        a_idx: [B] (int64)\n",
    "        returns:\n",
    "          logp: [B]\n",
    "          entropy: [B]\n",
    "          v: [B]\n",
    "        \"\"\"\n",
    "        logits, v = self.forward(obs)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        logp = dist.log_prob(a_idx)\n",
    "        entropy = dist.entropy()\n",
    "        return logp, entropy, v\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Rollout Buffer (vectorized)\n",
    "# -----------------------------\n",
    "class RolloutBuffer:\n",
    "    def __init__(self, T: int, n_envs: int, obs_dim: int, device: torch.device):\n",
    "        self.T = T\n",
    "        self.n_envs = n_envs\n",
    "        self.device = device\n",
    "\n",
    "        self.obs = torch.zeros((T, n_envs, obs_dim), dtype=torch.float32, device=device)\n",
    "        self.actions = torch.zeros((T, n_envs), dtype=torch.int64, device=device)\n",
    "        self.rewards = torch.zeros((T, n_envs), dtype=torch.float32, device=device)\n",
    "        self.dones = torch.zeros((T, n_envs), dtype=torch.float32, device=device)\n",
    "        self.logps = torch.zeros((T, n_envs), dtype=torch.float32, device=device)\n",
    "        self.values = torch.zeros((T, n_envs), dtype=torch.float32, device=device)\n",
    "\n",
    "        self.advantages = torch.zeros((T, n_envs), dtype=torch.float32, device=device)\n",
    "        self.returns = torch.zeros((T, n_envs), dtype=torch.float32, device=device)\n",
    "\n",
    "        self.ptr = 0\n",
    "\n",
    "    def add(self, obs, action, reward, done, logp, value):\n",
    "        t = self.ptr\n",
    "        self.obs[t] = obs\n",
    "        self.actions[t] = action\n",
    "        self.rewards[t] = reward\n",
    "        self.dones[t] = done\n",
    "        self.logps[t] = logp\n",
    "        self.values[t] = value\n",
    "        self.ptr += 1\n",
    "\n",
    "    def compute_gae(self, last_value: torch.Tensor, gamma: float, lam: float):\n",
    "        \"\"\"\n",
    "        last_value: V(s_T) for each env, shape [n_envs]\n",
    "        \"\"\"\n",
    "        adv = torch.zeros((self.n_envs,), dtype=torch.float32, device=self.device)\n",
    "        for t in reversed(range(self.T)):\n",
    "            nonterminal = 1.0 - self.dones[t]\n",
    "            next_value = last_value if t == self.T - 1 else self.values[t + 1]\n",
    "            delta = self.rewards[t] + gamma * nonterminal * next_value - self.values[t]\n",
    "            adv = delta + gamma * lam * nonterminal * adv\n",
    "            self.advantages[t] = adv\n",
    "\n",
    "        self.returns = self.advantages + self.values\n",
    "\n",
    "        # Advantage normalize over full batch (T*n_envs)\n",
    "        flat_adv = self.advantages.view(-1)\n",
    "        self.advantages = (self.advantages - flat_adv.mean()) / (flat_adv.std(unbiased=False) + 1e-8)\n",
    "\n",
    "    def flatten(self):\n",
    "        B = self.T * self.n_envs\n",
    "        obs = self.obs.view(B, -1)\n",
    "        actions = self.actions.view(B)\n",
    "        logps = self.logps.view(B)\n",
    "        values = self.values.view(B)\n",
    "        advantages = self.advantages.view(B)\n",
    "        returns = self.returns.view(B)\n",
    "        return obs, actions, logps, values, advantages, returns\n",
    "\n",
    "    def reset(self):\n",
    "        self.ptr = 0\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Vector env factory\n",
    "# -----------------------------\n",
    "def make_env(env_id: str, seed: int, idx: int):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id)\n",
    "        env.reset(seed=seed + idx)\n",
    "        env.action_space.seed(seed + idx)\n",
    "        env.observation_space.seed(seed + idx)\n",
    "        return env\n",
    "    return thunk\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Train\n",
    "# -----------------------------\n",
    "def train(cfg: Config):\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    device = torch.device(cfg.device if (cfg.device == \"cpu\" or torch.cuda.is_available()) else \"cpu\")\n",
    "    envs = gym.vector.SyncVectorEnv([make_env(cfg.env_id, cfg.seed, i) for i in range(cfg.n_envs)])\n",
    "\n",
    "    obs_np, info = envs.reset()\n",
    "    obs = torch.as_tensor(obs_np, dtype=torch.float32, device=device)\n",
    "\n",
    "    obs_dim = obs.shape[1]\n",
    "    n_actions = cfg.n_bins\n",
    "\n",
    "    # Discrete bins -> continuous action value (Pendulum expects shape (1,))\n",
    "    action_values = np.linspace(cfg.action_low, cfg.action_high, cfg.n_bins, dtype=np.float32)  # [n_bins]\n",
    "\n",
    "    net = DiscreteActorCritic(obs_dim, n_actions).to(device)\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=cfg.lr)\n",
    "\n",
    "    buf = RolloutBuffer(cfg.rollout_len, cfg.n_envs, obs_dim, device)\n",
    "\n",
    "    ep_returns = np.zeros(cfg.n_envs, dtype=np.float32)\n",
    "    recent_ep_returns = deque(maxlen=50)\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    for update in range(1, cfg.total_updates + 1):\n",
    "        buf.reset()\n",
    "\n",
    "        # -------- rollout collection --------\n",
    "        for t in range(cfg.rollout_len):\n",
    "            with torch.no_grad():\n",
    "                a_idx, logp, v = net.act(obs)  # a_idx: [n_envs]\n",
    "\n",
    "            # map discrete index -> continuous action, shape (n_envs, 1)\n",
    "            a_np = action_values[a_idx.cpu().numpy()].reshape(cfg.n_envs, 1)\n",
    "\n",
    "            next_obs_np, rewards_np, terminated, truncated, infos = envs.step(a_np)\n",
    "            done_np = np.logical_or(terminated, truncated).astype(np.float32)\n",
    "\n",
    "            rewards = torch.as_tensor(rewards_np, dtype=torch.float32, device=device)\n",
    "            dones = torch.as_tensor(done_np, dtype=torch.float32, device=device)\n",
    "            next_obs = torch.as_tensor(next_obs_np, dtype=torch.float32, device=device)\n",
    "\n",
    "            buf.add(\n",
    "                obs=obs,\n",
    "                action=a_idx,\n",
    "                reward=rewards,\n",
    "                done=dones,\n",
    "                logp=logp,\n",
    "                value=v\n",
    "            )\n",
    "\n",
    "            # episode accounting\n",
    "            ep_returns += rewards_np.astype(np.float32)\n",
    "            for i in range(cfg.n_envs):\n",
    "                if done_np[i] > 0.5:\n",
    "                    recent_ep_returns.append(float(ep_returns[i]))\n",
    "                    ep_returns[i] = 0.0\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "        # bootstrap last value\n",
    "        with torch.no_grad():\n",
    "            _, last_v = net.forward(obs)  # [n_envs]\n",
    "        buf.compute_gae(last_v, cfg.gamma, cfg.gae_lambda)\n",
    "\n",
    "        # -------- update (A2C: single epoch, full batch) --------\n",
    "        b_obs, b_actions, b_logps_old, b_values_old, b_adv, b_ret = buf.flatten()\n",
    "\n",
    "        logp_new, entropy, v_new = net.evaluate(b_obs, b_actions)\n",
    "\n",
    "        # Policy loss: -E[A log pi(a|s)]\n",
    "        policy_loss = -(b_adv * logp_new).mean()\n",
    "\n",
    "        # Value loss: E[(V - R)^2]\n",
    "        value_loss = F.mse_loss(v_new, b_ret)\n",
    "\n",
    "        # Entropy bonus (maximize entropy => minimize -entropy)\n",
    "        entropy_loss = -entropy.mean()\n",
    "\n",
    "        loss = policy_loss + cfg.value_coef * value_loss + cfg.entropy_coef * entropy_loss\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), cfg.max_grad_norm)\n",
    "        optim.step()\n",
    "\n",
    "        # -------- logging --------\n",
    "        if (update % cfg.log_every) == 0:\n",
    "            elapsed = time.time() - t0\n",
    "            avg50 = np.mean(recent_ep_returns) if len(recent_ep_returns) > 0 else float(\"nan\")\n",
    "            # detach()してからitem()を取ると警告を避けられる\n",
    "            print(\n",
    "                f\"[upd {update:4d}/{cfg.total_updates}] \"\n",
    "                f\"loss={loss.detach().item(): .3f} \"\n",
    "                f\"pi={policy_loss.detach().item(): .3f} \"\n",
    "                f\"v={value_loss.detach().item(): .3f} \"\n",
    "                f\"ent={entropy.detach().mean().item(): .3f} \"\n",
    "                f\"avg_return(50ep)={avg50: .1f} \"\n",
    "                f\"elapsed={elapsed: .1f}s device={device} bins={cfg.n_bins}\"\n",
    "            )\n",
    "\n",
    "    envs.close()\n",
    "    return net\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = Config(\n",
    "        n_bins=11,          # 11→21に増やすと精度は上がるが学習は難しくなりがち\n",
    "        n_envs=8,\n",
    "        rollout_len=256,\n",
    "        total_updates=3000,\n",
    "        lr=3e-4,\n",
    "        entropy_coef=1e-3,  # 離散で探索維持に効く\n",
    "        device=\"cuda\",\n",
    "        seed=0,\n",
    "        log_every=20,\n",
    "    )\n",
    "    train(cfg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
